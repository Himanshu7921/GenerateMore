{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f87e62",
   "metadata": {},
   "source": [
    "## Why Shifting to PyTorch?\n",
    "- Because of High Compute Time in Manual Looping and Memory Allocation\n",
    "\n",
    "## What will change?\n",
    "- Just the Core Classes but built on top of nn.Module()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f420e4d1",
   "metadata": {},
   "source": [
    "## Same Dataset used as earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2fa623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Load all the words from the '.txt' file\n",
    "words = open('names.txt', mode = 'r', encoding='utf-8').read().splitlines()\n",
    "\n",
    "# Encoder and Decoder\n",
    "# A little bit change in the Token Mapping\n",
    "# I'll use:\n",
    "# - 0 to denote ---> <PAD> tokens\n",
    "# - 1 to denote ---> '.' (end token)\n",
    "# - and all characters from [2, .., 28]\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "# Reserve 0 for PAD token\n",
    "stoi = {c: i+2 for i, c in enumerate(chars)}  # alphabet starts at index 2\n",
    "stoi['.'] = 1   # END token\n",
    "stoi['<PAD>'] = 0   # padding token\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "\n",
    "def encode_word(word, stoi):\n",
    "    return [stoi[c] for c in word] + [stoi['.']]\n",
    "\n",
    "def decode_indices(indices, itos):\n",
    "    return [itos[i.item()] for i in indices]\n",
    "\n",
    "encoded = [encode_word(w, stoi) for w in words]\n",
    "max_len = max(len(seq) for seq in encoded)\n",
    "\n",
    "def pad_sequences(sequences, max_len):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        x = seq[:-1]                        # all but last char\n",
    "        y = seq[1:]                         # all but first char\n",
    "\n",
    "        # padding\n",
    "        x += [0] * (max_len - len(x))\n",
    "        y += [-100] * (max_len - len(y))\n",
    "\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "def generate_dataset():\n",
    "    words = open('names.txt', mode = 'r', encoding='utf-8').read().splitlines()\n",
    "    encoded = [encode_word(w, stoi) for w in words]\n",
    "    max_len = max(len(seq) for seq in encoded)\n",
    "    x_tensor, y_tensor = pad_sequences(encoded, max_len)\n",
    "    return x_tensor.long(), y_tensor.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7557e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = generate_dataset()\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574b64b",
   "metadata": {},
   "source": [
    "## GRUCell's Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba81a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    \"\"\" \n",
    "    This Implementation is Differ from the PyTorch's Official Implementation in 2 Different Ways\n",
    "    NOTE-1:\n",
    "        This is not the official Implementation of PyTorch's GRUcell Because they use 2 biases per Gate\n",
    "        and i'm only using 1 bias per Gate\n",
    "\n",
    "        PyTorch's Official Implementation: \n",
    "        r = σ(W_ir x + b_ir + W_hr h + b_hr)\n",
    "        z = σ(W_iz x + b_iz + W_hz h + b_hz)\n",
    "        n = tanh(W_in x + b_in + r ⊙ (W_hn h + b_hn))\n",
    "\n",
    "        They Use 2 Bias per Gate\n",
    "    \n",
    "    NOTE-2: \n",
    "        They apply the reset gate (r) after the Multiplication of W_hn and addition of b_hn on the h_prev\n",
    "        2. Original implementation: Apply the Hadamard product (⊙) between r_t and h_prev and then apply the\n",
    "            Matrix Transformation and bias addition\n",
    "        3. What PyTorch does is, they apply the Matrix Transformation (Matrix Multiplication and bias addition) 1st and then\n",
    "            they apply the Hadamard product (⊙) between (W_hn h + b_hn)\n",
    "    \"\"\"\n",
    "    def __init__(self, embd_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        -> Bias only on x: input\n",
    "        -> No Bias on Hidden States\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Candidate transformation\n",
    "        self.Wx = nn.Linear(embd_dim, hidden_dim, bias = True)\n",
    "        self.Wh = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "\n",
    "        # Update Gate Specific Parameters\n",
    "        self.Wzx = nn.Linear(embd_dim, hidden_dim, bias = True)\n",
    "        self.Wzh = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.bias_z = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        # Reset Gate Specific Parameters\n",
    "        self.Wrx = nn.Linear(embd_dim, hidden_dim, bias = True)\n",
    "        self.Wrh = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.bias_r = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        --> A proposed update → candidate (h̃_t)\n",
    "        --> A decision gate → update gate (z_t)\n",
    "        --> A final controlled update → h_t\n",
    "\n",
    "        NOTE:   1. Reset Gate Filters h_prev\n",
    "                    - r_t = sigmoid( ( (x_t @ W_rx) + (h_prev @ W_rh) + b_r) )\n",
    "                2. Apply filter to h_prev\n",
    "                    - filtered_h_prev = r_t * h_prev [NOTE: (where * is element-wise multiplication)]\n",
    "                        - Meaning:\n",
    "                            - If r_t ≈ 0 → ignore old memory when forming candidate\n",
    "                            - If r_t ≈ 1 → use old memory fully\n",
    "                3. Compute candidate\n",
    "                    - h̃_t = tanh( ( (x_t @ W_hx) + (filtered_h_prev @ W_hh) + b_h) )\n",
    "                        - Meaning: \n",
    "                            - This produces a new memory proposal: A proposed update\n",
    "                4. Final hidden state\n",
    "                    - h_t = (1 - z_t) * h_prev + z_t * h̃_t [NOTE: (where * is element-wise multiplication)]\n",
    "        \"\"\"\n",
    "        r_t = torch.sigmoid((self.Wrx(x)) + (self.Wrh(h_prev)) + self.bias_r)\n",
    "        z_t = torch.sigmoid((self.Wzx(x)) + (self.Wzh(h_prev)) + self.bias_z)\n",
    "        h_tilde = torch.tanh(self.Wx(x) + self.Wh(r_t * h_prev))\n",
    "\n",
    "        h = (1 - z_t) * h_prev + z_t * h_tilde\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad552a",
   "metadata": {},
   "source": [
    "## GRULayer's Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cff796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULayer(nn.Module):\n",
    "    def __init__(self, embd_dim, hidden_dim, dropout = 0.0):\n",
    "        super().__init__()\n",
    "        self.grucell = GRUCell(embd_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, h_prev = None):\n",
    "        batch, seq_length, _ = x.shape # x.shape --> batch, seq_length, embd_dim\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(batch, self.grucell.hidden_dim, device = x.device)\n",
    "\n",
    "        hidden_states = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h_prev = self.grucell(x_t, h_prev)\n",
    "            h_prev = self.dropout(h_prev)\n",
    "            hidden_states.append(h_prev)\n",
    "        \n",
    "        # Stack list into tensor\n",
    "        hidden_states = torch.stack(hidden_states, dim=1)\n",
    "        return hidden_states\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4bb0de",
   "metadata": {},
   "source": [
    "## Linear Layer Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc0420aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_classes):\n",
    "        \"\"\"\n",
    "        This layer performs a linear projection on the GRU hidden states.\n",
    "        It maps the hidden vector (of size hidden_dim) into the vocabulary space (n_classes)\n",
    "        by applying a learnable affine transformation:\n",
    "\n",
    "            logits = W h + b\n",
    "\n",
    "        This is used to convert each GRU hidden state into class probabilities\n",
    "        (e.g., next-character prediction in a name generation model).\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.linear_projection = nn.Linear(in_features = hidden_dim, out_features = n_classes, bias = True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_projection(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd11cc",
   "metadata": {},
   "source": [
    "# GRU Model Family: Architectural Variants and Hyperparameter Configurations\n",
    "\n",
    "In this section, three distinct GRU-based architectures are defined for the task of character-level name generation.\n",
    "Each model represents a different point on the capacity–complexity spectrum, enabling controlled experiments regarding sequence modeling depth, representational power, and generalization.\n",
    "The following subsections document each model variant, its architectural design, and the rationale behind the selected hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. GRU-Small Model\n",
    "\n",
    "### Architectural Description\n",
    "\n",
    "The small variant is a lightweight single-layer GRU architecture designed primarily for baseline performance measurement and rapid experimentation.\n",
    "It consists of:\n",
    "\n",
    "* An embedding layer mapping discrete character indices to continuous vectors.\n",
    "* A single GRU layer operating over sequences of embeddings.\n",
    "* A linear projection layer mapping hidden states to vocabulary logits.\n",
    "\n",
    "### Hyperparameter Configuration\n",
    "\n",
    "```\n",
    "embedding_dim = 64\n",
    "hidden_dim    = 128\n",
    "epochs        = 35\n",
    "learning_rate = 1e-3\n",
    "batch_size    = 128\n",
    "block_size    = 12\n",
    "dropout       = 0.0\n",
    "optimizer     = Adam\n",
    "```\n",
    "\n",
    "### Purpose and Expected Behavior\n",
    "\n",
    "This model is suitable for preliminary studies, small datasets, debugging the training loop, and validating convergence behavior.\n",
    "Its capacity is intentionally constrained to expose underfitting tendencies and provide a clean comparison baseline for larger models.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. GRU-Medium Model\n",
    "\n",
    "### Architectural Description\n",
    "\n",
    "The medium-sized architecture increases modeling depth by stacking two GRU layers.\n",
    "The output of the first GRU layer serves as the input to the second, enabling the model to capture more complex sequential patterns and longer-range dependencies.\n",
    "It includes:\n",
    "\n",
    "* Embedding layer\n",
    "* GRU Layer 1\n",
    "* GRU Layer 2\n",
    "* Linear output layer\n",
    "\n",
    "### Hyperparameter Configuration\n",
    "\n",
    "```\n",
    "embedding_dim = 96\n",
    "hidden_dim    = 256\n",
    "epochs        = 40\n",
    "learning_rate = 5e-4\n",
    "batch_size    = 128\n",
    "block_size    = 16\n",
    "dropout       = 0.1\n",
    "optimizer     = Adam\n",
    "```\n",
    "\n",
    "### Purpose and Expected Behavior\n",
    "\n",
    "This configuration balances computational cost and expressive power.\n",
    "It is recommended for medium-sized datasets (20k–50k sequences) and typically exhibits substantially improved name-generation quality over the single-layer model, with a reduced risk of underfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. GRU-Large Model\n",
    "\n",
    "### Architectural Description\n",
    "\n",
    "The large variant maximizes modeling capacity by stacking three GRU layers and applying layer normalization before the final projection.\n",
    "This design improves gradient stability in deep recurrent stacks and enhances pattern extraction across long context windows.\n",
    "The model consists of:\n",
    "\n",
    "* Embedding layer\n",
    "* GRU Layer 1\n",
    "* GRU Layer 2\n",
    "* GRU Layer 3\n",
    "* LayerNorm for stabilization\n",
    "* Linear projection head\n",
    "\n",
    "### Hyperparameter Configuration\n",
    "\n",
    "```\n",
    "embedding_dim = 128\n",
    "hidden_dim    = 512\n",
    "epochs        = 60\n",
    "learning_rate = 3e-4\n",
    "batch_size    = 256\n",
    "block_size    = 20\n",
    "dropout       = 0.2\n",
    "optimizer     = AdamW\n",
    "```\n",
    "\n",
    "### Purpose and Expected Behavior\n",
    "\n",
    "This model is designed for large-scale experiments where high-quality sequence generation is essential.\n",
    "It is well suited for datasets with more than 30k examples (e.g., your dataset with 32k names).\n",
    "The combination of deeper recurrence, higher dimensionality, and AdamW regularization typically yields the strongest generalization and sampling quality.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "The three GRU architectures—Small, Medium, and Large—provide a systematic framework for analyzing how increasing representational depth and parameter count influence performance on name-generation tasks.\n",
    "These models are meant to be trained independently and compared using quantitative metrics (training and validation loss curves) and qualitative evaluation (sampled names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c98d464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_SmallModel(nn.Module):\n",
    "    def __init__(self, n_classes, embd_dim, hidden_dim, dropout = 0.0):\n",
    "        \"\"\"\n",
    "        I'll Train this Small sized GRU on this Hyperparameters\n",
    "            embd_dim = 64\n",
    "            hidden_dim = 128\n",
    "            epochs = 35\n",
    "            lr = 1e-3\n",
    "            batch_size = 128\n",
    "            block_size = 12\n",
    "            dropout = 0.0\n",
    "            optimizer = Adam\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_classes,\n",
    "            embedding_dim=embd_dim\n",
    "        )\n",
    "\n",
    "        self.gru = GRULayer(\n",
    "            embd_dim=embd_dim,\n",
    "            hidden_dim=hidden_dim\n",
    "        )\n",
    "\n",
    "        self.linear = Linear(\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_classes=n_classes\n",
    "        )\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) → indices of characters\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Embedding: (batch, seq_len, embd_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # 2. GRU: (batch, seq_len, hidden_dim)\n",
    "        h = self.gru(x)\n",
    "\n",
    "        # 3. Linear projection: output logits\n",
    "        logits = self.linear(h)   # (batch, seq_len, n_classes)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff4c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_MediumModel(nn.Module):\n",
    "    def __init__(self, n_classes, embd_dim, hidden_dim, dropout = 0.1):\n",
    "        \"\"\"\n",
    "        I'll Train this Medium sized GRU on this Hyperparameters\n",
    "            embd_dim = 96\n",
    "            hidden_dim = 256\n",
    "            epochs = 40\n",
    "            lr = 5e-4\n",
    "            batch_size = 128\n",
    "            block_size = 16\n",
    "            dropout = 0.1\n",
    "            optimizer = Adam\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_classes,\n",
    "            embedding_dim=embd_dim\n",
    "        )\n",
    "\n",
    "        self.gru_layer_1 = GRULayer(\n",
    "            embd_dim=embd_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout = dropout\n",
    "        )\n",
    "\n",
    "        # The output of previous gru_layer_1 is now the input of next gru_layer_2; that's why 'embd_dim = hidden_dim' for gru_layer_2\n",
    "        self.gru_layer_2 = GRULayer(\n",
    "            embd_dim = hidden_dim,\n",
    "            hidden_dim = hidden_dim,\n",
    "            dropout = dropout\n",
    "        )\n",
    "\n",
    "        self.linear = Linear(\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_classes=n_classes\n",
    "        )\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) → indices of characters\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Embedding: (batch, seq_len, embd_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # 2. GRU: (batch, seq_len, hidden_dim)\n",
    "        h1 = self.gru_layer_1(x)\n",
    "        h2 = self.gru_layer_2(h1)\n",
    "\n",
    "        # 3. Linear projection: output logits\n",
    "        logits = self.linear(h2)   # (batch, seq_len, n_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_LargeModel(nn.Module):\n",
    "    def __init__(self, n_classes, embd_dim, hidden_dim, dropout = 0.2):\n",
    "        \"\"\"\n",
    "        I'll Train this Large sized GRU on this Hyperparameters\n",
    "            embd_dim = 128\n",
    "            hidden_dim = 512\n",
    "            epochs = 60\n",
    "            lr = 3e-4\n",
    "            batch_size = 256\n",
    "            block_size = 20\n",
    "            dropout = 0.2\n",
    "            optimizer = AdamW\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_classes,\n",
    "            embedding_dim=embd_dim\n",
    "        )\n",
    "\n",
    "        self.gru_layer_1 = GRULayer(\n",
    "            embd_dim=embd_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout = dropout\n",
    "        )\n",
    "        self.gru_layer_2 = GRULayer(\n",
    "            embd_dim = hidden_dim,\n",
    "            hidden_dim = hidden_dim,\n",
    "            dropout = dropout\n",
    "        )\n",
    "        self.gru_layer_3 = GRULayer(\n",
    "            embd_dim = hidden_dim,\n",
    "            hidden_dim = hidden_dim,\n",
    "            dropout = dropout\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.linear = Linear(\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_classes=n_classes\n",
    "        )\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) → indices of characters\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Embedding: (batch, seq_len, embd_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # 2. GRU: (batch, seq_len, hidden_dim) \n",
    "        h1 = self.gru_layer_1(x)\n",
    "        h2 = self.gru_layer_2(h1)\n",
    "        h3 = self.gru_layer_3(h2)\n",
    "\n",
    "        # 3. LayerNorm Before Final Logits\n",
    "        h3_norm = self.norm(h3)\n",
    "\n",
    "        # 4. Linear projection: output logits\n",
    "        logits = self.linear(h3_norm)   # (batch, seq_len, n_classes)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9cf696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_REGISTRY = {\n",
    "    \"GRU_SmallModel\": GRU_SmallModel,\n",
    "    \"GRU_MediumModel\": GRU_MediumModel,\n",
    "    \"GRU_LargeModel\": GRU_LargeModel,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4eab6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_summary(model, model_name, epochs, lr, device):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MODEL TRAINING CONFIGURATION\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Model Name      : {model_name}\")\n",
    "    print(f\"Device          : {device}\")\n",
    "    print(f\"Total Epochs    : {epochs}\")\n",
    "    print(f\"Learning Rate   : {lr}\")\n",
    "    \n",
    "    print(\"\\nMODEL ARCHITECTURE\")\n",
    "    print(\"-\"*100)\n",
    "    for name, module in model.named_modules():\n",
    "        if name == \"\":\n",
    "            print(f\"  └── {module.__class__.__name__}()\")\n",
    "        else:\n",
    "            print(f\"  └── {name}: {module.__class__.__name__}()\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    # Count parameters\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTrainable Parameters: {n_params:,}\")\n",
    "    print(\"=\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5123be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation_loss(model, val_loader, loss_fn, device):\n",
    "    model.eval()     # IMPORTANT: evaluation mode\n",
    "    total_loss = 0.0\n",
    "    batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            logits = model(xb)\n",
    "\n",
    "            loss = loss_fn(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                yb.reshape(-1)\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batches += 1\n",
    "\n",
    "    model.train()   # switch back to training mode\n",
    "    return total_loss / batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12426413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, lr, device,\n",
    "                optimizer, loss_fn, model_name):\n",
    "\n",
    "    print_model_summary(model, model_name, epochs, lr, device)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        last_logits = None\n",
    "\n",
    "        for i, (xb, yb) in enumerate(train_loader):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "\n",
    "            # reshape for CE loss\n",
    "            # NOTE: CrossEntropyLoss expects:\n",
    "                    # logits: (N, C),  N --> number of Training samples --> batch_size, C --> n_classes\n",
    "                    # targets: (N)\n",
    "            \n",
    "            # Our logits = model.forward(input); Have shape = (batch_size, seq_length, n_classes)\n",
    "            # Cross Entropy extects, (training_examples, n_classes) so we should resize them\n",
    "            # logits.reshape(-1, vocab_size) = (batch_size * seq_length, n_classes) or calculate 'vocab_size' dynamically as, logits.size(-1)\n",
    "            # So, logits.reshape(-1, logits.size(-1)) --> logits.reshape(batch_size * seq_length, n_classes)\n",
    "\n",
    "            loss = loss_fn(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                yb.reshape(-1)\n",
    "            )\n",
    "            if i == 0:\n",
    "                last_logits = logits.detach()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        val_loss = evaluate_validation_loss(model, val_loader, loss_fn, device)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"[Epoch {epoch:04d}/{epochs}] \"\n",
    "                f\"Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | \"\n",
    "                f\"LR: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994cdfc",
   "metadata": {},
   "source": [
    "## Function for Sampling from the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd348a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_names(model, num_names, start_token, max_len, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    for i in range(1, num_names + 1):\n",
    "        context = torch.tensor([[start_token]], device=device)  # (1, 1)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits = model(context)              # (1, seq_len, vocab)\n",
    "            last_logits = logits[:, -1, :]       # last timestep\n",
    "            probs = torch.softmax(last_logits, dim=-1)\n",
    "\n",
    "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "            context = torch.cat([\n",
    "                context,\n",
    "                torch.tensor([[next_id]], device=device)\n",
    "            ], dim=1)\n",
    "\n",
    "            if next_id == 1:   # end token\n",
    "                break\n",
    "\n",
    "        # Decode\n",
    "        token_ids = context[0].tolist()[1:]      # remove start token\n",
    "        name = ''.join(itos[t] for t in token_ids)\n",
    "\n",
    "        results.append(name)\n",
    "\n",
    "        # Print with numeric prefix\n",
    "        print(f\"{i}. {name.capitalize()}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a403aa",
   "metadata": {},
   "source": [
    "## Function for Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7655ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model(model, base_name=\"GRU\", path=\"./saved_models/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # versioning\n",
    "    existing = [f for f in os.listdir(path) if f.startswith(base_name) and f.endswith(\".pth\")]\n",
    "    versions = []\n",
    "    for f in existing:\n",
    "        parts = f.replace(\".pth\", \"\").split(\"_v\")\n",
    "        if len(parts) == 2 and parts[1].isdigit():\n",
    "            versions.append(int(parts[1]))\n",
    "    next_version = max(versions, default=0) + 1\n",
    "\n",
    "    filename = f\"{base_name}_v{next_version}.pth\"\n",
    "    save_path = os.path.join(path, filename)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"model_class\": model.__class__.__name__,\n",
    "        \"n_classes\": model.n_classes,\n",
    "        \"embd_dim\": model.embd_dim,\n",
    "        \"hidden_dim\": model.hidden_dim,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"version\": next_version,\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, save_path)\n",
    "    torch.save(checkpoint, os.path.join(path, f\"{base_name}_latest.pth\"))\n",
    "\n",
    "    print(f\"\\nModel saved at: {save_path}\")\n",
    "    print(f\"Also updated: {base_name}_latest.pth\\n\")\n",
    "    return save_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0eb49",
   "metadata": {},
   "source": [
    "## Function for Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a04f99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def load_model(filepath, device):\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "\n",
    "    class_name = checkpoint[\"model_class\"]\n",
    "    if class_name not in MODEL_REGISTRY:\n",
    "        raise ValueError(f\"Unknown model class '{class_name}' in checkpoint.\")\n",
    "\n",
    "    ModelClass = MODEL_REGISTRY[class_name]\n",
    "\n",
    "    # instantiate dynamically\n",
    "    model = ModelClass(\n",
    "        n_classes=checkpoint[\"n_classes\"],\n",
    "        embd_dim=checkpoint[\"embd_dim\"],\n",
    "        hidden_dim=checkpoint[\"hidden_dim\"]\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "    # print loaded model metadata\n",
    "    print(\"\\n================ MODEL LOADED ================\")\n",
    "    print(f\"File: {filepath}\")\n",
    "    print(f\"Model Class: {class_name}\")\n",
    "    print(f\"Version: v{checkpoint['version']}\")\n",
    "    print(f\"Timestamp: {checkpoint['timestamp']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(\"Model Architecture:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if name != \"\":\n",
    "            print(f\"  └── {name}: {module.__class__.__name__}\")\n",
    "\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"Total Trainable Parameters: {count_parameters(model):,}\")\n",
    "    print(f\"Loaded on: {device}\")\n",
    "    print(\"==============================================\\n\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3359edc6",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed9289",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse:collapse; font-size:15px;\" border=\"1\">\n",
    "  <tr>\n",
    "    <th>Model Name</th>\n",
    "    <th>Total Params</th>\n",
    "    <th>Layers</th>\n",
    "    <th>Embedding Dim (d<sub>embd</sub>)</th>\n",
    "    <th>Hidden Dim (d<sub>hidden</sub>)</th>\n",
    "    <th>Batch Size</th>\n",
    "    <th>Block Size</th>\n",
    "    <th>Dropout</th>\n",
    "    <th>Learning Rate</th>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td>GRU-α (Small)</td>\n",
    "    <td><i>0.080M</i></td>\n",
    "    <td>1</td>\n",
    "    <td>64</td>\n",
    "    <td>128</td>\n",
    "    <td>128</td>\n",
    "    <td>12</td>\n",
    "    <td>0.0</td>\n",
    "    <td>\\$(1.0 \\times 10^{-3}$\\)</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td>GRU-β (Medium)</td>\n",
    "    <td><i>0.68M</i></td>\n",
    "    <td>2</td>\n",
    "    <td>96</td>\n",
    "    <td>256</td>\n",
    "    <td>128</td>\n",
    "    <td>16</td>\n",
    "    <td>0.1</td>\n",
    "    <td><i></i></td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td>GRU-γ (Large)</td>\n",
    "    <td><i>4.16M</i></td>\n",
    "    <td>3</td>\n",
    "    <td>128</td>\n",
    "    <td>512</td>\n",
    "    <td>256</td>\n",
    "    <td>20</td>\n",
    "    <td>0.2</td>\n",
    "    <td><i>3.0e−4</i></td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343fa20",
   "metadata": {},
   "source": [
    "## Defining 3 Different Architecture: Alpha, Beta, Gamma Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "178d6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------- Small GRU Model -------------------------------------------------------- \n",
    "small_model = GRU_SmallModel(\n",
    "    n_classes = 28,\n",
    "    embd_dim = 64,\n",
    "    hidden_dim = 128,\n",
    "    dropout = 0.0\n",
    ").to(device)\n",
    "small_model_name = \"GRU_alpha_model\"\n",
    "lr_small_model = 1e-3\n",
    "epochs_small_model = 35\n",
    "optimizer_small = torch.optim.Adam(small_model.parameters(), lr = lr_small_model)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------- Medium GRU Model -------------------------------------------------------- \n",
    "medium_model = GRU_MediumModel(\n",
    "    n_classes=28,\n",
    "    embd_dim = 96,\n",
    "    hidden_dim = 256,\n",
    "    dropout = 0.1\n",
    ").to(device)\n",
    "medium_model_name = \"GRU_beta_model\"\n",
    "lr_medium_model = 5e-4\n",
    "epochs_medium_model = 40\n",
    "optimizer_medium = torch.optim.Adam(medium_model.parameters(), lr = lr_medium_model)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------- Large GRU Model -------------------------------------------------------- \n",
    "large_model = GRU_LargeModel(\n",
    "    n_classes=28,\n",
    "    embd_dim = 128,\n",
    "    hidden_dim = 512,\n",
    "    dropout = 0.2\n",
    ").to(device)\n",
    "large_model_name = \"GRU_gamma_model\"\n",
    "lr_large_model = 3e-4\n",
    "epochs_large_model = 60\n",
    "optimizer_large = torch.optim.AdamW(large_model.parameters(), lr = lr_large_model)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------- Model Registry -------------------------------------------------------- \n",
    "MODEL_REGISTRY = {\n",
    "    \"GRU_SmallModel\": GRU_SmallModel,\n",
    "    \"GRU_MediumModel\": GRU_MediumModel,\n",
    "    \"GRU_LargeModel\": GRU_LargeModel,\n",
    "}\n",
    "\n",
    "\n",
    "# ---------------------------------------------------- Train and Validation Data Loader -------------------------------------------------------- \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(x_train, y_train),\n",
    "    batch_size=64,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(x_val, y_val),\n",
    "    batch_size = 64,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234dfd40",
   "metadata": {},
   "source": [
    "## Training GRU Small Model: GRU_alpha_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "987b5f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODEL TRAINING CONFIGURATION\n",
      "====================================================================================================\n",
      "Model Name      : GRU_alpha_model\n",
      "Device          : cuda\n",
      "Total Epochs    : 35\n",
      "Learning Rate   : 0.001\n",
      "\n",
      "MODEL ARCHITECTURE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  └── GRU_SmallModel()\n",
      "  └── embedding: Embedding()\n",
      "  └── gru: GRULayer()\n",
      "  └── gru.grucell: GRUCell()\n",
      "  └── gru.grucell.Wx: Linear()\n",
      "  └── gru.grucell.Wh: Linear()\n",
      "  └── gru.grucell.Wzx: Linear()\n",
      "  └── gru.grucell.Wzh: Linear()\n",
      "  └── gru.grucell.Wrx: Linear()\n",
      "  └── gru.grucell.Wrh: Linear()\n",
      "  └── gru.dropout: Dropout()\n",
      "  └── linear: Linear()\n",
      "  └── linear.linear_projection: Linear()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trainable Parameters: 79,772\n",
      "====================================================================================================\n",
      "\n",
      "[Epoch 0010/35] Train Loss: 1.7499 | Val Loss: 1.8252 | LR: 0.001000\n",
      "[Epoch 0020/35] Train Loss: 1.6492 | Val Loss: 1.8151 | LR: 0.001000\n",
      "[Epoch 0030/35] Train Loss: 1.5909 | Val Loss: 1.8292 | LR: 0.001000\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=small_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=epochs_small_model,\n",
    "    lr=lr_small_model,\n",
    "    device=device,\n",
    "    optimizer=optimizer_small,\n",
    "    loss_fn=loss_fn,\n",
    "    model_name=small_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cbb3652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Urt.\n",
      "2. Elyann.\n",
      "3. Ynner.\n",
      "4. Ika.\n",
      "5. Hidaleigh.\n",
      "6. Ynzel.\n",
      "7. Ynley.\n",
      "8. Ella.\n",
      "9. Iyana.\n",
      "10. Oil.\n",
      "11. Aylynn.\n",
      "12. Handrea.\n",
      "13. Ynishka.\n",
      "14. Ayanne.\n",
      "15. Ynnsleigh.\n",
      "16. Inthim.\n",
      "17. Ynnan.\n",
      "18. Endy.\n",
      "19. Ysen.\n",
      "20. Hloei.\n"
     ]
    }
   ],
   "source": [
    "generated = generate_names(\n",
    "    model=small_model,\n",
    "    num_names=20,\n",
    "    start_token=0,\n",
    "    max_len=20,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31ca01f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at: ./saved_models/GRU_alpha_model_v1.pth\n",
      "Also updated: GRU_alpha_model_latest.pth\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./saved_models/GRU_alpha_model_v1.pth'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(small_model, base_name=\"GRU_alpha_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9015d8a",
   "metadata": {},
   "source": [
    "## Train Medium Model: GRU_beta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4574f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODEL TRAINING CONFIGURATION\n",
      "====================================================================================================\n",
      "Model Name      : GRU_beta_model\n",
      "Device          : cuda\n",
      "Total Epochs    : 40\n",
      "Learning Rate   : 0.0005\n",
      "\n",
      "MODEL ARCHITECTURE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  └── GRU_MediumModel()\n",
      "  └── embedding: Embedding()\n",
      "  └── gru_layer_1: GRULayer()\n",
      "  └── gru_layer_1.grucell: GRUCell()\n",
      "  └── gru_layer_1.grucell.Wx: Linear()\n",
      "  └── gru_layer_1.grucell.Wh: Linear()\n",
      "  └── gru_layer_1.grucell.Wzx: Linear()\n",
      "  └── gru_layer_1.grucell.Wzh: Linear()\n",
      "  └── gru_layer_1.grucell.Wrx: Linear()\n",
      "  └── gru_layer_1.grucell.Wrh: Linear()\n",
      "  └── gru_layer_1.dropout: Dropout()\n",
      "  └── gru_layer_2: GRULayer()\n",
      "  └── gru_layer_2.grucell: GRUCell()\n",
      "  └── gru_layer_2.grucell.Wx: Linear()\n",
      "  └── gru_layer_2.grucell.Wh: Linear()\n",
      "  └── gru_layer_2.grucell.Wzx: Linear()\n",
      "  └── gru_layer_2.grucell.Wzh: Linear()\n",
      "  └── gru_layer_2.grucell.Wrx: Linear()\n",
      "  └── gru_layer_2.grucell.Wrh: Linear()\n",
      "  └── gru_layer_2.dropout: Dropout()\n",
      "  └── linear: Linear()\n",
      "  └── linear.linear_projection: Linear()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trainable Parameters: 675,996\n",
      "====================================================================================================\n",
      "\n",
      "[Epoch 0010/40] Train Loss: 1.7375 | Val Loss: 1.7699 | LR: 0.000500\n",
      "[Epoch 0020/40] Train Loss: 1.6216 | Val Loss: 1.7383 | LR: 0.000500\n",
      "[Epoch 0030/40] Train Loss: 1.5606 | Val Loss: 1.7427 | LR: 0.000500\n",
      "[Epoch 0040/40] Train Loss: 1.5195 | Val Loss: 1.7576 | LR: 0.000500\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=medium_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=epochs_medium_model,\n",
    "    lr=lr_medium_model,\n",
    "    device=device,\n",
    "    optimizer=optimizer_medium,\n",
    "    loss_fn=loss_fn,\n",
    "    model_name = medium_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08e2e7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. .\n",
      "2. Quinlynn.\n",
      "3. Xander.\n",
      "4. Suriah.\n",
      "5. .\n",
      "6. Karie.\n",
      "7. Arona.\n",
      "8. Xara.\n",
      "9. Elodi.\n",
      "10. Mannabelle.\n",
      "11. Somani.\n",
      "12. Attie.\n",
      "13. Fadir.\n",
      "14. Runiel.\n",
      "15. Xaro.\n",
      "16. Quest.\n",
      "17. Samiya.\n",
      "18. Braelynn.\n",
      "19. Dannah.\n",
      "20. Maddelene.\n"
     ]
    }
   ],
   "source": [
    "generated = generate_names(\n",
    "    model=medium_model,\n",
    "    num_names=20,\n",
    "    start_token=0,\n",
    "    max_len=20,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8131e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at: ./saved_models/GRU_beta_model_v1.pth\n",
      "Also updated: GRU_beta_model_latest.pth\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./saved_models/GRU_beta_model_v1.pth'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(medium_model, base_name=\"GRU_beta_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77555dce",
   "metadata": {},
   "source": [
    "## Train Large Model: GRU_gamma_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "102c440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODEL TRAINING CONFIGURATION\n",
      "====================================================================================================\n",
      "Model Name      : GRU_gamma_model\n",
      "Device          : cuda\n",
      "Total Epochs    : 60\n",
      "Learning Rate   : 0.0003\n",
      "\n",
      "MODEL ARCHITECTURE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  └── GRU_LargeModel()\n",
      "  └── embedding: Embedding()\n",
      "  └── gru_layer_1: GRULayer()\n",
      "  └── gru_layer_1.grucell: GRUCell()\n",
      "  └── gru_layer_1.grucell.Wx: Linear()\n",
      "  └── gru_layer_1.grucell.Wh: Linear()\n",
      "  └── gru_layer_1.grucell.Wzx: Linear()\n",
      "  └── gru_layer_1.grucell.Wzh: Linear()\n",
      "  └── gru_layer_1.grucell.Wrx: Linear()\n",
      "  └── gru_layer_1.grucell.Wrh: Linear()\n",
      "  └── gru_layer_1.dropout: Dropout()\n",
      "  └── gru_layer_2: GRULayer()\n",
      "  └── gru_layer_2.grucell: GRUCell()\n",
      "  └── gru_layer_2.grucell.Wx: Linear()\n",
      "  └── gru_layer_2.grucell.Wh: Linear()\n",
      "  └── gru_layer_2.grucell.Wzx: Linear()\n",
      "  └── gru_layer_2.grucell.Wzh: Linear()\n",
      "  └── gru_layer_2.grucell.Wrx: Linear()\n",
      "  └── gru_layer_2.grucell.Wrh: Linear()\n",
      "  └── gru_layer_2.dropout: Dropout()\n",
      "  └── gru_layer_3: GRULayer()\n",
      "  └── gru_layer_3.grucell: GRUCell()\n",
      "  └── gru_layer_3.grucell.Wx: Linear()\n",
      "  └── gru_layer_3.grucell.Wh: Linear()\n",
      "  └── gru_layer_3.grucell.Wzx: Linear()\n",
      "  └── gru_layer_3.grucell.Wzh: Linear()\n",
      "  └── gru_layer_3.grucell.Wrx: Linear()\n",
      "  └── gru_layer_3.grucell.Wrh: Linear()\n",
      "  └── gru_layer_3.dropout: Dropout()\n",
      "  └── norm: LayerNorm()\n",
      "  └── linear: Linear()\n",
      "  └── linear.linear_projection: Linear()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trainable Parameters: 4,155,420\n",
      "====================================================================================================\n",
      "\n",
      "[Epoch 0010/60] Train Loss: 1.7467 | Val Loss: 1.7812 | LR: 0.000300\n",
      "[Epoch 0020/60] Train Loss: 1.5990 | Val Loss: 1.7635 | LR: 0.000300\n",
      "[Epoch 0030/60] Train Loss: 1.5173 | Val Loss: 1.7954 | LR: 0.000300\n",
      "[Epoch 0040/60] Train Loss: 1.4678 | Val Loss: 1.8309 | LR: 0.000300\n",
      "[Epoch 0050/60] Train Loss: 1.4374 | Val Loss: 1.8774 | LR: 0.000300\n",
      "[Epoch 0060/60] Train Loss: 1.4146 | Val Loss: 1.9051 | LR: 0.000300\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=large_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=epochs_large_model,\n",
    "    lr=lr_large_model,\n",
    "    device=device,\n",
    "    optimizer=optimizer_large,\n",
    "    loss_fn=loss_fn,\n",
    "    model_name=large_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbd3acf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Intly.\n",
      "2. Inley.\n",
      "3. Aili.\n",
      "4. Aivah.\n",
      "5. Adell.\n",
      "6. Arris.\n",
      "7. Oakley.\n",
      "8. Aelen.\n",
      "9. Ooa.\n",
      "10. Ayari.\n",
      "11. Ayaan.\n",
      "12. Inam.\n",
      "13. Aisaiah.\n",
      "14. Aerin.\n",
      "15. Inda.\n",
      "16. Ira.\n",
      "17. Iria.\n",
      "18. Aiya.\n",
      "19. Ona.\n",
      "20. Inna.\n"
     ]
    }
   ],
   "source": [
    "generated = generate_names(\n",
    "    model=large_model,\n",
    "    num_names=20,\n",
    "    start_token=0,\n",
    "    max_len=20,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab08117f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at: ./saved_models/GRU_gamma_model_v1.pth\n",
      "Also updated: GRU_gamma_model_latest.pth\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./saved_models/GRU_gamma_model_v1.pth'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(large_model, base_name=\"GRU_gamma_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9eb98",
   "metadata": {},
   "source": [
    "## Load All the Model's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca202351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ MODEL LOADED ================\n",
      "File: ./saved_models/GRU_alpha_model_v1.pth\n",
      "Model Class: GRU_SmallModel\n",
      "Version: v1\n",
      "Timestamp: 2025-12-11 20:34:52\n",
      "----------------------------------------------\n",
      "Model Architecture:\n",
      "  └── embedding: Embedding\n",
      "  └── gru: GRULayer\n",
      "  └── gru.grucell: GRUCell\n",
      "  └── gru.grucell.Wx: Linear\n",
      "  └── gru.grucell.Wh: Linear\n",
      "  └── gru.grucell.Wzx: Linear\n",
      "  └── gru.grucell.Wzh: Linear\n",
      "  └── gru.grucell.Wrx: Linear\n",
      "  └── gru.grucell.Wrh: Linear\n",
      "  └── gru.dropout: Dropout\n",
      "  └── linear: Linear\n",
      "  └── linear.linear_projection: Linear\n",
      "----------------------------------------------\n",
      "Total Trainable Parameters: 79,772\n",
      "Loaded on: cuda\n",
      "==============================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_24668\\2349754167.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_model(\"./saved_models/GRU_alpha_model_v1.pth\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "666db734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ MODEL LOADED ================\n",
      "File: ./saved_models/GRU_beta_model_v1.pth\n",
      "Model Class: GRU_MediumModel\n",
      "Version: v1\n",
      "Timestamp: 2025-12-11 21:02:05\n",
      "----------------------------------------------\n",
      "Model Architecture:\n",
      "  └── embedding: Embedding\n",
      "  └── gru_layer_1: GRULayer\n",
      "  └── gru_layer_1.grucell: GRUCell\n",
      "  └── gru_layer_1.grucell.Wx: Linear\n",
      "  └── gru_layer_1.grucell.Wh: Linear\n",
      "  └── gru_layer_1.grucell.Wzx: Linear\n",
      "  └── gru_layer_1.grucell.Wzh: Linear\n",
      "  └── gru_layer_1.grucell.Wrx: Linear\n",
      "  └── gru_layer_1.grucell.Wrh: Linear\n",
      "  └── gru_layer_1.dropout: Dropout\n",
      "  └── gru_layer_2: GRULayer\n",
      "  └── gru_layer_2.grucell: GRUCell\n",
      "  └── gru_layer_2.grucell.Wx: Linear\n",
      "  └── gru_layer_2.grucell.Wh: Linear\n",
      "  └── gru_layer_2.grucell.Wzx: Linear\n",
      "  └── gru_layer_2.grucell.Wzh: Linear\n",
      "  └── gru_layer_2.grucell.Wrx: Linear\n",
      "  └── gru_layer_2.grucell.Wrh: Linear\n",
      "  └── gru_layer_2.dropout: Dropout\n",
      "  └── linear: Linear\n",
      "  └── linear.linear_projection: Linear\n",
      "----------------------------------------------\n",
      "Total Trainable Parameters: 675,996\n",
      "Loaded on: cuda\n",
      "==============================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_24668\\2349754167.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_model(\"./saved_models/GRU_beta_model_v1.pth\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c34d9eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_24668\\2349754167.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ MODEL LOADED ================\n",
      "File: ./saved_models/GRU_gamma_model_v1.pth\n",
      "Model Class: GRU_LargeModel\n",
      "Version: v1\n",
      "Timestamp: 2025-12-11 22:10:22\n",
      "----------------------------------------------\n",
      "Model Architecture:\n",
      "  └── embedding: Embedding\n",
      "  └── gru_layer_1: GRULayer\n",
      "  └── gru_layer_1.grucell: GRUCell\n",
      "  └── gru_layer_1.grucell.Wx: Linear\n",
      "  └── gru_layer_1.grucell.Wh: Linear\n",
      "  └── gru_layer_1.grucell.Wzx: Linear\n",
      "  └── gru_layer_1.grucell.Wzh: Linear\n",
      "  └── gru_layer_1.grucell.Wrx: Linear\n",
      "  └── gru_layer_1.grucell.Wrh: Linear\n",
      "  └── gru_layer_1.dropout: Dropout\n",
      "  └── gru_layer_2: GRULayer\n",
      "  └── gru_layer_2.grucell: GRUCell\n",
      "  └── gru_layer_2.grucell.Wx: Linear\n",
      "  └── gru_layer_2.grucell.Wh: Linear\n",
      "  └── gru_layer_2.grucell.Wzx: Linear\n",
      "  └── gru_layer_2.grucell.Wzh: Linear\n",
      "  └── gru_layer_2.grucell.Wrx: Linear\n",
      "  └── gru_layer_2.grucell.Wrh: Linear\n",
      "  └── gru_layer_2.dropout: Dropout\n",
      "  └── gru_layer_3: GRULayer\n",
      "  └── gru_layer_3.grucell: GRUCell\n",
      "  └── gru_layer_3.grucell.Wx: Linear\n",
      "  └── gru_layer_3.grucell.Wh: Linear\n",
      "  └── gru_layer_3.grucell.Wzx: Linear\n",
      "  └── gru_layer_3.grucell.Wzh: Linear\n",
      "  └── gru_layer_3.grucell.Wrx: Linear\n",
      "  └── gru_layer_3.grucell.Wrh: Linear\n",
      "  └── gru_layer_3.dropout: Dropout\n",
      "  └── norm: LayerNorm\n",
      "  └── linear: Linear\n",
      "  └── linear.linear_projection: Linear\n",
      "----------------------------------------------\n",
      "Total Trainable Parameters: 4,155,420\n",
      "Loaded on: cuda\n",
      "==============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_model(\"./saved_models/GRU_gamma_model_v1.pth\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dbf1ef",
   "metadata": {},
   "source": [
    "# **GRU Model Family — Architecture Summary**\n",
    "\n",
    "| **Model Name**     | **nₚₐᵣₐₘₛ** | **nₗₐᵧₑᵣₛ** | **dₑₘᵦd** | **dₕᵢddₑₙ** | **Batch Size** | **Block Size** | **Dropout** | **Optimizer** |    **Learning Rate** |\n",
    "| ------------------ | ----------: | ----------: | --------: | ----------: | -------------: | -------------: | ----------: | ------------- | -------------------: |\n",
    "| **GRU-α (Small)**  |   **~180K** |       1 GRU |        64 |         128 |            128 |             12 |         0.0 | Adam          | (1.0 \\times 10^{-3}) |\n",
    "| **GRU-β (Medium)** |   **~520K** |       2 GRU |        96 |         256 |            128 |             16 |         0.1 | Adam          | (5.0 \\times 10^{-4}) |\n",
    "| **GRU-γ (Large)**  |   **~2.1M** |       3 GRU |       128 |         512 |            256 |             20 |         0.2 | AdamW         | (3.0 \\times 10^{-4}) |\n",
    "\n",
    "---\n",
    "\n",
    "# **Notes on Table Design**\n",
    "\n",
    "This table matches GPT-3’s formatting style:\n",
    "\n",
    "* **Scientific notation** for learning rate.\n",
    "* **Compact architecture descriptions** (similar to `d_model`, `n_layers`, `n_heads`).\n",
    "* **Consistent numerical alignment** for research readability.\n",
    "* **Greek-letter model names** mimic GPT naming conventions while staying unique.\n",
    "\n",
    "---\n",
    "\n",
    "# **Parameter Count Estimates**\n",
    "\n",
    "Here is how approximate counts were derived:\n",
    "\n",
    "### **1. GRU-α (Small)**\n",
    "\n",
    "* embd_dim = 64\n",
    "* hidden_dim = 128\n",
    "* 1 GRU layer + embedding + linear\n",
    "* Parameters ≈ **0.080M**\n",
    "\n",
    "### **2. GRU-β (Medium)**\n",
    "\n",
    "* embd_dim = 96\n",
    "* hidden_dim = 256\n",
    "* 2 stacked GRUs + embedding + linear\n",
    "* Parameters ≈ **0.68M**\n",
    "\n",
    "### **3. GRU-γ (Large)**\n",
    "\n",
    "* embd_dim = 128\n",
    "* hidden_dim = 512\n",
    "* 3 stacked GRUs + embedding + linear + layernorm\n",
    "* Parameters ≈ **4.16M**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
