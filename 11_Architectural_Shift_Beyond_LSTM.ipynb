{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b77cd37",
   "metadata": {},
   "source": [
    "## Transition: From Recurrent Memory to Relational Sequence Modeling\n",
    "\n",
    "In the previous notebook, we analyzed LSTMs through a set of analytical lenses and showed that their limitations arise from core architectural assumptions rather than optimization or data constraints. These limitations consistently pointed to the same underlying causes: forced sequential dependency, fixed-capacity memory, premature importance decisions, compressed representations, destructive memory access, and poor scalability to long sequences.\n",
    "\n",
    "This notebook builds directly on those insights. Instead of modifying recurrence or improving gating mechanisms, we explore an alternative architectural direction that removes the need for timestep-to-timestep dependency altogether. The goal is not to replace recurrence with a specific named model, but to derive a new class of sequence architectures that satisfy the architectural requirements identified by the lens-based analysis.\n",
    "\n",
    "We begin by constructing a sequence model in which memory scales with input length, relevance is computed dynamically at usage time, interactions between sequence elements are content-based rather than position-based, and computation can be performed in parallel across all positions. Only after deriving these properties from first principles will we connect them to existing architectural realizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a561d1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6992b960",
   "metadata": {},
   "source": [
    "# Deriving Self-Attention — From the Very First Step\n",
    "\n",
    "## Step 0: What problem are we *actually* solving?\n",
    "\n",
    "We are given a sequence:\n",
    "\n",
    "$\n",
    "x_1, x_2, x_3, \\dots, x_T\n",
    "$\n",
    "\n",
    "Example (keep this fixed in your head):\n",
    "\n",
    "> “The keys to the cabinet are missing”\n",
    "\n",
    "Our goal is **not**:\n",
    "\n",
    "* to predict next token yet\n",
    "* to name an architecture\n",
    "\n",
    "Our goal is only this:\n",
    "\n",
    "> **For each position, build a representation that contains the information it actually needs from the rest of the sequence.**\n",
    "\n",
    "That’s it.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: What did LSTM do wrong (minimum statement)\n",
    "\n",
    "LSTM tried to do this:\n",
    "\n",
    "$\n",
    "\\text{many tokens} ;\\rightarrow; \\text{one memory vector}\n",
    "$\n",
    "\n",
    "That forced:\n",
    "\n",
    "* compression\n",
    "* guessing importance early\n",
    "* sequential dependency\n",
    "\n",
    "So we impose our **first rule**:\n",
    "\n",
    "> **We will NOT compress the entire sequence into one vector.**\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: New starting assumption (very basic)\n",
    "\n",
    "Instead of one memory, we keep **one vector per token**.\n",
    "\n",
    "So we do the most boring thing possible:\n",
    "\n",
    "$\n",
    "x_i ;\\rightarrow; h_i \\in \\mathbb{R}^d\n",
    "$\n",
    "\n",
    "Now we have:\n",
    "\n",
    "$\n",
    "h_1, h_2, h_3, \\dots, h_T\n",
    "$\n",
    "\n",
    "At this point:\n",
    "\n",
    "* no recurrence\n",
    "* no order logic\n",
    "* no interaction\n",
    "\n",
    "Just vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Why this is still useless\n",
    "\n",
    "Right now:\n",
    "\n",
    "* $ (h_{1}) $ knows nothing about $ (h_{2}) $\n",
    "* $ (h_{5}) $ knows nothing about $ (h_{2}) $\n",
    "\n",
    "But language requires this:\n",
    "\n",
    "> “When deciding something at position (i), I must look at other positions.”\n",
    "\n",
    "So we ask the **first real question**:\n",
    "\n",
    "> **How should one token look at other tokens?**\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: The most basic form of “looking”\n",
    "\n",
    "To “look at” another token means:\n",
    "\n",
    "> Measure **how relevant** token (j) is to token (i)\n",
    "\n",
    "So we want a **number**:\n",
    "\n",
    "$\n",
    "\\text{relevance}(i, j) \\in \\mathbb{R}\n",
    "$\n",
    "\n",
    "What can we use?\n",
    "\n",
    "The simplest thing in linear algebra:\n",
    "\n",
    "* dot product\n",
    "\n",
    "So first attempt:\n",
    "\n",
    "$\n",
    "\\text{relevance}(i, j) = h_i^\\top h_{j}\n",
    "$\n",
    "\n",
    "This says:\n",
    "\n",
    "* if vectors align → relevant\n",
    "* if not → irrelevant\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Why raw dot product is not enough\n",
    "\n",
    "Problem:\n",
    "\n",
    "* all tokens use the same representation\n",
    "* token has no way to say **“I’m asking”** vs **“I’m providing”**\n",
    "\n",
    "So we introduce a **very small idea**:\n",
    "\n",
    "> A token should behave differently when *asking* than when *answering*.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Split roles (this is the key mental shift)\n",
    "\n",
    "Each token will produce **three different vectors**:\n",
    "\n",
    "1. One to **ask** questions\n",
    "2. One to **advertise what it contains**\n",
    "3. One to **send information**\n",
    "\n",
    "So for token (i):\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\text{Query: } & q_i = W_Q h_i \\\n",
    "\\text{Key: } & k_i = W_K h_i \\\n",
    "\\text{Value: } & v_i = W_V h_i\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "This is not fancy.\n",
    "It’s just **three linear layers**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7: Now define relevance properly\n",
    "\n",
    "Token (i) asks:\n",
    "\n",
    "> “Which other tokens matter to me?”\n",
    "\n",
    "So relevance becomes:\n",
    "\n",
    "$\n",
    "s_{ij} = q_i^\\top k_j\n",
    "$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "* (q_i): what I want\n",
    "* (k_j): what token (j) offers\n",
    "* dot product → match quality\n",
    "\n",
    "---\n",
    "\n",
    "## Step 8: Turn relevance into importance weights\n",
    "\n",
    "Raw scores are meaningless alone.\n",
    "\n",
    "We want:\n",
    "\n",
    "* higher score → more influence\n",
    "* total influence = 1\n",
    "\n",
    "So we normalize:\n",
    "\n",
    "$\n",
    "\\alpha_{ij} =\n",
    "\\frac{e^{s_{ij}}}{\\sum_{m=1}^{T} e^{s_{im}}}\n",
    "$\n",
    "\n",
    "Now:\n",
    "$\n",
    "\\sum_{j=1}^{T} \\alpha_{ij} = 1\n",
    "$\n",
    "\n",
    "These are **importance weights**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 9: Finally, build the new representation\n",
    "\n",
    "Token (i) now **collects information**:\n",
    "\n",
    "$\n",
    "z_i = \\sum_{j=1}^{T} \\alpha_{ij} v_j\n",
    "$\n",
    "\n",
    "Meaning:\n",
    "\n",
    "* look at all tokens\n",
    "* take what matters\n",
    "* ignore the rest\n",
    "\n",
    "This is the **entire mechanism**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 10: Stop and reflect (important)\n",
    "\n",
    "Notice what we achieved **without naming anything**:\n",
    "\n",
    "* no compression\n",
    "* no recurrence\n",
    "* no guessing early\n",
    "* no destructive memory\n",
    "* no long gradient paths\n",
    "\n",
    "Each (z_i) is:\n",
    "\n",
    "> “token (i) + exactly the context it needs”\n",
    "\n",
    "---\n",
    "\n",
    "## Step 11: Write it compactly (only now)\n",
    "\n",
    "Let:\n",
    "\n",
    "$\n",
    "Q = HW_Q,\\quad K = HW_K,\\quad V = HW_V\n",
    "$\n",
    "\n",
    "Then:\n",
    "\n",
    "$\n",
    "Z = \\text{softmax}(QK^\\top)V\n",
    "$\n",
    "\n",
    "This is just **Steps 6–9 written together**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 12: Only now — give it a name\n",
    "\n",
    "> A mechanism where each element in a sequence builds its representation by dynamically weighting other elements in the same sequence is called **self-attention**.\n",
    "\n",
    "Not because it’s popular.\n",
    "Because **nothing else satisfies the constraints you derived from LSTM’s failures**.\n",
    "\n",
    "---\n",
    "\n",
    "## One-line intuition (keep this)\n",
    "\n",
    "> LSTM remembers by overwriting; self-attention remembers by keeping everything and choosing later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8226879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b3a2ed1",
   "metadata": {},
   "source": [
    "## Step 0: What problem are we *actually* solving?\n",
    "\n",
    "We are given a sequence:\n",
    "\n",
    "$\n",
    "x_1, x_2, x_3, \\dots, x_T\n",
    "$\n",
    "\n",
    "Example (keep this fixed in your head):\n",
    "\n",
    "> “The keys to the cabinet are missing”\n",
    "\n",
    "Our goal is **not**:\n",
    "\n",
    "* to predict next token yet\n",
    "* to name an architecture\n",
    "\n",
    "Our goal is only this:\n",
    "\n",
    "> **For each position, build a representation that contains the information it actually needs from the rest of the sequence.**\n",
    "\n",
    "That’s it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b92881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "text = open(\"tiny_shakespeare.txt\", 'r', encoding='utf-8').read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "\n",
    "data = torch.tensor([stoi[c] for c in text], dtype=torch.long)\n",
    "seq_length = 50  # sequence length\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split='train', batch_size = 64):\n",
    "    source = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(source) - seq_length - 1, (batch_size,))\n",
    "    X = torch.stack([source[i:i+seq_length] for i in ix])\n",
    "    Y = torch.stack([source[i+1:i+seq_length+1] for i in ix])\n",
    "    return X, Y\n",
    "x_dummy, y_dummy = get_batch(batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baaf0094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is text:  ty to my elders.\n",
      "\n",
      "KATHARINA:\n",
      "Of all thy suitors, h\n",
      "This is target:  y to my elders.\n",
      "\n",
      "KATHARINA:\n",
      "Of all thy suitors, he\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step-0: Now lets build the representation for each of the input char: Embeddings\n",
      "> Before Embeddings: Input Sequence looks something like this:\n",
      "          tensor([58, 63,  1, 58, 53,  1, 51, 63,  1, 43])\n",
      "> After Embeddings: Now each Input Sequence is a 2-D vector representation in 2D Space:\n",
      "          tensor([[0.6645, 0.2370],\n",
      "        [0.7891, 0.9039],\n",
      "        [0.0207, 0.0243],\n",
      "        [0.6645, 0.2370],\n",
      "        [0.4648, 0.0068],\n",
      "        [0.0207, 0.0243],\n",
      "        [0.5886, 0.1974],\n",
      "        [0.7891, 0.9039],\n",
      "        [0.0207, 0.0243],\n",
      "        [0.5165, 0.7936]])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "> Now we have: h1, h2, h3, h4, h5, ..... h_t\n",
      "> At this point: \n",
      "    * no recurrence\n",
      "    * no order logic\n",
      "    * no interaction\n",
      "NOTE: But this is useless because now h5 dsn't know anything about h2 and so on... \n",
      "But language requires this:\n",
      " > “When deciding something at position (i), I must look at other positions.”\n",
      "\n",
      "Now Lets Build a mechanism to get the information from other positions as well\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "This is our 1st 5 Embedding Vectors, tensor([[0.6645, 0.2370],\n",
      "        [0.7891, 0.9039],\n",
      "        [0.0207, 0.0243],\n",
      "        [0.6645, 0.2370],\n",
      "        [0.4648, 0.0068]])\n",
      "now if we want to see wheather token at index 3rd tensor([0.6645, 0.2370]) is relevent to token at index 2nd tensor([0.0207, 0.0243]),\n",
      "then we compute this,\n",
      "tensor([0.6645, 0.2370]) @ tensor([0.0207, 0.0243]) = 0.020\n",
      "> Higher relative dot product → higher relevance after normalization, tensor([0.6645, 0.2370])\n",
      "    * if vectors align → relevant\n",
      "    * if not → irrelevant\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Problem:\n",
      "    * all tokens use the same representation\n",
      "    * token has no way to say “I’m asking” vs “I’m providing”\n",
      "So now we use 3 different Matrix which behaves differently when asking vs giving the information\n",
      "and They are,\n",
      "1) Query: asks which tokes are relevent to me\n",
      "2) Key: Responds to query and says wheather it is really important to specific query or not\n",
      "3) Value: Which set of values I need to add so that I can update my current embedding representation so that i can use the information from the relevent key\n",
      "> Now A token should behave differently when asking than when answering.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "We want that for each token in embedding_dim there must be a Query, i.e for each Token we want a Query => Q.shape = (n_embedding_dims, n_query_dim)\n",
      "for Each Query we want a key, so => K.shape = Q.shape  = (n_embedding_dims, n_query_dim)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence: 'ty to'\n",
      "index = 0 | char = t\n",
      "index = 1 | char = y\n",
      "index = 2 | char =  \n",
      "index = 3 | char = t\n",
      "index = 4 | char = o\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Now if let's say token at index 3rd have to check wheather token at index 2nd is relevent or not then,\n",
      "Token 3 will raise a Query as, tensor([0.6637, 0.7656])\n",
      "Token 2 will raise a Key as, tensor([0.0170, 0.0325])\n",
      "To check Relevency: we compute, tensor([0.6637, 0.7656]) @ tensor([0.0170, 0.0325]) = 0.036\n",
      "For Token o: \n",
      "> How much token at index = 4 (o) is relevent to token at index = 4 (o) = 0.246595\n",
      "> How much token at index = 4 (o) is relevent to token at index = 3 (t) = 0.447800\n",
      "> How much token at index = 4 (o) is relevent to token at index = 2 ( ) = 0.021034\n",
      "> How much token at index = 4 (o) is relevent to token at index = 1 (y) = 0.792416\n",
      "> How much token at index = 4 (o) is relevent to token at index = 0 (t) = 0.447800\n",
      "----------------------------------------------------------------------------------------------------\n",
      "For Token t: \n",
      "> How much token at index = 3 (t) is relevent to token at index = 3 (t) = 0.768195\n",
      "> How much token at index = 3 (t) is relevent to token at index = 2 ( ) = 0.036160\n",
      "> How much token at index = 3 (t) is relevent to token at index = 1 (y) = 1.362200\n",
      "> How much token at index = 3 (t) is relevent to token at index = 0 (t) = 0.768195\n",
      "----------------------------------------------------------------------------------------------------\n",
      "For Token  : \n",
      "> How much token at index = 2 ( ) is relevent to token at index = 2 ( ) = 0.001579\n",
      "> How much token at index = 2 ( ) is relevent to token at index = 1 (y) = 0.059495\n",
      "> How much token at index = 2 ( ) is relevent to token at index = 0 (t) = 0.033453\n",
      "----------------------------------------------------------------------------------------------------\n",
      "For Token y: \n",
      "> How much token at index = 1 (y) is relevent to token at index = 1 (y) = 2.245054\n",
      "> How much token at index = 1 (y) is relevent to token at index = 0 (t) = 1.262442\n",
      "----------------------------------------------------------------------------------------------------\n",
      "For Token t: \n",
      "> How much token at index = 2 (t) is relevent to token at index = 0 (t) = 0.768195\n",
      "----------------------------------------------------------------------------------------------------\n",
      "OR we can implicitly calculate this as, \n",
      "query = (x_enc[0][:5] @ w_q)\n",
      "key = (x_enc[0][:5] @ w_k)\n",
      "scores = query @ key.T\n",
      "Now scores[i][j] denotes how much j is relevent to i\n",
      "> How much token at index = 4 (o) is relevent to token at index = 3 (t) = 0.447800\n",
      "> Sentence = `ty to`\n",
      "> Token 3 = `t`\n",
      "> Token 1 = `y`\n",
      "normalized_score[3] = tensor([0.2000, 0.3623, 0.0962, 0.2000, 0.1415])\n",
      "> This means that for Token at index 3, This shows that how much token 3 cares about every other token\n",
      "> How much token 3 cares about token 1? = normalized_score[3][1] = 0.3623\n",
      "> This means that token 3 cares mostly about Token 1,\n",
      "> Which essentially means that, Token 3 will use most of the information from Token 1,\n",
      "   but also uses information from rest of the Tokens, nothing is discarded completely (unless weight ≈ 0)\n",
      "> How to use information from other tokens?,\n",
      "we use Value Matrix for that,\n",
      "Z = normalized_score @ value \n",
      "`normalized_score @ value` makes every token rewrite itself using information from all other tokens, weighted by relevance \n",
      "> For Updating the representation of token 3, we would do this,\n",
      "normalized_score[3] = tensor([0.2000, 0.3623, 0.0962, 0.2000, 0.1415])\n",
      "value = tensor([[0.6523, 0.5831, 0.6611],\n",
      "        [0.9725, 1.1785, 1.2015],\n",
      "        [0.0257, 0.0314, 0.0319],\n",
      "        [0.6523, 0.5831, 0.6611],\n",
      "        [0.4057, 0.2837, 0.3560]])\n",
      "We will do, normalized_score[3] @ value = tensor([0.6731, 0.7034, 0.7532]), what does this means?\n",
      "This is the new updated context aware representation of Token 3\n",
      "> New Context Aware Representation for Token 3 (context-enriched embedding) = tensor([0.6731, 0.7034, 0.7532])\n",
      "> Old Representation for Token 3 (original embedding (no context)) = tensor([0.6645, 0.2370])\n",
      "NOTE: Notice how the new context-rich embedding lives in new 3d space? and Original embedding was in 2d space?\n",
      "> The reason is: Old Representation for Token 3 lives in embedding space and n_embedding_dims = 2, \n",
      "and New Context Aware Representation for Token 3 lives in Value space and n_values_dims = 3\n",
      "> Notice how, n_embedding_dims != n_values_dims? This is Architectural Design\n",
      "> We can optionally project this 3d --> 2d or Value space --> embedding space, by just another linear Projection Layer\n",
      "> If we want to use Stacked Layers then, the attention output must be mapped back to the embedding space. This is NOT OPTONAL ANYMORE\n",
      "\n",
      " ------------------------------------------------------------ Multi-head Attention --------------------------------------------------------------------------------- \n",
      "> But wait, in Language like 'English', etc there are many relationships we want to discover,\n",
      "instead of letting a single head to learn all possible relationship in 'n_embedding_dims' vector-space\n",
      "we project this input sequence into 'h' different sub-space of each 'n_embedding_dims/h' vector sub-spaces and in each of the 'h' sub-spaces\n",
      "> We allow the model to learn the relationships, learnable parameters and discoveries independently and they all operate on the same input sequence and then we concatenate them all\n",
      "> Benefit? Number of Parameters stays the same because after concatenation of 'h' different sub-spaces of each dim = n_embedding_dims/h\n",
      "we get 'n_embedding_dims/h * h' = 'n_embedding_dims' again, so number of parameters are unchanged and each of the different 'h' learned differet patterns and we concatenate them to preserve all learnable patterns\n",
      "> Conclusion: Multi-head attention works because it allows multiple independent relational views of the same sequence to coexist without interference, and preserves all of them through concatenation.\n",
      "\n",
      "------------------------------------------------------------ Positional Embeddings ------------------------------------------------------------\n",
      "But wait, even after learning all token-to-token dependencies, we ignored a fundamental property of language: position.\n",
      "Self-attention can learn relationships between words, but without positional information it has no notion of order.\n",
      "For example:\n",
      "S-1: The dog bit the man.\n",
      "S-2: The man bit the dog.\n",
      "Both sentences contain the same words, but their meanings are completely different due to word order.\n",
      "Without positional encodings, a self-attention model treats the input as a set, not a sequence, and cannot distinguish these cases.\n",
      "> Therefore, we must explicitly inject positional information so the model can learn relationships with respect to position.\n",
      "> Where injection happens\n",
      "> Given token embeddings: \n",
      ">>> X ∈ R^{T × n_embedding_dims}\n",
      "> and positional encodings:\n",
      ">>> P ∈ R^{T × d_model}\n",
      "we form, \n",
      "X_pos = X + P\n",
      "> All attention (Q, K, V) is computed from X_pos, not X.\n",
      "Q = X_pos @ W_q\n",
      "K = X_pos @ W_k\n",
      "V = X_pos @ W_v\n",
      "\n",
      "> There are two common ways to do this:\n",
      "Method 1: Learned Positional Embeddings\n",
      "> Create an embedding table for positions\n",
      "> One vector per position index\n",
      ">>> pos_embedding = nn.Embedding(max_len, n_embedding_dims)\n",
      ">>> For a sequence of length T:\n",
      ">>> positions = [0, 1, 2, ..., T-1]\n",
      ">>> P = pos_embedding(positions)\n",
      ">>> X_pos = X + P\n",
      "Method 2: Fixed (Sinusoidal) Positional Encodings\n",
      "Here, positions are not learned.\n",
      "For position pos and dimension i:\n",
      "P[pos, 2i]   = sin(pos / 10000^(2i / d_model))\n",
      "P[pos, 2i+1] = cos(pos / 10000^(2i / d_model))\n",
      "Then:\n",
      "X_pos = X + P\n",
      "Now,\n",
      " * Each position has a unique pattern\n",
      " * Relative positions can be inferred via linear operations\n",
      " * No extra learned parameters\n",
      "> Because sine and cosine allow linear layers to infer relative offsets between positions, even though the encoding itself is absolute.\n",
      "Example: \n",
      "Embeddings Shape: torch.Size([65, 2])\n",
      "Positional Embeddings shape: torch.Size([65, 2])\n",
      "we define: 'x_pos = x_enc + P' which contains the positional information as well\n",
      "First 2 values of x_enc = tensor([[0.6645, 0.2370],\n",
      "        [0.7891, 0.9039]])\n",
      "First 2 values of x_pos = tensor([[0.7988, 1.0592],\n",
      "        [1.6367, 1.3248]])\n",
      "> NOTE: Notice how `x_pos` is enriched with positional infromation as well?\n",
      "Note: Sinusoidal positional encodings are useful because shifting a position by Δ,\n",
      "      corresponds to a fixed linear transformation of its encoding, allowing attention layers to infer relative positions using only linear operations.\n"
     ]
    }
   ],
   "source": [
    "text = ''.join([itos[num.item()] for num in x_dummy[0]])\n",
    "target = ''.join([itos[num.item()] for num in y_dummy[0]])\n",
    "print(\"This is text: \", text)\n",
    "print(\"This is target: \", target)\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(\"Step-0: Now lets build the representation for each of the input char: Embeddings\")\n",
    "n_embedding_dims = 2\n",
    "embeddings = torch.rand(len(chars), n_embedding_dims)\n",
    "x_enc = embeddings[x_dummy] # Now each input char is now a 2-D vector representation in 2D Space\n",
    "x_enc[0][:10]\n",
    "\n",
    "print(f\"> Before Embeddings: Input Sequence looks something like this:\\n          {x_dummy[0][:10]}\")\n",
    "print(f\"> After Embeddings: Now each Input Sequence is a 2-D vector representation in 2D Space:\\n          {x_enc[0][:10]}\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"> Now we have: h1, h2, h3, h4, h5, ..... h_t\")\n",
    "print(f\"> At this point: \")\n",
    "print(f\"    * no recurrence\")\n",
    "print(f\"    * no order logic\")\n",
    "print(f\"    * no interaction\")\n",
    "print(f\"NOTE: But this is useless because now h5 dsn't know anything about h2 and so on... \")\n",
    "print(f\"But language requires this:\\n > “When deciding something at position (i), I must look at other positions.”\")\n",
    "print(f\"\\nNow Lets Build a mechanism to get the information from other positions as well\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"\\nThis is our 1st 5 Embedding Vectors, {x_enc[0][:5]}\\nnow if we want to see wheather token at index 3rd {x_enc[0][:5][3]} is relevent to token at index 2nd {x_enc[0][:5][2]},\\nthen we compute this,\\n{x_enc[0][:5][3]} @ {x_enc[0][:5][2]} = {(x_enc[0][:5][3] @ x_enc[0][:5][2]):.3f}\")\n",
    "print(f\"> Higher relative dot product → higher relevance after normalization, {x_enc[0][:5][3]}\")\n",
    "print(f\"    * if vectors align → relevant\")\n",
    "print(f\"    * if not → irrelevant\")\n",
    "print(\"-\" * 100)\n",
    "print(\"Problem:\")\n",
    "print(f\"    * all tokens use the same representation\")\n",
    "print(f\"    * token has no way to say “I’m asking” vs “I’m providing”\")\n",
    "print(f\"So now we use 3 different Matrix which behaves differently when asking vs giving the information\")\n",
    "print(f\"and They are,\\n1) Query: asks which tokes are relevent to me\\n2) Key: Responds to query and says wheather it is really important to specific query or not\")\n",
    "print(f\"3) Value: Which set of values I need to add so that I can update my current embedding representation so that i can use the information from the relevent key\")\n",
    "print(\"> Now A token should behave differently when asking than when answering.\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Creating 3 Linear Projections vector\n",
    "n_query_dims = n_embedding_dims # one query per token\n",
    "print(\"We want that for each token in embedding_dim there must be a Query, i.e for each Token we want a Query => Q.shape = (n_embedding_dims, n_query_dim)\")\n",
    "print(\"for Each Query we want a key, so => K.shape = Q.shape  = (n_embedding_dims, n_query_dim)\")\n",
    "n_key_dims = n_query_dims # one key per token\n",
    "n_values_dims = 3\n",
    "w_q = torch.rand(n_query_dims, n_query_dims)\n",
    "w_k = torch.rand(n_query_dims, n_key_dims)\n",
    "w_v = torch.rand(n_query_dims, n_values_dims)\n",
    "sentence = ''.join([itos[num.item()] for num in x_dummy[0][:5]])\n",
    "print(\"-\" * 100)\n",
    "print(f\"Sentence: '{sentence}'\")\n",
    "for i, char in enumerate(sentence):\n",
    "    print(f\"index = {i} | char = {char}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Now if let's say token at index  3rd have to check wheather token at index 2nd is relevent or not then,\n",
    "# 2nd token will expose his information about what i can answer as: w_k @ x_enc[0][:5][2]\n",
    "print(\"Now if let's say token at index 3rd have to check wheather token at index 2nd is relevent or not then,\")\n",
    "print(f\"Token 3 will raise a Query as, {x_enc[0][:5][3] @ w_q}\")\n",
    "print(f\"Token 2 will raise a Key as, {x_enc[0][:5][2] @ w_k}\")\n",
    "print(f\"To check Relevency: we compute, {x_enc[0][:5][3] @ w_q} @ {x_enc[0][:5][2] @ w_k} = {((x_enc[0][:5][3] @ w_q) @ (x_enc[0][:5][2] @ w_k)):.3f}\")\n",
    "x_enc[0][:5][3] @ w_q # Here the 3rd token is asking wheather someone is relevent to me or not?\n",
    "x_enc[0][:5][2] @ w_k\n",
    "# If someone is looking for relevance and on the other hand if someone if giving its relevense for all tokens then we can get the answer as:\n",
    "(x_enc[0][:5][3] @ w_q) @ (x_enc[0][:5][2] @ w_k) # This is how much important token at index 2 is, to token at index 3\n",
    "\n",
    "\n",
    "# If we compute this for every 5 tokens then,\n",
    "relevance_4_4 = (x_enc[0][:5][4] @ w_q) @ (x_enc[0][:5][4] @ w_k) # How much token at index 4th is relevent to token at index 4 (itself)\n",
    "relevance_4_3 = (x_enc[0][:5][4] @ w_q) @ (x_enc[0][:5][3] @ w_k) # How much token at index 4th is relevent to token at index 3\n",
    "relevance_4_2 = (x_enc[0][:5][4] @ w_q) @ (x_enc[0][:5][2] @ w_k) # How much token at index 4th is relevent to token at index 2\n",
    "relevance_4_1 = (x_enc[0][:5][4] @ w_q) @ (x_enc[0][:5][1] @ w_k) # How much token at index 4th is relevent to token at index 1\n",
    "relevance_4_0 = (x_enc[0][:5][4] @ w_q) @ (x_enc[0][:5][0] @ w_k) # How much token at index 4th is relevent to token at index 0\n",
    "# -------------------------------------------------------------------------------------\n",
    "relevance_3_3 = (x_enc[0][:5][3] @ w_q) @ (x_enc[0][:5][3] @ w_k) # How much token at index 3rd is relevent to token at index 3rd (itself)\n",
    "relevance_3_2 = (x_enc[0][:5][3] @ w_q) @ (x_enc[0][:5][2] @ w_k) # How much token at index 3rd is relevent to token at index 2nd\n",
    "relevance_3_1 = (x_enc[0][:5][3] @ w_q) @ (x_enc[0][:5][1] @ w_k) # How much token at index 3rd is relevent to token at index 1st\n",
    "relevance_3_0 = (x_enc[0][:5][3] @ w_q) @ (x_enc[0][:5][0] @ w_k) # How much token at index 3rd is relevent to token at index 0\n",
    "# -------------------------------------------------------------------------------------\n",
    "relevance_2_2 = (x_enc[0][:5][2] @ w_q) @ (x_enc[0][:5][2] @ w_k) # How much token at index 2nd is relevent to token at index 2nd (itself)\n",
    "relevance_2_1 = (x_enc[0][:5][2] @ w_q) @ (x_enc[0][:5][1] @ w_k) # How much token at index 2nd is relevent to token at index 1st\n",
    "relevance_2_0 = (x_enc[0][:5][2] @ w_q) @ (x_enc[0][:5][0] @ w_k) # How much token at index 2nd is relevent to token at index 0\n",
    "# -------------------------------------------------------------------------------------\n",
    "relevance_1_1 = (x_enc[0][:5][1] @ w_q) @ (x_enc[0][:5][1] @ w_k) # How much token at index 1st is relevent to token at index 1st (itself)\n",
    "relevance_1_0 = (x_enc[0][:5][1] @ w_q) @ (x_enc[0][:5][0] @ w_k) # How much token at index 1st is relevent to token at index 0\n",
    "# -------------------------------------------------------------------------------------\n",
    "relevance_0_0 = (x_enc[0][:5][0] @ w_q) @ (x_enc[0][:5][0] @ w_k) # How much token at index 2nd is relevent to token at index 0 (itself)\n",
    "\n",
    "print(f\"For Token {sentence[4]}: \")\n",
    "print(f\"> How much token at index = 4 ({sentence[4]}) is relevent to token at index = 4 ({sentence[4]}) = {relevance_4_4:3f}\")\n",
    "print(f\"> How much token at index = 4 ({sentence[4]}) is relevent to token at index = 3 ({sentence[3]}) = {relevance_4_3:3f}\")\n",
    "print(f\"> How much token at index = 4 ({sentence[4]}) is relevent to token at index = 2 ({sentence[2]}) = {relevance_4_2:3f}\")\n",
    "print(f\"> How much token at index = 4 ({sentence[4]}) is relevent to token at index = 1 ({sentence[1]}) = {relevance_4_1:3f}\")\n",
    "print(f\"> How much token at index = 4 ({sentence[4]}) is relevent to token at index = 0 ({sentence[0]}) = {relevance_4_0:3f}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(f\"For Token {sentence[3]}: \")\n",
    "print(f\"> How much token at index = 3 ({sentence[3]}) is relevent to token at index = 3 ({sentence[3]}) = {relevance_3_3:3f}\")\n",
    "print(f\"> How much token at index = 3 ({sentence[3]}) is relevent to token at index = 2 ({sentence[2]}) = {relevance_3_2:3f}\")\n",
    "print(f\"> How much token at index = 3 ({sentence[3]}) is relevent to token at index = 1 ({sentence[1]}) = {relevance_3_1:3f}\")\n",
    "print(f\"> How much token at index = 3 ({sentence[3]}) is relevent to token at index = 0 ({sentence[0]}) = {relevance_3_0:3f}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(f\"For Token {sentence[2]}: \")\n",
    "print(f\"> How much token at index = 2 ({sentence[2]}) is relevent to token at index = 2 ({sentence[2]}) = {relevance_2_2:3f}\")\n",
    "print(f\"> How much token at index = 2 ({sentence[2]}) is relevent to token at index = 1 ({sentence[1]}) = {relevance_2_1:3f}\")\n",
    "print(f\"> How much token at index = 2 ({sentence[2]}) is relevent to token at index = 0 ({sentence[0]}) = {relevance_2_0:3f}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(f\"For Token {sentence[1]}: \")\n",
    "print(f\"> How much token at index = 1 ({sentence[1]}) is relevent to token at index = 1 ({sentence[1]}) = {relevance_1_1:3f}\")\n",
    "print(f\"> How much token at index = 1 ({sentence[1]}) is relevent to token at index = 0 ({sentence[0]}) = {relevance_1_0:3f}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(f\"For Token {sentence[0]}: \")\n",
    "print(f\"> How much token at index = 2 ({sentence[0]}) is relevent to token at index = 0 ({sentence[0]}) = {relevance_0_0:3f}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(\"OR we can implicitly calculate this as, \")\n",
    "print(f\"query = (x_enc[0][:5] @ w_q)\")\n",
    "print(f\"key = (x_enc[0][:5] @ w_k)\")\n",
    "print(f\"scores = query @ key.T\")\n",
    "query = (x_enc[0][:5] @ w_q)\n",
    "key = (x_enc[0][:5] @ w_k)\n",
    "value = (x_enc[0][:5] @ w_v)\n",
    "scores = query @ key.T\n",
    "exp_scores = torch.exp(scores)\n",
    "normalized_score = exp_scores / exp_scores.sum(dim = 1, keepdim=True)\n",
    "Z = normalized_score @ value\n",
    "print(\"Now scores[i][j] denotes how much j is relevent to i\")\n",
    "print(f\"> How much token at index = 4 ({sentence[4]}) is relevent to token at index = 3 ({sentence[3]}) = {scores[4][3]:3f}\")\n",
    "\n",
    "normalized_score[3]\n",
    "arg_max = torch.argmax(normalized_score[3]).item()\n",
    "print(f\"> Sentence = `{sentence}`\")\n",
    "print(f\"> Token 3 = `{sentence[3]}`\")\n",
    "print(f\"> Token {arg_max} = `{sentence[arg_max]}`\")\n",
    "print(f\"normalized_score[3] = {normalized_score[3]}\")\n",
    "print(f\"> This means that for Token at index 3, This shows that how much token 3 cares about every other token\")\n",
    "print(f\"> How much token 3 cares about token 1? = normalized_score[3][1] = {normalized_score[3][1]:.4f}\")\n",
    "print(f\"> This means that token 3 cares mostly about Token {arg_max},\")\n",
    "print(f\"> Which essentially means that, Token 3 will use most of the information from Token {arg_max},\\n   but also uses information from rest of the Tokens, nothing is discarded completely (unless weight ≈ 0)\")\n",
    "print(f\"> How to use information from other tokens?,\\nwe use Value Matrix for that,\\nZ = normalized_score @ value \")\n",
    "print(f\"`normalized_score @ value` makes every token rewrite itself using information from all other tokens, weighted by relevance \")\n",
    "print(f\"> For Updating the representation of token 3, we would do this,\")\n",
    "print(f\"normalized_score[3] = {normalized_score[3]}\")\n",
    "print(f\"value = {value}\")\n",
    "z_3 = normalized_score[3] @ value\n",
    "print(f\"We will do, normalized_score[3] @ value = {normalized_score[3] @ value}, what does this means?\")\n",
    "print(f\"This is the new updated context aware representation of Token 3\")\n",
    "print(f\"> New Context Aware Representation for Token 3 (context-enriched embedding) = {z_3}\")\n",
    "print(f\"> Old Representation for Token 3 (original embedding (no context)) = {x_enc[0][3]}\")\n",
    "print(f\"NOTE: Notice how the new context-rich embedding lives in new 3d space? and Original embedding was in 2d space?\")\n",
    "print(f\"> The reason is: Old Representation for Token 3 lives in embedding space and n_embedding_dims = 2, \")\n",
    "print(f\"and New Context Aware Representation for Token 3 lives in Value space and n_values_dims = 3\")\n",
    "print(\"> Notice how, n_embedding_dims != n_values_dims? This is Architectural Design\")\n",
    "print(f\"> We can optionally project this 3d --> 2d or Value space --> embedding space, by just another linear Projection Layer\")\n",
    "print(f\"> If we want to use Stacked Layers then, the attention output must be mapped back to the embedding space. This is NOT OPTONAL ANYMORE\")\n",
    "print()\n",
    "print(\" ------------------------------------------------------------ Multi-head Attention --------------------------------------------------------------------------------- \")\n",
    "print(\"> But wait, in Language like 'English', etc there are many relationships we want to discover,\")\n",
    "print(\"instead of letting a single head to learn all possible relationship in 'n_embedding_dims' vector-space\")\n",
    "print(\"we project this input sequence into 'h' different sub-space of each 'n_embedding_dims/h' vector sub-spaces and in each of the 'h' sub-spaces\")\n",
    "print(\"> We allow the model to learn the relationships, learnable parameters and discoveries independently and they all operate on the same input sequence and then we concatenate them all\")\n",
    "print(\"> Benefit? Number of Parameters stays the same because after concatenation of 'h' different sub-spaces of each dim = n_embedding_dims/h\")\n",
    "print(\"we get 'n_embedding_dims/h * h' = 'n_embedding_dims' again, so number of parameters are unchanged and each of the different 'h' learned differet patterns and we concatenate them to preserve all learnable patterns\")\n",
    "print(\"> Conclusion: Multi-head attention works because it allows multiple independent relational views of the same sequence to coexist without interference, and preserves all of them through concatenation.\")\n",
    "print()\n",
    "print(\"------------------------------------------------------------ Positional Embeddings ------------------------------------------------------------\")\n",
    "print(\"But wait, even after learning all token-to-token dependencies, we ignored a fundamental property of language: position.\")\n",
    "print(\"Self-attention can learn relationships between words, but without positional information it has no notion of order.\")\n",
    "print(\"For example:\")\n",
    "print(\"S-1: The dog bit the man.\")\n",
    "print(\"S-2: The man bit the dog.\")\n",
    "print(\"Both sentences contain the same words, but their meanings are completely different due to word order.\")\n",
    "print(\"Without positional encodings, a self-attention model treats the input as a set, not a sequence, and cannot distinguish these cases.\")\n",
    "print(\"> Therefore, we must explicitly inject positional information so the model can learn relationships with respect to position.\")\n",
    "print(\"> Where injection happens\")\n",
    "print(\"> Given token embeddings: \")\n",
    "print(\">>> X ∈ R^{T × n_embedding_dims}\")\n",
    "print(\"> and positional encodings:\")\n",
    "print(\">>> P ∈ R^{T × d_model}\")\n",
    "print(\"we form, \")\n",
    "print(\"X_pos = X + P\")\n",
    "print(\"> All attention (Q, K, V) is computed from X_pos, not X.\")\n",
    "print(\"Q = X_pos @ W_q\")\n",
    "print(\"K = X_pos @ W_k\")\n",
    "print(\"V = X_pos @ W_v\")\n",
    "print(\"\\n> There are two common ways to do this:\")\n",
    "print(\"Method 1: Learned Positional Embeddings\")\n",
    "print(\"> Create an embedding table for positions\")\n",
    "print(\"> One vector per position index\")\n",
    "print(\">>> pos_embedding = nn.Embedding(max_len, n_embedding_dims)\")\n",
    "print(\">>> For a sequence of length T:\")\n",
    "print(\">>> positions = [0, 1, 2, ..., T-1]\")\n",
    "print(\">>> P = pos_embedding(positions)\")\n",
    "print(\">>> X_pos = X + P\")\n",
    "print(\"Method 2: Fixed (Sinusoidal) Positional Encodings\")\n",
    "print(\"Here, positions are not learned.\")\n",
    "print(\"For position pos and dimension i:\")\n",
    "print(\"P[pos, 2i]   = sin(pos / 10000^(2i / d_model))\")\n",
    "print(\"P[pos, 2i+1] = cos(pos / 10000^(2i / d_model))\")\n",
    "print(\"Then:\\nX_pos = X + P\")\n",
    "print(\"Now,\\n * Each position has a unique pattern\\n * Relative positions can be inferred via linear operations\\n * No extra learned parameters\")\n",
    "print(\"> Because sine and cosine allow linear layers to infer relative offsets between positions, even though the encoding itself is absolute.\")\n",
    "print(\"Example: \")\n",
    "# Method-1: Learned Positional Embeddings\n",
    "# Prepare the learned positional embeddings\n",
    "pos_embeddings = torch.rand(len(chars), n_embedding_dims) # each token will get a learned positional embeddings of 'n_embedding_dims' dimensions\n",
    "pos_idx = torch.arange(seq_length) # we encode the positions\n",
    "print(f\"Embeddings Shape: {embeddings.shape}\")\n",
    "print(f\"Positional Embeddings shape: {pos_embeddings.shape}\")\n",
    "P = pos_embeddings[pos_idx] # now this P contains the positional informations only\n",
    "x_enc = x_enc.squeeze(dim = 0) # remove the extra batch-dim or add the extra batch dim to 'P'\n",
    "x_pos = x_enc + P # Now this x_pos contains the positional information and learned encoding infromation as well\n",
    "print(\"we define: 'x_pos = x_enc + P' which contains the positional information as well\")\n",
    "print(f\"First 2 values of x_enc = {x_enc[:2]}\")\n",
    "print(f\"First 2 values of x_pos = {x_pos[:2]}\")\n",
    "print(\"> NOTE: Notice how `x_pos` is enriched with positional infromation as well?\")\n",
    "print(\"Note: Sinusoidal positional encodings are useful because shifting a position by Δ,\")\n",
    "print(\"      corresponds to a fixed linear transformation of its encoding, allowing attention layers to infer relative positions using only linear operations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f3591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
