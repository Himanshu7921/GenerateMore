{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3db42ba",
   "metadata": {},
   "source": [
    "# **LSTMs: From GRU Memory Limitation to Long-Term Memory Design**\n",
    "\n",
    "In a GRU, memory is carried by the hidden state $h_t$.\n",
    "The update rule is given by:\n",
    "\n",
    "$$h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t$$\n",
    "\n",
    "If we simplify this expression, we obtain:\n",
    "\n",
    "$$h_t = z_t (\\tilde{h}_t - h_{t-1}) + h_{t-1}$$\n",
    "\n",
    "Here, the **new memory** is $h_t$ and the **previous memory** is $h_{t-1}$.\n",
    "This can be interpreted as:\n",
    "\n",
    "$$\\text{new memory} = \\text{old memory} + \\text{some change}$$\n",
    "\n",
    "However, an important issue appears at this point.\n",
    "Even when the update gate is very small, for example $z_t = 0.00001$ (intuitively meaning “do not update the old memory”), the equation still produces:\n",
    "\n",
    "$$\\text{new memory} = 0.99999 \\cdot h_{t-1}$$\n",
    "\n",
    "This means that only **99.999%** of the previous memory is retained.\n",
    "After many time steps, this multiplicative decay compounds:\n",
    "\n",
    "$$(0.99999)^{100}$$\n",
    "\n",
    "As a result, some critical information is gradually lost.\n",
    "This explains why, for long sequences, the GRU design struggles to retain memory over very long time spans.\n",
    "\n",
    "Observing the equation again:\n",
    "\n",
    "$$h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t$$\n",
    "\n",
    "we see that **both memory removal and memory update are controlled by a single gate**, $z_t$.\n",
    "This coupled control motivates the design change introduced in LSTM, where these responsibilities are split for better manipulation of memory.\n",
    "\n",
    "---\n",
    "\n",
    "## **Solution: Redesigning Memory Update**\n",
    "\n",
    "The goal is to update memory such that:\n",
    "\n",
    "* **100% of the old memory can be retained**\n",
    "* the update is **linear**, so no forced decay of old information occurs\n",
    "* the flow of information remains fully controlled by the neural network\n",
    "\n",
    "The update must satisfy:\n",
    "\n",
    "$$\\text{new memory} = \\text{old memory} \\quad \\text{(must be possible)}$$\n",
    "\n",
    "and also allow:\n",
    "\n",
    "$$\\text{new memory} = \\text{old memory} + \\text{some information}$$\n",
    "\n",
    "where this new information depends on the current input and the current context.\n",
    "\n",
    "Introducing gating control, this formulation becomes:\n",
    "\n",
    "$$\\text{new memory} = f_t \\cdot \\text{old memory} + i_t \\cdot g_t$$\n",
    "\n",
    "This is the **long-term memory** formulation, answering what happened many time steps earlier.\n",
    "\n",
    "The **current hidden state** is then defined as:\n",
    "\n",
    "$$\\text{current hidden state} = o_t \\cdot \\tanh(\\text{new memory})$$\n",
    "\n",
    "which represents **short-term memory**, answering what happened most recently.\n",
    "\n",
    "---\n",
    "\n",
    "## **Gate Definitions and Roles**\n",
    "\n",
    "* $f_t$ → forget gate\n",
    "* $i_t$ → input gate\n",
    "* $g_t$ → candidate gate\n",
    "* $o_t$ → output gate\n",
    "\n",
    "The long-term memory is defined as:\n",
    "\n",
    "$$c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$$\n",
    "\n",
    "This guarantees that **100% retention of old memory is possible across any time step**, with a linear update mechanism.\n",
    "\n",
    "---\n",
    "\n",
    "### **Forget Gate**\n",
    "\n",
    "$$f_t = \\sigma(W x + U h_{t-1} + b)$$\n",
    "\n",
    "Given the current input $x$ and previous context $h_{t-1}$, this gate analyzes which information from $c_{t-1}$ should be kept.\n",
    "\n",
    "---\n",
    "\n",
    "### **Input Gate**\n",
    "\n",
    "$$i_t = \\sigma(W x + U h_{t-1} + b)$$\n",
    "\n",
    "This gate determines **how much new information** is added to the old memory.\n",
    "It only controls the **strength** of the update.\n",
    "The actual content to be written is decided by $g_t$.\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$i_t \\cdot g_t$$\n",
    "\n",
    "represents the update applied to the long-term memory.\n",
    "\n",
    "---\n",
    "\n",
    "### **Candidate Gate**\n",
    "\n",
    "$$g_t = \\tanh(W x + U h_{t-1} + b)$$\n",
    "\n",
    "Given the current input and previous context, this gate determines **what new information should be stored** in long-term memory.\n",
    "\n",
    "The use of `tanh` keeps the candidate memory **bounded and stable**, while the strength of the update is controlled by $i_t$.\n",
    "\n",
    "---\n",
    "\n",
    "## **Long-Term Memory Update**\n",
    "\n",
    "$$c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$$\n",
    "\n",
    "* $f_t$ → how much of the old memory to keep\n",
    "* $i_t$ → how much new information to add\n",
    "* $g_t$ → content of the new memory\n",
    "\n",
    "These two gates were a **single update gate in GRU**, where:\n",
    "\n",
    "* $(1 - z_t)$ controlled erasure\n",
    "* $z_t$ controlled update\n",
    "\n",
    "---\n",
    "\n",
    "## **From GRU to LSTM (Conceptual Mapping)**\n",
    "\n",
    "$$(1 - z_t) \\rightarrow f_t$$\n",
    "\n",
    "$$z_t \\rightarrow i_t$$\n",
    "\n",
    "$$h_{t-1} \\rightarrow c_{t-1} \\quad \\text{(long-term memory)}$$\n",
    "\n",
    "$$\\tilde{h}_t \\rightarrow g_t$$\n",
    "\n",
    "**Note:** This mapping is conceptual.\n",
    "LSTM allows **independent control** of retention and update, which GRU cannot.\n",
    "\n",
    "---\n",
    "\n",
    "## **Output Gate and Hidden State**\n",
    "\n",
    "$$o_t = \\sigma(W x + U h_{t-1} + b)$$\n",
    "\n",
    "Given the current input and recent context, this gate determines **how much of the long-term memory should be exposed**.\n",
    "\n",
    "The hidden state is then computed as:\n",
    "\n",
    "$$h_t = o_t \\cdot \\tanh(c_t)$$\n",
    "\n",
    "This represents **short-term memory** and is returned as the visible state of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de475223",
   "metadata": {},
   "source": [
    "### **Summary**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_t &= \\sigma(W_f x_t + U_f h_{t-1} + b_f) \\\\\n",
    "i_t &= \\sigma(W_i x_t + U_i h_{t-1} + b_i) \\\\\n",
    "g_t &= \\tanh(W_g x_t + U_g h_{t-1} + b_g) \\\\\n",
    "c_t &= f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "o_t &= \\sigma(W_o x_t + U_o h_{t-1} + b_o) \\\\\n",
    "h_t &= o_t \\odot \\tanh(c_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### **Notation**\n",
    "\n",
    "* $\\sigma(\\cdot)$: sigmoid activation  \n",
    "* $\\tanh(\\cdot)$: hyperbolic tangent  \n",
    "* $\\odot$: element-wise (Hadamard) product  \n",
    "* $W_*, U_*, b_*$: trainable parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
