{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3db42ba",
   "metadata": {},
   "source": [
    "# **LSTMs: From GRU Memory Limitation to Long-Term Memory Design**\n",
    "\n",
    "In a GRU, memory is carried by the hidden state $h_t$.\n",
    "The update rule is given by:\n",
    "\n",
    "$$h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t$$\n",
    "\n",
    "If we simplify this expression, we obtain:\n",
    "\n",
    "$$h_t = z_t (\\tilde{h}_t - h_{t-1}) + h_{t-1}$$\n",
    "\n",
    "Here, the **new memory** is $h_t$ and the **previous memory** is $h_{t-1}$.\n",
    "This can be interpreted as:\n",
    "\n",
    "$$\\text{new memory} = \\text{old memory} + \\text{some change}$$\n",
    "\n",
    "However, an important issue appears at this point.\n",
    "Even when the update gate is very small, for example $z_t = 0.00001$ (intuitively meaning “do not update the old memory”), the equation still produces:\n",
    "\n",
    "$$\\text{new memory} = 0.99999 \\cdot h_{t-1}$$\n",
    "\n",
    "This means that only **99.999%** of the previous memory is retained.\n",
    "After many time steps, this multiplicative decay compounds:\n",
    "\n",
    "$$(0.99999)^{100}$$\n",
    "\n",
    "As a result, some critical information is gradually lost.\n",
    "This explains why, for long sequences, the GRU design struggles to retain memory over very long time spans.\n",
    "\n",
    "Observing the equation again:\n",
    "\n",
    "$$h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t$$\n",
    "\n",
    "we see that **both memory removal and memory update are controlled by a single gate**, $z_t$.\n",
    "This coupled control motivates the design change introduced in LSTM, where these responsibilities are split for better manipulation of memory.\n",
    "\n",
    "---\n",
    "\n",
    "## **Solution: Redesigning Memory Update**\n",
    "\n",
    "The goal is to update memory such that:\n",
    "\n",
    "* **100% of the old memory can be retained**\n",
    "* the update is **linear**, so no forced decay of old information occurs\n",
    "* the flow of information remains fully controlled by the neural network\n",
    "\n",
    "The update must satisfy:\n",
    "\n",
    "$$\\text{new memory} = \\text{old memory} \\quad \\text{(must be possible)}$$\n",
    "\n",
    "and also allow:\n",
    "\n",
    "$$\\text{new memory} = \\text{old memory} + \\text{some information}$$\n",
    "\n",
    "where this new information depends on the current input and the current context.\n",
    "\n",
    "Introducing gating control, this formulation becomes:\n",
    "\n",
    "$$\\text{new memory} = f_t \\cdot \\text{old memory} + i_t \\cdot g_t$$\n",
    "\n",
    "This is the **long-term memory** formulation, answering what happened many time steps earlier.\n",
    "\n",
    "The **current hidden state** is then defined as:\n",
    "\n",
    "$$\\text{current hidden state} = o_t \\cdot \\tanh(\\text{new memory})$$\n",
    "\n",
    "which represents **short-term memory**, answering what happened most recently.\n",
    "\n",
    "---\n",
    "\n",
    "## **Gate Definitions and Roles**\n",
    "\n",
    "* $f_t$ → forget gate\n",
    "* $i_t$ → input gate\n",
    "* $g_t$ → candidate gate\n",
    "* $o_t$ → output gate\n",
    "\n",
    "The long-term memory is defined as:\n",
    "\n",
    "$$c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$$\n",
    "\n",
    "This guarantees that **100% retention of old memory is possible across any time step**, with a linear update mechanism.\n",
    "\n",
    "---\n",
    "\n",
    "### **Forget Gate**\n",
    "\n",
    "$$f_t = \\sigma(W x + U h_{t-1} + b)$$\n",
    "\n",
    "Given the current input $x$ and previous context $h_{t-1}$, this gate analyzes which information from $c_{t-1}$ should be kept.\n",
    "\n",
    "---\n",
    "\n",
    "### **Input Gate**\n",
    "\n",
    "$$i_t = \\sigma(W x + U h_{t-1} + b)$$\n",
    "\n",
    "This gate determines **how much new information** is added to the old memory.\n",
    "It only controls the **strength** of the update.\n",
    "The actual content to be written is decided by $g_t$.\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$i_t \\cdot g_t$$\n",
    "\n",
    "represents the update applied to the long-term memory.\n",
    "\n",
    "---\n",
    "\n",
    "### **Candidate Gate**\n",
    "\n",
    "$$g_t = \\tanh(W x + U h_{t-1} + b)$$\n",
    "\n",
    "Given the current input and previous context, this gate determines **what new information should be stored** in long-term memory.\n",
    "\n",
    "The use of `tanh` keeps the candidate memory **bounded and stable**, while the strength of the update is controlled by $i_t$.\n",
    "\n",
    "---\n",
    "\n",
    "## **Long-Term Memory Update**\n",
    "\n",
    "$$c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$$\n",
    "\n",
    "* $f_t$ → how much of the old memory to keep\n",
    "* $i_t$ → how much new information to add\n",
    "* $g_t$ → content of the new memory\n",
    "\n",
    "These two gates were a **single update gate in GRU**, where:\n",
    "\n",
    "* $(1 - z_t)$ controlled erasure\n",
    "* $z_t$ controlled update\n",
    "\n",
    "---\n",
    "\n",
    "## **From GRU to LSTM (Conceptual Mapping)**\n",
    "\n",
    "$$(1 - z_t) \\rightarrow f_t$$\n",
    "\n",
    "$$z_t \\rightarrow i_t$$\n",
    "\n",
    "$$h_{t-1} \\rightarrow c_{t-1} \\quad \\text{(long-term memory)}$$\n",
    "\n",
    "$$\\tilde{h}_t \\rightarrow g_t$$\n",
    "\n",
    "**Note:** This mapping is conceptual.\n",
    "LSTM allows **independent control** of retention and update, which GRU cannot.\n",
    "\n",
    "---\n",
    "\n",
    "## **Output Gate and Hidden State**\n",
    "\n",
    "$$o_t = \\sigma(W x + U h_{t-1} + b)$$\n",
    "\n",
    "Given the current input and recent context, this gate determines **how much of the long-term memory should be exposed**.\n",
    "\n",
    "The hidden state is then computed as:\n",
    "\n",
    "$$h_t = o_t \\cdot \\tanh(c_t)$$\n",
    "\n",
    "This represents **short-term memory** and is returned as the visible state of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de475223",
   "metadata": {},
   "source": [
    "### **Summary**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_t &= \\sigma(W_f x_t + U_f h_{t-1} + b_f) \\\\\n",
    "i_t &= \\sigma(W_i x_t + U_i h_{t-1} + b_i) \\\\\n",
    "g_t &= \\tanh(W_g x_t + U_g h_{t-1} + b_g) \\\\\n",
    "c_t &= f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "o_t &= \\sigma(W_o x_t + U_o h_{t-1} + b_o) \\\\\n",
    "h_t &= o_t \\odot \\tanh(c_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### **Notation**\n",
    "\n",
    "* $\\sigma(\\cdot)$: sigmoid activation  \n",
    "* $\\tanh(\\cdot)$: hyperbolic tangent  \n",
    "* $\\odot$: element-wise (Hadamard) product  \n",
    "* $W_*, U_*, b_*$: trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055490f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ff2bc70",
   "metadata": {},
   "source": [
    "### Building an LSTM From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819680a1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "f_t &= \\sigma(W_f x_t + U_f h_{t-1} + b_f) \\\\\n",
    "i_t &= \\sigma(W_i x_t + U_i h_{t-1} + b_i) \\\\\n",
    "g_t &= \\tanh(W_g x_t + U_g h_{t-1} + b_g) \\\\\n",
    "c_t &= f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "o_t &= \\sigma(W_o x_t + U_o h_{t-1} + b_o) \\\\\n",
    "h_t &= o_t \\odot \\tanh(c_t)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a81cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db65792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \"\"\" \n",
    "    This Implementation is Differ from the PyTorch's Official Implementation\n",
    "    NOTE:\n",
    "        This is not the official Implementation of PyTorch's GRUcell Because they use 2 biases per Gate\n",
    "        and i'm only using 1 bias per Gate\n",
    "\n",
    "        PyTorch's Official Implementation: (2 biases per gate: input bias + hidden bias)\n",
    "        # f_t = σ(W_if x_t + b_if + W_hf h_{t-1} + b_hf)\n",
    "        # i_t = σ(W_ii x_t + b_ii + W_hi h_{t-1} + b_hi)\n",
    "        # g_t = tanh(W_ig x_t + b_ig + W_hg h_{t-1} + b_hg)\n",
    "        # c_t = f_t ⊙ c_{t-1} + i_t ⊙ g_t\n",
    "        # o_t = σ(W_io x_t + b_io + W_ho h_{t-1} + b_ho)\n",
    "        # h_t = o_t ⊙ tanh(c_t)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_dim):\n",
    "        \"\"\"\n",
    "        This Cell is responsible for computing the forward pass of single LSTM call\n",
    "        Stack this into Layers to get a working LSTMLayers\n",
    "\n",
    "        We have total of 3 Gates + 1 candidate state\n",
    "\n",
    "        # Forget gate\n",
    "        # f_t = σ(W_f x_t + U_f h_{t-1} + b_f)\n",
    "\n",
    "        # Input gate\n",
    "        # i_t = σ(W_i x_t + U_i h_{t-1} + b_i)\n",
    "\n",
    "        # Candidate memory (candidate state)\n",
    "        # g_t = tanh(W_g x_t + U_g h_{t-1} + b_g)\n",
    "\n",
    "        # Cell state (long-term memory)\n",
    "        # c_t = f_t ⊙ c_{t-1} + i_t ⊙ g_t\n",
    "\n",
    "        # Output gate\n",
    "        # o_t = σ(W_o x_t + U_o h_{t-1} + b_o)\n",
    "\n",
    "        # Hidden state (short-term memory)\n",
    "        # h_t = o_t ⊙ tanh(c_t)\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Forget Gate\n",
    "        self.W_f_x = nn.Linear(in_features = self.in_features, out_features = self.hidden_dim, bias = False)\n",
    "        self.W_f_h = nn.Linear(in_features = self.hidden_dim, out_features = self.hidden_dim, bias = False)\n",
    "        self.b_f = nn.Parameter(torch.ones(self.hidden_dim))\n",
    "\n",
    "        # Input Gate\n",
    "        self.W_i_x = nn.Linear(in_features = self.in_features, out_features = self.hidden_dim, bias = False)\n",
    "        self.W_i_h = nn.Linear(in_features = self.hidden_dim, out_features = self.hidden_dim, bias = False)\n",
    "        self.b_i = nn.Parameter(torch.zeros(self.hidden_dim))\n",
    "\n",
    "        # Output Gate\n",
    "        self.W_o_x = nn.Linear(in_features = self.in_features, out_features = self.hidden_dim, bias = False)\n",
    "        self.W_o_h = nn.Linear(in_features = self.hidden_dim, out_features = self.hidden_dim, bias = False)\n",
    "        self.b_o = nn.Parameter(torch.zeros(self.hidden_dim))\n",
    "\n",
    "        # Candidate State\n",
    "        self.W_g_x = nn.Linear(in_features = self.in_features, out_features = self.hidden_dim, bias = False)\n",
    "        self.W_g_h = nn.Linear(in_features = self.hidden_dim, out_features = self.hidden_dim, bias = False)\n",
    "        self.b_g = nn.Parameter(torch.zeros(self.hidden_dim))\n",
    "    \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        # Calculate candidate cell state\n",
    "        g_t = torch.tanh(self.W_g_x(x) + self.W_g_h(h_prev) + self.b_g)\n",
    "        i_t = torch.sigmoid(self.W_i_x(x) + self.W_i_h(h_prev) + self.b_i)\n",
    "        f_t = torch.sigmoid(self.W_f_x(x)  +self.W_f_h(h_prev) + self.b_f)\n",
    "        c_t = f_t * c_prev + i_t * g_t # Long Term Memory\n",
    "        o_t = torch.sigmoid(self.W_o_x(x) + self.W_o_h(h_prev) + self.b_o)\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        return h_t, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c047f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, n_layers: int):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(\n",
    "                LSTMCell(\n",
    "                    in_features = self.in_features if i == 0 else self.hidden_dim,\n",
    "                    hidden_dim = self.hidden_dim\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x, h_prev = None, c_prev = None):\n",
    "        batch_dim, seq_length, _ = x.shape # x.shape = batch_dim, seq_length, embd_dim\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(self.n_layers, batch_dim, self.hidden_dim, device = x.device)\n",
    "        if c_prev is None:\n",
    "            c_prev = torch.zeros(self.n_layers, batch_dim, self.hidden_dim, device = x.device)\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :]\n",
    "            new_h = []\n",
    "            new_c = []\n",
    "            for layer_idx, cell in enumerate(self.layers):\n",
    "                h, c = cell(x_t, h_prev[layer_idx], c_prev[layer_idx])\n",
    "                new_h.append(h)\n",
    "                new_c.append(c)\n",
    "                x_t = h\n",
    "            \n",
    "            h_prev = torch.stack(new_h, dim=0)\n",
    "            c_prev = torch.stack(new_c, dim=0)\n",
    "            outputs.append(x_t)\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs, (h_prev, c_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d6c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.linear_projection = nn.Linear(\n",
    "            in_features=self.in_features,\n",
    "            out_features=self.out_features,\n",
    "            bias=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_projection(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23249be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMModel(nn.Module):\n",
    "    def __init__(self, model_name: str, n_layers: int, vocab_size: int, in_features: int, hidden_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.vocab_size = vocab_size\n",
    "        self.in_features = in_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.linear_projection = LinearLayer(in_features = self.hidden_dim, out_features = self.vocab_size)\n",
    "        self.lstm_layer = LSTMLayer(\n",
    "            in_features = in_features,\n",
    "            hidden_dim = hidden_dim,\n",
    "            n_layers = n_layers\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(self.hidden_dim)\n",
    "        \n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.in_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, h_prev = None, c_prev = None):\n",
    "        x_embd = self.embeddings(x) # ---> shape = (batch, seq_length, embd_dim)\n",
    "        outputs, (h_n, c_n) = self.lstm_layer(x = x_embd,\n",
    "                                h_prev = h_prev,\n",
    "                                c_prev = c_prev\n",
    "        )\n",
    "        last_layer_output = outputs # for Language Modeling we are only interested in last layer's output,\n",
    "                                    # its like saying: \"Give me the most processed understanding of the sequence.\"\"\n",
    "        last_layer_out = self.dropout(last_layer_output)\n",
    "        last_layer_out_norm = self.layer_norm(last_layer_out)\n",
    "        logits = self.linear_projection(last_layer_out_norm)\n",
    "        return logits, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc30955",
   "metadata": {},
   "source": [
    "## Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e006071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "text = open(\"tiny_shakespeare.txt\", 'r', encoding='utf-8').read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "\n",
    "data = torch.tensor([stoi[c] for c in text], dtype=torch.long)\n",
    "seq_length = 128  # sequence length\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split='train', batch_size=64):\n",
    "    source = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(source) - seq_length - 1, (batch_size,))\n",
    "    X = torch.stack([source[i:i+seq_length] for i in ix])\n",
    "    Y = torch.stack([source[i+1:i+seq_length+1] for i in ix])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa96cf9",
   "metadata": {},
   "source": [
    "## Function for Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a4fd244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def train_model(\n",
    "        model: MyLSTMModel,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss_fn,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        device,\n",
    "        clip_value=1.0,\n",
    "        val_interval=1,\n",
    "        steps = 200\n",
    "    ):\n",
    "    \n",
    "    print(f\"\\n---------------- Training Started for {model.model_name} Model ----------------\\n\")\n",
    "    # steps ---> How many batches will get involve in forwardpass and backward pass\n",
    "    # steps = 200, and batch_size = 64 meaning 200 batches of each size = 64 will get involved in forwardpass and backward pass\n",
    "    # 200 * 64 * seq_length = 200 * 64 * 128 = 1.64M tokens/epoch for forward pass and 1.46M token/epoch for backward pass\n",
    "    # so for larger models, keep larger steps, for Astra-gamma step = 400 \n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for _ in range(steps):\n",
    "            X, Y = get_batch(split=\"train\", batch_size=batch_size)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, _ = model(X)\n",
    "            loss = loss_fn(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                Y.reshape(-1)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= steps\n",
    "\n",
    "        # Validation\n",
    "        val_loss = None\n",
    "        if epoch % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Xv, Yv = get_batch(split=\"val\", batch_size=batch_size)\n",
    "                Xv, Yv = Xv.to(device), Yv.to(device)\n",
    "\n",
    "                logits, _ = model(Xv)\n",
    "                val_loss = loss_fn(\n",
    "                    logits.reshape(-1, logits.size(-1)),\n",
    "                    Yv.reshape(-1)\n",
    "                ).item()\n",
    "\n",
    "        # Lr Scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Epoch and Loss Details\n",
    "        if val_loss is not None:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\n---------------- Training Completed for {model.model_name} Model ----------------\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e775ab0b",
   "metadata": {},
   "source": [
    "## Sampling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aed8898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_greedy(model, stoi, itos, start_text=\"A\", max_new_tokens=200):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_ids = torch.tensor([stoi[c] for c in start_text], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    h, c = None, None\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, (h, c) = model(input_ids[:, -1:], h, c)\n",
    "        h, c = h.detach(), c.detach()\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "        input_ids = torch.cat([input_ids, next_id.unsqueeze(1)], dim=1)\n",
    "\n",
    "    return ''.join(itos[i] for i in input_ids[0].tolist())\n",
    "\n",
    "\n",
    "\n",
    "def sample_with_temperature(model, stoi, itos, start_text=\"A\", max_new_tokens=200, temperature=1.0):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_ids = torch.tensor([stoi[c] for c in start_text], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    h, c = None, None\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, (h, c) = model(input_ids[:, -1:], h, c)\n",
    "        h, c = h.detach(), c.detach()\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids = torch.cat([input_ids, next_id], dim=1)\n",
    "\n",
    "    return ''.join(itos[i] for i in input_ids[0].tolist())\n",
    "\n",
    "\n",
    "\n",
    "def sample_top_k(model, stoi, itos, start_text=\"A\", max_new_tokens=200, k=20, temperature=1.0):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_ids = torch.tensor([stoi[c] for c in start_text], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    h, c = None, None\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, (h, c) = model(input_ids[:, -1:], h, c)\n",
    "        h, c = h.detach(), c.detach()\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        topk_vals, topk_idx = torch.topk(logits, k)\n",
    "        probs = F.softmax(topk_vals, dim=-1)\n",
    "        sampled_idx = torch.multinomial(probs, num_samples=1)\n",
    "        next_id = topk_idx.gather(-1, sampled_idx)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_id], dim=1)\n",
    "\n",
    "    return ''.join(itos[i] for i in input_ids[0].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3011de1b",
   "metadata": {},
   "source": [
    "## Function for Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "785c96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def __init__(self, model_name: str, n_layers: int, vocab_size: int, in_features: int, hidden_dim: int, dropout: float):\n",
    "    #     super().__init__()\n",
    "    #     self.model_name = model_name\n",
    "    #     self.vocab_size = vocab_size\n",
    "    #     self.in_features = in_features\n",
    "    #     self.hidden_dim = hidden_dim\n",
    "    #     self.n_layers = n_layers\n",
    "    #     self.linear_projection = LinearLayer(in_features = self.hidden_dim, out_features = self.vocab_size)\n",
    "    #     self.lstm_layer = LSTMLayer(\n",
    "    #         in_features = in_features,\n",
    "    #         hidden_dim = hidden_dim,\n",
    "    #         n_layers = n_layers\n",
    "    #     )\n",
    "    #     self.layer_norm = nn.LayerNorm(self.hidden_dim)\n",
    "        \n",
    "    #     self.embeddings = nn.Embedding(self.vocab_size, self.in_features)\n",
    "    #     self.dropout = nn.Dropout(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c3f4ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model(model: MyLSTMModel, base_name=\"leviathan\", path=\"./leviathan_saved_model/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # versioning\n",
    "    existing = [f for f in os.listdir(path) if f.startswith(base_name) and f.endswith(\".pth\")]\n",
    "    versions = []\n",
    "    for f in existing:\n",
    "        parts = f.replace(\".pth\", \"\").split(\"_v\")\n",
    "        if len(parts) == 2 and parts[1].isdigit():\n",
    "            versions.append(int(parts[1]))\n",
    "    next_version = max(versions, default=0) + 1\n",
    "\n",
    "    filename = f\"{base_name}_v{next_version}.pth\"\n",
    "    save_path = os.path.join(path, filename)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"model_class\": model.__class__.__name__,\n",
    "        \"model_name\": model.model_name,\n",
    "        \"in_features\": model.in_features,\n",
    "        \"hidden_dim\": model.hidden_dim,\n",
    "        \"dropout\": model.dropout.p,\n",
    "        \"vocab_size\": model.vocab_size,\n",
    "        \"n_layers\": model.n_layers,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"version\": next_version,\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, save_path)\n",
    "    torch.save(checkpoint, os.path.join(path, f\"{base_name}_latest.pth\"))\n",
    "\n",
    "    print(f\"\\nModel saved at: {save_path}\")\n",
    "    print(f\"Also updated: {base_name}_latest.pth\\n\")\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781511ad",
   "metadata": {},
   "source": [
    "## Function for Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aedc5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def load_model(filepath, device):\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "\n",
    "    # extract architecture parameters from checkpoint\n",
    "    model_name  = checkpoint[\"model_name\"]\n",
    "    vocab_size  = checkpoint[\"vocab_size\"]\n",
    "    in_features    = checkpoint[\"in_features\"]\n",
    "    hidden_dim  = checkpoint[\"hidden_dim\"]\n",
    "    dropout     = checkpoint[\"dropout\"]\n",
    "    n_layers     = checkpoint[\"n_layers\"]\n",
    "\n",
    "    # print(\"------------------------------- dropout = \", dropout)\n",
    "\n",
    "    # instantiate model using all saved metadata\n",
    "    model = MyLSTMModel(\n",
    "        model_name = model_name,\n",
    "        n_layers = n_layers,\n",
    "        vocab_size = vocab_size,\n",
    "        in_features = in_features,\n",
    "        hidden_dim = hidden_dim,\n",
    "        dropout    = dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # load weights\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "    # pretty print metadata\n",
    "    print(\"\\n================ MODEL LOADED ================\")\n",
    "    print(f\"Loaded File      : {filepath}\")\n",
    "    print(f\"Model Name       : {model_name}\")\n",
    "    print(f\"Model Class      : {checkpoint['model_class']}\")\n",
    "    print(f\"Version          : v{checkpoint['version']}\")\n",
    "    print(f\"Timestamp        : {checkpoint['timestamp']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(\"Model Architecture:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if name != \"\":\n",
    "            print(f\"  └── {name}: {module.__class__.__name__}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(f\"Total Parameters : {count_parameters(model):,}\")\n",
    "    print(f\"Loaded on Device : {device}\")\n",
    "    print(\"==============================================\\n\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d7321",
   "metadata": {},
   "source": [
    "## Function for Printing the Sumamry of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3166bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_summary(model, model_name, epochs, lr, device):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Leviathan-LSTM MODEL SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Model Name       : {model_name}\")\n",
    "    print(f\"Device           : {device}\")\n",
    "    print(f\"Total Epochs     : {epochs}\")\n",
    "    print(f\"Learning Rate    : {lr}\")\n",
    "\n",
    "    print(\"\\nMODEL ARCHITECTURE\")\n",
    "    print(\"-\"*100)\n",
    "    for name, module in model.named_modules():\n",
    "        if name == \"\":\n",
    "            continue\n",
    "        print(f\"  └── {name}: {module.__class__.__name__}()\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model_size_mb = n_params * 4 / (1024**2)\n",
    "\n",
    "    print(f\"\\nTrainable Parameters : {n_params:,}\")\n",
    "    print(f\"Model Size : {model_size_mb:.2f} MB\")\n",
    "\n",
    "    print(\"\\nPARAMETER BREAKDOWN\")\n",
    "    print(\"-\"*100)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name:40s} : {param.numel():,}\")\n",
    "\n",
    "    print(\"=\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75406a76",
   "metadata": {},
   "source": [
    "## Now lets define the architecture and train our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b92483c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------- Define Leviathan-LSTM Architecture  -----------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embd_dim = 768\n",
    "hidden_dim = 1024\n",
    "model_name = \"Leviathan-LSTM\"\n",
    "\n",
    "leviathan_model = MyLSTMModel(\n",
    "    model_name = model_name,\n",
    "    n_layers = 3,\n",
    "    vocab_size = len(stoi),\n",
    "    in_features = embd_dim,\n",
    "    hidden_dim = hidden_dim,\n",
    "    dropout = 0.2\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------- Training Hyperparameters  --------------------------------------------------\n",
    "leviathan_model_lr = 1e-3\n",
    "epochs_leviathan_model = 6\n",
    "weight_decay_leviathan_model = 0.01\n",
    "\n",
    "optimizer_large = torch.optim.AdamW(\n",
    "    leviathan_model.parameters(),\n",
    "    lr = leviathan_model_lr,\n",
    "    weight_decay = weight_decay_leviathan_model\n",
    ")\n",
    "\n",
    "# CosineAnnealingLR scheduler: cleaner convergence & better text quality\n",
    "scheduler_large = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_large,\n",
    "    T_max = epochs_leviathan_model\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44188f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "Leviathan-LSTM MODEL SUMMARY\n",
      "====================================================================================================\n",
      "Model Name       : Leviathan-LSTM\n",
      "Device           : cuda\n",
      "Total Epochs     : 6\n",
      "Learning Rate    : 0.001\n",
      "\n",
      "MODEL ARCHITECTURE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  └── linear_projection: LinearLayer()\n",
      "  └── linear_projection.linear_projection: Linear()\n",
      "  └── lstm_layer: LSTMLayer()\n",
      "  └── lstm_layer.layers: ModuleList()\n",
      "  └── lstm_layer.layers.0: LSTMCell()\n",
      "  └── lstm_layer.layers.0.W_f_x: Linear()\n",
      "  └── lstm_layer.layers.0.W_f_h: Linear()\n",
      "  └── lstm_layer.layers.0.W_i_x: Linear()\n",
      "  └── lstm_layer.layers.0.W_i_h: Linear()\n",
      "  └── lstm_layer.layers.0.W_o_x: Linear()\n",
      "  └── lstm_layer.layers.0.W_o_h: Linear()\n",
      "  └── lstm_layer.layers.0.W_g_x: Linear()\n",
      "  └── lstm_layer.layers.0.W_g_h: Linear()\n",
      "  └── lstm_layer.layers.1: LSTMCell()\n",
      "  └── lstm_layer.layers.1.W_f_x: Linear()\n",
      "  └── lstm_layer.layers.1.W_f_h: Linear()\n",
      "  └── lstm_layer.layers.1.W_i_x: Linear()\n",
      "  └── lstm_layer.layers.1.W_i_h: Linear()\n",
      "  └── lstm_layer.layers.1.W_o_x: Linear()\n",
      "  └── lstm_layer.layers.1.W_o_h: Linear()\n",
      "  └── lstm_layer.layers.1.W_g_x: Linear()\n",
      "  └── lstm_layer.layers.1.W_g_h: Linear()\n",
      "  └── lstm_layer.layers.2: LSTMCell()\n",
      "  └── lstm_layer.layers.2.W_f_x: Linear()\n",
      "  └── lstm_layer.layers.2.W_f_h: Linear()\n",
      "  └── lstm_layer.layers.2.W_i_x: Linear()\n",
      "  └── lstm_layer.layers.2.W_i_h: Linear()\n",
      "  └── lstm_layer.layers.2.W_o_x: Linear()\n",
      "  └── lstm_layer.layers.2.W_o_h: Linear()\n",
      "  └── lstm_layer.layers.2.W_g_x: Linear()\n",
      "  └── lstm_layer.layers.2.W_g_h: Linear()\n",
      "  └── layer_norm: LayerNorm()\n",
      "  └── embeddings: Embedding()\n",
      "  └── dropout: Dropout()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trainable Parameters : 24,248,129\n",
      "Model Size : 92.50 MB\n",
      "\n",
      "PARAMETER BREAKDOWN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "linear_projection.linear_projection.weight : 66,560\n",
      "linear_projection.linear_projection.bias : 65\n",
      "lstm_layer.layers.0.b_f                  : 1,024\n",
      "lstm_layer.layers.0.b_i                  : 1,024\n",
      "lstm_layer.layers.0.b_o                  : 1,024\n",
      "lstm_layer.layers.0.b_g                  : 1,024\n",
      "lstm_layer.layers.0.W_f_x.weight         : 786,432\n",
      "lstm_layer.layers.0.W_f_h.weight         : 1,048,576\n",
      "lstm_layer.layers.0.W_i_x.weight         : 786,432\n",
      "lstm_layer.layers.0.W_i_h.weight         : 1,048,576\n",
      "lstm_layer.layers.0.W_o_x.weight         : 786,432\n",
      "lstm_layer.layers.0.W_o_h.weight         : 1,048,576\n",
      "lstm_layer.layers.0.W_g_x.weight         : 786,432\n",
      "lstm_layer.layers.0.W_g_h.weight         : 1,048,576\n",
      "lstm_layer.layers.1.b_f                  : 1,024\n",
      "lstm_layer.layers.1.b_i                  : 1,024\n",
      "lstm_layer.layers.1.b_o                  : 1,024\n",
      "lstm_layer.layers.1.b_g                  : 1,024\n",
      "lstm_layer.layers.1.W_f_x.weight         : 1,048,576\n",
      "lstm_layer.layers.1.W_f_h.weight         : 1,048,576\n",
      "lstm_layer.layers.1.W_i_x.weight         : 1,048,576\n",
      "lstm_layer.layers.1.W_i_h.weight         : 1,048,576\n",
      "lstm_layer.layers.1.W_o_x.weight         : 1,048,576\n",
      "lstm_layer.layers.1.W_o_h.weight         : 1,048,576\n",
      "lstm_layer.layers.1.W_g_x.weight         : 1,048,576\n",
      "lstm_layer.layers.1.W_g_h.weight         : 1,048,576\n",
      "lstm_layer.layers.2.b_f                  : 1,024\n",
      "lstm_layer.layers.2.b_i                  : 1,024\n",
      "lstm_layer.layers.2.b_o                  : 1,024\n",
      "lstm_layer.layers.2.b_g                  : 1,024\n",
      "lstm_layer.layers.2.W_f_x.weight         : 1,048,576\n",
      "lstm_layer.layers.2.W_f_h.weight         : 1,048,576\n",
      "lstm_layer.layers.2.W_i_x.weight         : 1,048,576\n",
      "lstm_layer.layers.2.W_i_h.weight         : 1,048,576\n",
      "lstm_layer.layers.2.W_o_x.weight         : 1,048,576\n",
      "lstm_layer.layers.2.W_o_h.weight         : 1,048,576\n",
      "lstm_layer.layers.2.W_g_x.weight         : 1,048,576\n",
      "lstm_layer.layers.2.W_g_h.weight         : 1,048,576\n",
      "layer_norm.weight                        : 1,024\n",
      "layer_norm.bias                          : 1,024\n",
      "embeddings.weight                        : 49,920\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_model_summary(model = leviathan_model,\n",
    "                    model_name = model_name,\n",
    "                    epochs = epochs_leviathan_model,\n",
    "                    lr = leviathan_model_lr,\n",
    "                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b931525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- Training Started for Leviathan-LSTM Model ----------------\n",
      "\n",
      "Epoch 01/6 | Train Loss: 2.4545 | Val Loss: 1.7801\n",
      "Epoch 02/6 | Train Loss: 1.5172 | Val Loss: 1.7129\n",
      "Epoch 03/6 | Train Loss: 1.3635 | Val Loss: 1.6001\n",
      "Epoch 04/6 | Train Loss: 1.2862 | Val Loss: 1.6344\n",
      "Epoch 05/6 | Train Loss: 1.2302 | Val Loss: 1.5539\n",
      "Epoch 06/6 | Train Loss: 1.1935 | Val Loss: 1.4981\n",
      "\n",
      "---------------- Training Completed for Leviathan-LSTM Model ----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Train Leviathan-LSTM ----------------\n",
    "\n",
    "trained_leviathan_model = train_model(\n",
    "    model = leviathan_model,\n",
    "    optimizer = optimizer_large,\n",
    "    scheduler = scheduler_large,\n",
    "    loss_fn = loss_fn,\n",
    "    epochs = epochs_leviathan_model,\n",
    "    batch_size = 64,\n",
    "    device = device,\n",
    "    clip_value = 1.0,\n",
    "    val_interval = 1,\n",
    "    steps = 200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb983af6",
   "metadata": {},
   "source": [
    "## Sampling from the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68075ba5",
   "metadata": {},
   "source": [
    "### Greedy Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf2d4877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Greedy Sampling -------- \n",
      "And then the sea that we have seen the seas\n",
      "And see the sea to the sea to the season of the sea\n",
      "That thou hast so stand to the sea to the seas,\n",
      "And then the sea that the state of the season,\n",
      "And then the sea that the state of the season,\n",
      "And then the sea that the state of the season,\n",
      "And then the sea\n"
     ]
    }
   ],
   "source": [
    "greedy_text = sample_greedy(\n",
    "    model = trained_leviathan_model,\n",
    "    stoi = stoi,\n",
    "    itos = itos,\n",
    "    start_text = \"A\",\n",
    "    max_new_tokens = 300\n",
    ")\n",
    "\n",
    "print(\"-------- Greedy Sampling -------- \")\n",
    "print(greedy_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe279df1",
   "metadata": {},
   "source": [
    "### Temperature Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88a241b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Temperature Sampling (0.8) -------- \n",
      "And then we have a son a friar.\n",
      "\n",
      "AUTOLYCUS:\n",
      "I am too: move have from me after, some refuge unto\n",
      "the cause of me of brother.\n",
      "\n",
      "KING EDWARD IV:\n",
      "What send the world hath still she devil that royalty.\n",
      "\n",
      "PAULINA:\n",
      "No; for I have say, I do see him; that he\n",
      "weeping made them at your father's good much.\n",
      "\n",
      "KING E\n"
     ]
    }
   ],
   "source": [
    "temp_text = sample_with_temperature(\n",
    "    model = trained_leviathan_model,\n",
    "    stoi = stoi,\n",
    "    itos = itos,\n",
    "    start_text = \"A\",\n",
    "    max_new_tokens = 300,\n",
    "    temperature = 0.8\n",
    ")\n",
    "\n",
    "print(\"-------- Temperature Sampling (0.8) -------- \")\n",
    "print(temp_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f9dbd",
   "metadata": {},
   "source": [
    "### Top-k Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7b96598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Top-K Sampling (k=20, temp=0.8) -------- \n",
      "A:\n",
      "And look to have. I may be looked in this part\n",
      "Of trumpets found to search the least better men\n",
      "That stout of thy sword of them; those are forseal\n",
      "Shall be the air charges. A father hath\n",
      "have wound the customer together, though undertake them;\n",
      "But she be as if you have not mocked forewarr\n",
      "Since it be abstance, boy.\n",
      "\n",
      "POLIXENES:\n",
      "It is a master again of my dearest,\n",
      "At your eyes like to please your most power.\n",
      "\n",
      "CORIOLANUS:\n",
      "And by the which we learn'd the state forfect these\n",
      "new with eyes to daughter'd your prince,\n",
      "This is a very of all the news, preverasing\n",
      "To youthful blood in heart is shall be so\n",
      "The well-much of thine seasons be pursuit?\n",
      "O, when I saw the daughter to such a life,\n",
      "Against the field of the world and heavy day.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Away with the bastard news, and nose more sound.\n",
      "\n",
      "KING RICHARD III:\n",
      "What are the king like this thing of his hand.\n",
      "\n",
      "CORIOLANUS:\n",
      "Why should the state and means to the dead;\n",
      "And send the palmers of the humble of me.\n",
      "\n",
      "GLOUCESTER:\n",
      "It is not of my father did not long but\n",
      "since the world be thought to have her court.\n",
      "\n",
      "MISTRESS OVERDONE:\n",
      "Alas, when the duke is deadly to the shored,\n",
      "Will he shall be speedy to time of provost.\n",
      "\n",
      "PERDITA:\n",
      "And so like sovereignty.\n",
      "\n",
      "LUCIO:\n",
      "You have ever so desires at the white of you.\n",
      "\n",
      "KING RICHARD III:\n",
      "The stately they fight on coats, for mercy\n",
      "And here he may drown all their battles defends\n",
      "Where they dare in one that rest.\n",
      "Heaven to the form'd will complain to sin,\n",
      "That you have hath way, I will call my lengthen and\n",
      "The heart as fastest out our death, I will hear\n",
      "He tender beats my friends doth so.\n",
      "\n",
      "First Citizen:\n",
      "Now friends me to my tent the father lives,\n",
      "More princing your honours better thee.\n",
      "\n",
      "First Servant:\n",
      "Ay, that was our house of Wiltshire that our voice\n",
      "Intends the honour of breathes and men,\n",
      "For here hath like a subject near there.\n",
      "\n",
      "Shepherd:\n",
      "So fly, my lord,\n",
      "I pray thee thou dost our colours with a half,\n",
      "Which he hath breed their state of holy banishment\n",
      "That thou setting of your hands.\n",
      "\n",
      "RICHA\n"
     ]
    }
   ],
   "source": [
    "topk_text = sample_top_k(\n",
    "    model = trained_leviathan_model,\n",
    "    stoi = stoi,\n",
    "    itos = itos,\n",
    "    start_text = \"A\",\n",
    "    max_new_tokens = 2000,\n",
    "    k = 20,\n",
    "    temperature = 0.8\n",
    ")\n",
    "\n",
    "print(\"-------- Top-K Sampling (k=20, temp=0.8) -------- \")\n",
    "print(topk_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aee3b0",
   "metadata": {},
   "source": [
    "## Saving the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "225f1d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at: ./leviathan_saved_model/leviathan_v1.pth\n",
      "Also updated: leviathan_latest.pth\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained_leviathan_model_path = save_model(model = trained_leviathan_model, base_name = \"leviathan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ee21935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ MODEL LOADED ================\n",
      "Loaded File      : ./leviathan_saved_model/leviathan_v1.pth\n",
      "Model Name       : Leviathan-LSTM\n",
      "Model Class      : MyLSTMModel\n",
      "Version          : v1\n",
      "Timestamp        : 2025-12-15 13:26:08\n",
      "----------------------------------------------\n",
      "Model Architecture:\n",
      "  └── linear_projection: LinearLayer\n",
      "  └── linear_projection.linear_projection: Linear\n",
      "  └── lstm_layer: LSTMLayer\n",
      "  └── lstm_layer.layers: ModuleList\n",
      "  └── lstm_layer.layers.0: LSTMCell\n",
      "  └── lstm_layer.layers.0.W_f_x: Linear\n",
      "  └── lstm_layer.layers.0.W_f_h: Linear\n",
      "  └── lstm_layer.layers.0.W_i_x: Linear\n",
      "  └── lstm_layer.layers.0.W_i_h: Linear\n",
      "  └── lstm_layer.layers.0.W_o_x: Linear\n",
      "  └── lstm_layer.layers.0.W_o_h: Linear\n",
      "  └── lstm_layer.layers.0.W_g_x: Linear\n",
      "  └── lstm_layer.layers.0.W_g_h: Linear\n",
      "  └── lstm_layer.layers.1: LSTMCell\n",
      "  └── lstm_layer.layers.1.W_f_x: Linear\n",
      "  └── lstm_layer.layers.1.W_f_h: Linear\n",
      "  └── lstm_layer.layers.1.W_i_x: Linear\n",
      "  └── lstm_layer.layers.1.W_i_h: Linear\n",
      "  └── lstm_layer.layers.1.W_o_x: Linear\n",
      "  └── lstm_layer.layers.1.W_o_h: Linear\n",
      "  └── lstm_layer.layers.1.W_g_x: Linear\n",
      "  └── lstm_layer.layers.1.W_g_h: Linear\n",
      "  └── lstm_layer.layers.2: LSTMCell\n",
      "  └── lstm_layer.layers.2.W_f_x: Linear\n",
      "  └── lstm_layer.layers.2.W_f_h: Linear\n",
      "  └── lstm_layer.layers.2.W_i_x: Linear\n",
      "  └── lstm_layer.layers.2.W_i_h: Linear\n",
      "  └── lstm_layer.layers.2.W_o_x: Linear\n",
      "  └── lstm_layer.layers.2.W_o_h: Linear\n",
      "  └── lstm_layer.layers.2.W_g_x: Linear\n",
      "  └── lstm_layer.layers.2.W_g_h: Linear\n",
      "  └── layer_norm: LayerNorm\n",
      "  └── embeddings: Embedding\n",
      "  └── dropout: Dropout\n",
      "----------------------------------------------\n",
      "Total Parameters : 24,248,129\n",
      "Loaded on Device : cuda\n",
      "==============================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_19096\\1037960762.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_leviathan_model = load_model(filepath = trained_leviathan_model_path, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c9fd8b",
   "metadata": {},
   "source": [
    "### Sampling from the Loaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46c39b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Top-K Sampling (k=20, temp=0.8) -------- \n",
      "A:\n",
      "'Tis a traitor of the man that I would not\n",
      "this after waeking? there wars I beseecher.\n",
      "\n",
      "ROMEO:\n",
      "I shall, deep me: good my lord, is some glistering.\n",
      "\n",
      "ANGELO:\n",
      "Yea, and you are for some majesty.\n",
      "\n",
      "GLOUCESTER:\n",
      "This first that the seats on Marcius for the moved\n",
      "As as you come fly for a common king;\n",
      "Which\n"
     ]
    }
   ],
   "source": [
    "topk_text = sample_top_k(\n",
    "    model = loaded_leviathan_model,\n",
    "    stoi = stoi,\n",
    "    itos = itos,\n",
    "    start_text = \"A\",\n",
    "    max_new_tokens = 300,\n",
    "    k = 20,\n",
    "    temperature = 0.8\n",
    ")\n",
    "\n",
    "print(\"-------- Top-K Sampling (k=20, temp=0.8) -------- \")\n",
    "print(topk_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
