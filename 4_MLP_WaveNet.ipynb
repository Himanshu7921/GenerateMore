{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81802f6c",
   "metadata": {},
   "source": [
    "## **Implementing WaveNet: A Generative Model for Raw Audio (Adapted for Word Generation)**\n",
    "\n",
    "* WaveNet is originally a **generative model for raw audio**.\n",
    "  In this notebook, I will adapt its underlying **architecture** for our **word generation model**, since the core design principles are similar.\n",
    "\n",
    "* The goal is to **improve our baseline model’s architecture** by aligning it more closely with the **WaveNet-inspired approach** discussed in the research paper \"https://arxiv.org/pdf/1609.03499\".\n",
    "\n",
    "---\n",
    "\n",
    "### **Baseline Model (Previous Architecture)**\n",
    "\n",
    "```python\n",
    "model_2 = Sequential([\n",
    "    Embeddings(vocab_size, n_embeddings),\n",
    "    Flatten(),\n",
    "    Linear(in_features, out_features),  BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, vocab_size)\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Upgrading Towards a WaveNet-like Model**\n",
    "\n",
    "In this notebook, I will move beyond the **basic Neural Network + Batch Normalization setup** and introduce a **WaveNet-inspired architecture**.\n",
    "\n",
    "Unlike the baseline model, where squashing/non-linear transformations occur **suddenly**, the WaveNet approach allows features to be **progressively compressed and transformed** through **causal and dilated convolutions**, capturing **hierarchical patterns** in a smoother, more structured manner.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of WaveNet over Plain Neural Networks**\n",
    "\n",
    "1. **Better Long-Term Dependency Modeling**\n",
    "\n",
    "   * WaveNet’s **dilated convolutions** can capture patterns over much longer contexts without requiring extremely deep layers.\n",
    "\n",
    "2. **Smoother Feature Extraction**\n",
    "\n",
    "   * Instead of forcing representations to collapse quickly, WaveNet progressively refines them, leading to more stable and expressive outputs.\n",
    "\n",
    "3. **Improved Generative Quality**\n",
    "\n",
    "   * The autoregressive setup enables WaveNet to generate highly realistic sequences (in audio or text), compared to the sometimes rigid outputs of standard feedforward networks.\n",
    "\n",
    "4. **Scalability**\n",
    "\n",
    "   * Easier to parallelize compared to recurrent models like RNNs or LSTMs, while still capturing temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2e17b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3fe40e",
   "metadata": {},
   "source": [
    "# Laoding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "435da4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the words from the '.txt' file\n",
    "words = open('names.txt', mode = 'r', encoding='utf-8').read().splitlines()\n",
    "words[:10]\n",
    "\n",
    "# Encoder and Decoder\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {c:i+1 for i, c in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:c for c, i in stoi.items()}\n",
    "\n",
    "# Generate train, test and validation Dataset\n",
    "def generate_dataset(words, block_size):\n",
    "    x, y = [], []\n",
    "    for w in words:\n",
    "        # print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            idx = stoi[ch]\n",
    "            x.append(context)\n",
    "            y.append(idx)\n",
    "            # print(f\"{''.join([itos[i] for i in context])} --> {itos[idx]}\")\n",
    "            context = context[1:] + [idx]\n",
    "    x, y = torch.tensor(x), torch.tensor(y)\n",
    "    return x, y\n",
    "\n",
    "def get_split(data, train_split: float, test_split: float, val_split: float, block_size: int):\n",
    "    import random\n",
    "    random.seed(42)\n",
    "\n",
    "    if (train_split + test_split + val_split) != 1:\n",
    "        raise ValueError(\"All splits must sum to 100% of the data\")\n",
    "    else: \n",
    "        random.shuffle(data)\n",
    "        n1 = int(train_split* len(data))\n",
    "        n2 = int((train_split + val_split) * len(data))\n",
    "        x_train, y_train = generate_dataset(data[:n1], block_size)\n",
    "        x_val, y_val = generate_dataset(data[n1:n2], block_size)\n",
    "        x_test, y_test = generate_dataset(data[n2:], block_size)\n",
    "\n",
    "        return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = get_split(data = words, train_split = 0.8, test_split = 0.1, val_split = 0.1, block_size = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c104e",
   "metadata": {},
   "source": [
    "## Defining the Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "093f745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias: bool = True):\n",
    "        self.weight = torch.nn.Parameter(torch.empty(in_features, out_features))\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(out_features)) if bias else None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.output = x @ self.weight + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def __call__(self, x): \n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, in_features, training: bool = True, momentum = 0.1, eps = 1e-05):\n",
    "        self.in_features = in_features\n",
    "        self.training = training\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(1, self.in_features), requires_grad = True)\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(1, self.in_features), requires_grad = True)\n",
    "        self.running_mean = torch.zeros(1, self.in_features)\n",
    "        self.running_var = torch.ones(1, self.in_features)\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            batch_mean = x.mean(dim = 0, keepdim = True)\n",
    "            batch_var = x.var(dim = 0, keepdim = True, unbiased = False)\n",
    "\n",
    "        else: \n",
    "            batch_mean = self.running_mean \n",
    "            batch_var = self.running_var\n",
    "        \n",
    "        x_hat = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
    "        self.output = x_hat * self.gamma + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, input):\n",
    "        self.output = torch.tanh(input)\n",
    "        return self.output\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Embeddings:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = torch.nn.Parameter(torch.randn(in_features, out_features))\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.output = self.weight[x]\n",
    "        return self.output\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "class Flatten:\n",
    "    def forward(self, x):\n",
    "        self.output = x.view(x.shape[0], -1)\n",
    "        return self.output\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb20b6b",
   "metadata": {},
   "source": [
    "## Class for registering the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e68faf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.layers = self.model.layers\n",
    "        self.parameters = model.parameters()\n",
    "        self.n_parameters = sum([p.nelement() for p in self.parameters])\n",
    "        self.model_type = self.check_model()\n",
    "        print(f\"{self.model_type} registered with Learnable Parameters: {self.n_parameters}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def generate_names(self, num_names: int = 5, block_size: int = 8):\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Generating names from {self.model_type}\")\n",
    "        g = torch.Generator().manual_seed(42)\n",
    "        for _ in range(num_names):\n",
    "            out = []\n",
    "            context = [0] * block_size\n",
    "            while True:\n",
    "                for layer in self.layers:\n",
    "                    if isinstance(layer, BatchNorm1d):\n",
    "                        layer.training = False\n",
    "                \n",
    "                x = torch.tensor([context])\n",
    "                for layer in self.layers:\n",
    "                    x = layer(x)\n",
    "                    \n",
    "                probs = F.softmax(x, dim = 1)\n",
    "                idx = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "                context = context[1:] + [idx]\n",
    "                out.append(idx)\n",
    "                if idx == 0:\n",
    "                    break\n",
    "        \n",
    "            print(''.join(itos[i] for i in out))\n",
    "    \n",
    "    def train_model(self, lr: float = 0.01, epochs: int = 200000):\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Training {self.model_type} | Epochs: {200000} | lr: {lr}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            # mini-batch processing\n",
    "            rand_idx = torch.randint(0, x_train.shape[0], (32,))\n",
    "            \n",
    "            x = x_train[rand_idx]\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "\n",
    "            loss = F.cross_entropy(x, y_train[rand_idx])\n",
    "\n",
    "            # Backward pass\n",
    "            for p in self.parameters:\n",
    "                p.grad = None\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            for p in self.parameters:\n",
    "                p.data -= lr * p.grad\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                print(f\"{i} / {epochs} Loss: {loss}\")\n",
    "            \n",
    "            # break\n",
    "    \n",
    "    def check_model(self):\n",
    "        self.model_type = \"Plain_MLP_Model\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BatchNorm1d):\n",
    "                self.model_type = \"BatchNorm_MLP_Model\"\n",
    "                break\n",
    "        return self.model_type\n",
    "    \n",
    "    # Evaluate the loss on validation test\n",
    "    def eval_loss(self, split):\n",
    "        if split == \"train\":\n",
    "            x_data, y_data = x_train, y_train\n",
    "        elif split == \"test\":\n",
    "            x_data, y_data = x_test, y_test\n",
    "        elif split == \"val\":\n",
    "            x_data, y_data = x_val, y_val\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train', 'test', or 'val'\")\n",
    "\n",
    "        x = x_data\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BatchNorm1d):\n",
    "                layer.training = False\n",
    "                break\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        loss_val = F.cross_entropy(x, y_data)\n",
    "        print(f\"Loss on {split} split = {loss_val}\")\n",
    "        return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707d4c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain_MLP_Model registered with Learnable Parameters: 62097\n",
      "BatchNorm_MLP_Model registered with Learnable Parameters: 62897\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "n_embedings = 10\n",
    "vocab_size = len(chars) + 1\n",
    "block_size = 8\n",
    "in_features = n_embedings * block_size\n",
    "out_features = 200\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Embeddings(vocab_size, n_embedings),\n",
    "    Flatten(),\n",
    "    Linear(in_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, vocab_size)\n",
    "])\n",
    "\n",
    "model_2 = Sequential([\n",
    "    Embeddings(vocab_size, n_embedings),\n",
    "    Flatten(),\n",
    "    Linear(in_features, out_features),  BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, vocab_size)\n",
    "])\n",
    "\n",
    "\n",
    "# Register the Model for further tracking\n",
    "model_1 = MLPModel(model = model_1)\n",
    "model_2 = MLPModel(model = model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090f1f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Training Plain_MLP_Model | Epochs: 200000 | lr: 0.01\n",
      "----------------------------------------\n",
      "0 / 200000 Loss: 3.3664844036102295\n",
      "10000 / 200000 Loss: 1.8311465978622437\n",
      "20000 / 200000 Loss: 2.044177532196045\n",
      "30000 / 200000 Loss: 2.006683588027954\n",
      "40000 / 200000 Loss: 2.6584677696228027\n",
      "50000 / 200000 Loss: 2.1751649379730225\n",
      "60000 / 200000 Loss: 2.027599334716797\n",
      "70000 / 200000 Loss: 1.8963265419006348\n",
      "80000 / 200000 Loss: 2.0012118816375732\n",
      "90000 / 200000 Loss: 2.311673879623413\n",
      "100000 / 200000 Loss: 1.9995720386505127\n",
      "110000 / 200000 Loss: 1.9229329824447632\n",
      "120000 / 200000 Loss: 2.4603238105773926\n",
      "130000 / 200000 Loss: 1.8924949169158936\n",
      "140000 / 200000 Loss: 2.2731924057006836\n",
      "150000 / 200000 Loss: 2.0405640602111816\n",
      "160000 / 200000 Loss: 2.2669055461883545\n",
      "170000 / 200000 Loss: 2.355252742767334\n",
      "180000 / 200000 Loss: 1.8750427961349487\n",
      "190000 / 200000 Loss: 1.8019914627075195\n",
      "----------------------------------------\n",
      "Training BatchNorm_MLP_Model | Epochs: 200000 | lr: 0.05\n",
      "----------------------------------------\n",
      "0 / 200000 Loss: 3.6842896938323975\n",
      "10000 / 200000 Loss: 1.9008784294128418\n",
      "20000 / 200000 Loss: 2.197782039642334\n",
      "30000 / 200000 Loss: 2.1006760597229004\n",
      "40000 / 200000 Loss: 1.8959486484527588\n",
      "50000 / 200000 Loss: 1.8180686235427856\n",
      "60000 / 200000 Loss: 1.8664915561676025\n",
      "70000 / 200000 Loss: 1.9066970348358154\n",
      "80000 / 200000 Loss: 2.1570792198181152\n",
      "90000 / 200000 Loss: 1.7821218967437744\n",
      "100000 / 200000 Loss: 1.664002537727356\n",
      "110000 / 200000 Loss: 1.7653181552886963\n",
      "120000 / 200000 Loss: 2.1442530155181885\n",
      "130000 / 200000 Loss: 2.1364455223083496\n",
      "140000 / 200000 Loss: 2.5683858394622803\n",
      "150000 / 200000 Loss: 1.8240025043487549\n",
      "160000 / 200000 Loss: 1.8795379400253296\n",
      "170000 / 200000 Loss: 1.7934852838516235\n",
      "180000 / 200000 Loss: 1.8282902240753174\n",
      "190000 / 200000 Loss: 2.1554055213928223\n"
     ]
    }
   ],
   "source": [
    "# Train the Models\n",
    "model_1.train_model()\n",
    "model_2.train_model(lr = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27f68ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Generating names from Plain_MLP_Model\n",
      "yessy.\n",
      "havilin.\n",
      "dlagkin.\n",
      "zainaya.\n",
      "tryvie.\n",
      "chen.\n",
      "emberly.\n",
      "milah.\n",
      "----------------------------------------\n",
      "Generating names from BatchNorm_MLP_Model\n",
      "yeosyah.\n",
      "marie.\n",
      "daxen.\n",
      "sadee.\n",
      "jyanna.\n",
      "amerie.\n",
      "caena.\n",
      "dayson.\n"
     ]
    }
   ],
   "source": [
    "# generate some words from different Models\n",
    "model_1.generate_names(block_size)\n",
    "model_2.generate_names(block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e542171a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain_MLP_Model\n",
      "Loss on train split = 1.9853750467300415\n",
      "Loss on test split = 2.0512566566467285\n",
      "Loss on val split = 2.052518129348755\n",
      "----------------------------------------\n",
      "BatchNorm_MLP_Model\n",
      "Loss on train split = 1.8535659313201904\n",
      "Loss on test split = 2.0409789085388184\n",
      "Loss on val split = 2.0408637523651123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.0409, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test and Validation loss checking logic: will implemnet later; because i need to re-train the entire model\n",
    "print(\"Plain_MLP_Model\")\n",
    "model_1.eval_loss('train')\n",
    "model_1.eval_loss('test')\n",
    "model_1.eval_loss('val')\n",
    "print(\"-\" * 40)\n",
    "print(\"BatchNorm_MLP_Model\")\n",
    "model_2.eval_loss('train')\n",
    "model_2.eval_loss('test')\n",
    "model_2.eval_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1899b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_name(obj: object) -> str:\n",
    "    return str(type(obj).__name__)\n",
    "\n",
    "def get_shape_description(model: MLPModel) -> None:\n",
    "    class_names = []\n",
    "    for layer in model.layers:\n",
    "        class_names.append(get_class_name(layer))\n",
    "    class_names\n",
    "\n",
    "    rand_idx = torch.randint(1, x_train.shape[0], size = (4, ))\n",
    "    x_batch, _ = x_train[rand_idx], y_train[rand_idx]\n",
    "\n",
    "    input = x_batch\n",
    "    print(f\"{class_names[0]}_input.shape = {input.shape}\")\n",
    "    for i in range(len(class_names)):\n",
    "        input = model.layers[i](input)\n",
    "        print(f\"output.shape after passing from {class_names[i]}_layer : {input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de7af0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model = Plain_MLP_Model\n",
      "--------------------------------------------------------------------------------\n",
      "Embeddings_input.shape = torch.Size([4, 8])\n",
      "output.shape after passing from Embeddings_layer : torch.Size([4, 8, 10])\n",
      "output.shape after passing from Flatten_layer : torch.Size([4, 80])\n",
      "output.shape after passing from Linear_layer : torch.Size([4, 200])\n",
      "output.shape after passing from Tanh_layer : torch.Size([4, 200])\n",
      "output.shape after passing from Linear_layer : torch.Size([4, 200])\n",
      "output.shape after passing from Tanh_layer : torch.Size([4, 200])\n",
      "output.shape after passing from Linear_layer : torch.Size([4, 27])\n",
      "--------------------------------------------------------------------------------\n",
      "Model = BatchNorm_MLP_Model\n",
      "Embeddings_input.shape = torch.Size([4, 8])\n",
      "output.shape after passing from Embeddings_layer : torch.Size([4, 8, 10])\n",
      "output.shape after passing from Flatten_layer : torch.Size([4, 80])\n",
      "output.shape after passing from Linear_layer : torch.Size([4, 200])\n",
      "output.shape after passing from BatchNorm1d_layer : torch.Size([4, 200])\n",
      "output.shape after passing from Tanh_layer : torch.Size([4, 200])\n",
      "output.shape after passing from Linear_layer : torch.Size([4, 200])\n",
      "output.shape after passing from BatchNorm1d_layer : torch.Size([4, 200])\n",
      "output.shape after passing from Tanh_layer : torch.Size([4, 200])\n",
      "output.shape after passing from Linear_layer : torch.Size([4, 27])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Model = Plain_MLP_Model\")\n",
    "print(\"-\" * 80)\n",
    "get_shape_description(model = model_1)\n",
    "print(\"-\" * 80)\n",
    "print(\"Model = BatchNorm_MLP_Model\")\n",
    "get_shape_description(model = model_2)\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f08b1",
   "metadata": {},
   "source": [
    "## Loss Analysis\n",
    "\n",
    "I evaluated the impact of increasing the `block_size` from **3 → 8** on both a Plain MLP and a BatchNorm-augmented MLP.\n",
    "\n",
    "| Model             | Block Size | Train Loss | Test Loss | Val Loss |\n",
    "| ----------------- | ---------- | ---------- | --------- | -------- |\n",
    "| **Plain MLP**     | 3          | 2.0001     | 2.0939    | 2.1003   |\n",
    "| **BatchNorm MLP** | 3          | 2.0006     | 2.1040    | 2.1081   |\n",
    "| **Plain MLP**     | 8          | 1.750       | 2.051    | 2.0525   |\n",
    "| **BatchNorm MLP** | 8          | 1.8535     | 2.0409    | 2.0408   |\n",
    "\n",
    "---\n",
    "\n",
    "**Observation & Interpretation:**\n",
    "\n",
    "* Both models show reduced loss when the block size is increased.\n",
    "* **Plain MLP** achieves the largest improvement, especially in validation loss, suggesting stronger generalization.\n",
    "* **BatchNorm MLP** also benefits, though the improvements are smaller since BatchNorm already stabilizes training.\n",
    "* The results indicate that a larger block size enables the model to **capture longer-range dependencies** in the data, which enhances learning efficiency and reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a712c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* To be added after implementing the WaveNet architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
