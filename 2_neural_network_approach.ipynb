{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb5ae179",
   "metadata": {},
   "source": [
    "# Part-2: The Neural Network Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9e88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b9c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8d1e203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('.', 'e') = 0, 5\n",
      "If input is: '.' (0) | output will be: 'e' (5)\n",
      "('e', 'm') = 5, 13\n",
      "If input is: 'e' (5) | output will be: 'm' (13)\n",
      "('m', 'm') = 13, 13\n",
      "If input is: 'm' (13) | output will be: 'm' (13)\n",
      "('m', 'a') = 13, 1\n",
      "If input is: 'm' (13) | output will be: 'a' (1)\n",
      "('a', '.') = 1, 0\n",
      "If input is: 'a' (1) | output will be: '.' (0)\n",
      "('.', 'o') = 0, 15\n",
      "If input is: '.' (0) | output will be: 'o' (15)\n",
      "('o', 'l') = 15, 12\n",
      "If input is: 'o' (15) | output will be: 'l' (12)\n",
      "('l', 'i') = 12, 9\n",
      "If input is: 'l' (12) | output will be: 'i' (9)\n",
      "('i', 'v') = 9, 22\n",
      "If input is: 'i' (9) | output will be: 'v' (22)\n",
      "('v', 'i') = 22, 9\n",
      "If input is: 'v' (22) | output will be: 'i' (9)\n",
      "('i', 'a') = 9, 1\n",
      "If input is: 'i' (9) | output will be: 'a' (1)\n",
      "('a', '.') = 1, 0\n",
      "If input is: 'a' (1) | output will be: '.' (0)\n",
      "('.', 'a') = 0, 1\n",
      "If input is: '.' (0) | output will be: 'a' (1)\n",
      "('a', 'v') = 1, 22\n",
      "If input is: 'a' (1) | output will be: 'v' (22)\n",
      "('v', 'a') = 22, 1\n",
      "If input is: 'v' (22) | output will be: 'a' (1)\n",
      "('a', '.') = 1, 0\n",
      "If input is: 'a' (1) | output will be: '.' (0)\n",
      "('.', 'i') = 0, 9\n",
      "If input is: '.' (0) | output will be: 'i' (9)\n",
      "('i', 's') = 9, 19\n",
      "If input is: 'i' (9) | output will be: 's' (19)\n",
      "('s', 'a') = 19, 1\n",
      "If input is: 's' (19) | output will be: 'a' (1)\n",
      "('a', 'b') = 1, 2\n",
      "If input is: 'a' (1) | output will be: 'b' (2)\n",
      "('b', 'e') = 2, 5\n",
      "If input is: 'b' (2) | output will be: 'e' (5)\n",
      "('e', 'l') = 5, 12\n",
      "If input is: 'e' (5) | output will be: 'l' (12)\n",
      "('l', 'l') = 12, 12\n",
      "If input is: 'l' (12) | output will be: 'l' (12)\n",
      "('l', 'a') = 12, 1\n",
      "If input is: 'l' (12) | output will be: 'a' (1)\n",
      "('a', '.') = 1, 0\n",
      "If input is: 'a' (1) | output will be: '.' (0)\n",
      "('.', 's') = 0, 19\n",
      "If input is: '.' (0) | output will be: 's' (19)\n",
      "('s', 'o') = 19, 15\n",
      "If input is: 's' (19) | output will be: 'o' (15)\n",
      "('o', 'p') = 15, 16\n",
      "If input is: 'o' (15) | output will be: 'p' (16)\n",
      "('p', 'h') = 16, 8\n",
      "If input is: 'p' (16) | output will be: 'h' (8)\n",
      "('h', 'i') = 8, 9\n",
      "If input is: 'h' (8) | output will be: 'i' (9)\n",
      "('i', 'a') = 9, 1\n",
      "If input is: 'i' (9) | output will be: 'a' (1)\n",
      "('a', '.') = 1, 0\n",
      "If input is: 'a' (1) | output will be: '.' (0)\n",
      "\n",
      "Therefore, this list has been created:  x = [0, 5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19, 15, 16, 8, 9, 1], y = [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19, 15, 16, 8, 9, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Defining the training set for model training (x, y) => input output mapping\n",
    "x, y = [], []\n",
    "for w in words[:5]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        \n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "\n",
    "        print(f\"{bigram} = {idx1}, {idx2}\")\n",
    "        print(f\"If input is: '{bigram[0]}' ({idx1}) | output will be: '{bigram[1]}' ({idx2})\")\n",
    "        x.append(idx1)\n",
    "        y.append(idx2)\n",
    "print()\n",
    "print(\"Therefore, this list has been created: \", end = \" \")\n",
    "print(f\"x = {x}, y = {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132ce599",
   "metadata": {},
   "source": [
    "### One-hot encoding:\n",
    "- We convert index-based data into one-hot vectors, where a single position corresponding to the index is activated (set to 1), while all other positions remain 0.\n",
    "- eg: [4, 5, 2] will look like this [[0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0]]\n",
    "- i.e '5 will look like this [0, 0, 0, 0, 0, 1]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afbd38c",
   "metadata": {},
   "source": [
    "### Conclusion so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6180cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as f\n",
    "import torch\n",
    "x = torch.tensor(x)\n",
    "xenc= f.one_hot(x, num_classes=27).float() # One-Hot encodings (input to the Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e3aedcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0396, 0.0698, 0.0227, 0.0037, 0.0562, 0.0153, 0.0626, 0.0112, 0.0421,\n",
      "        0.0084, 0.0581, 0.1125, 0.0951, 0.0292, 0.0181, 0.0107, 0.0247, 0.0316,\n",
      "        0.0586, 0.0140, 0.1109, 0.0019, 0.0147, 0.0067, 0.0502, 0.0033, 0.0280])\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "weight = torch.randn((27,27), generator=g)\n",
    "logits = xenc @ weight # raw output of the Models (including -ve numbers and numbers > 1)\n",
    "count = torch.exp(logits) # (converting the -ve number to +ve; without loosing meaning)\n",
    "prob = count / count.sum(dim = 1, keepdims = True) # Converting them into probabilities\n",
    "# The Above method is called as Softmax activation layer (Cobverts the models raw output to Prediction probabilites)\n",
    "print(prob[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08bff46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram example 1: '.e' (indexes (0, 5))\n",
      "Input to the neural network = 0\n",
      "Output of the neural network (Probabilities): tensor([0.1230, 0.0793, 0.0441, 0.0022, 0.0353, 0.0052, 0.0172, 0.0036, 0.0084,\n",
      "        0.0932, 0.0121, 0.0044, 0.0087, 0.0102, 0.0083, 0.0384, 0.0926, 0.0153,\n",
      "        0.0109, 0.0278, 0.0084, 0.0527, 0.0399, 0.0962, 0.0644, 0.0655, 0.0330])\n",
      "Actual label of the Neural Network: 5\n",
      "Probability assigned by the Neural Network to the correct label: 0.0052\n",
      "Log likelihood = -5.2567949295043945\n",
      "Negative Log likelihood (Loss) = 5.2567949295043945\n",
      "---------------\n",
      "Bigram example 2: 'em' (indexes (5, 13))\n",
      "Input to the neural network = 5\n",
      "Output of the neural network (Probabilities): tensor([0.0396, 0.0698, 0.0227, 0.0037, 0.0562, 0.0153, 0.0626, 0.0112, 0.0421,\n",
      "        0.0084, 0.0581, 0.1125, 0.0951, 0.0292, 0.0181, 0.0107, 0.0247, 0.0316,\n",
      "        0.0586, 0.0140, 0.1109, 0.0019, 0.0147, 0.0067, 0.0502, 0.0033, 0.0280])\n",
      "Actual label of the Neural Network: 13\n",
      "Probability assigned by the Neural Network to the correct label: 0.0292\n",
      "Log likelihood = -3.5344078540802\n",
      "Negative Log likelihood (Loss) = 3.5344078540802\n",
      "---------------\n",
      "Bigram example 3: 'mm' (indexes (13, 13))\n",
      "Input to the neural network = 13\n",
      "Output of the neural network (Probabilities): tensor([0.0123, 0.0078, 0.0250, 0.0187, 0.0050, 0.0125, 0.0074, 0.0099, 0.1254,\n",
      "        0.1147, 0.0934, 0.0176, 0.0144, 0.0029, 0.0085, 0.0116, 0.1132, 0.0138,\n",
      "        0.0203, 0.1464, 0.0943, 0.0242, 0.0326, 0.0140, 0.0340, 0.0149, 0.0053])\n",
      "Actual label of the Neural Network: 13\n",
      "Probability assigned by the Neural Network to the correct label: 0.0029\n",
      "Log likelihood = -5.8390212059021\n",
      "Negative Log likelihood (Loss) = 5.8390212059021\n",
      "---------------\n",
      "Bigram example 4: 'ma' (indexes (13, 1))\n",
      "Input to the neural network = 13\n",
      "Output of the neural network (Probabilities): tensor([0.0123, 0.0078, 0.0250, 0.0187, 0.0050, 0.0125, 0.0074, 0.0099, 0.1254,\n",
      "        0.1147, 0.0934, 0.0176, 0.0144, 0.0029, 0.0085, 0.0116, 0.1132, 0.0138,\n",
      "        0.0203, 0.1464, 0.0943, 0.0242, 0.0326, 0.0140, 0.0340, 0.0149, 0.0053])\n",
      "Actual label of the Neural Network: 1\n",
      "Probability assigned by the Neural Network to the correct label: 0.0078\n",
      "Log likelihood = -4.859468460083008\n",
      "Negative Log likelihood (Loss) = 4.859468460083008\n",
      "---------------\n",
      "Bigram example 5: 'a.' (indexes (1, 0))\n",
      "Input to the neural network = 1\n",
      "Output of the neural network (Probabilities): tensor([0.0934, 0.0195, 0.0256, 0.0191, 0.0581, 0.0062, 0.0103, 0.0197, 0.1369,\n",
      "        0.0338, 0.0161, 0.0334, 0.0113, 0.0052, 0.0665, 0.0102, 0.0135, 0.0069,\n",
      "        0.2054, 0.0072, 0.0151, 0.0099, 0.0127, 0.0266, 0.0416, 0.0151, 0.0809])\n",
      "Actual label of the Neural Network: 0\n",
      "Probability assigned by the Neural Network to the correct label: 0.0934\n",
      "Log likelihood = -2.3709540367126465\n",
      "Negative Log likelihood (Loss) = 2.3709540367126465\n",
      "---------------\n",
      "==============================\n",
      "Average Negative Log likelihood = 4.372129440307617\n"
     ]
    }
   ],
   "source": [
    "negative_log_likelihood = torch.zeros((5)) # Tracking the model’s loss during training\n",
    "n = 0\n",
    "for i in range(5):\n",
    "    x_char = itos[x[i].item()]\n",
    "    y_char = itos[y[i]]\n",
    "    print(f\"Bigram example {i+1}: '{x_char + y_char}' (indexes ({x[i]}, {y[i]}))\")\n",
    "    print(f\"Input to the neural network = {x[i]}\")\n",
    "    print(f\"Output of the neural network (Probabilities): {prob[i]}\")\n",
    "    print(f\"Actual label of the Neural Network: {y[i]}\")\n",
    "    p = prob[i, y[i]]\n",
    "    print(f\"Probability assigned by the Neural Network to the correct label: {p.item():.4f}\")\n",
    "    logp = torch.log(p)\n",
    "    print(f\"Log likelihood = {logp.item()}\")\n",
    "    negative_log_likelihood[i] = -logp\n",
    "    print(f\"Negative Log likelihood (Loss) = {-logp.item()}\")\n",
    "    n += 1\n",
    "    print(\"-\"*15)\n",
    "\n",
    "print(\"=\" * 30)\n",
    "avg_nlls = negative_log_likelihood.sum() / n\n",
    "print(f\"Average Negative Log likelihood = {avg_nlls}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61a0461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x = [0, 5, 13, 13, 1, 0, 15, 12, 9, 22], y = [5, 13, 13, 1, 0, 15, 12, 9, 22, 9]\n"
     ]
    }
   ],
   "source": [
    "# Applying the same process to all elements of the corpus\n",
    "x, y = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        x.append(idx1)\n",
    "        y.append(idx2)\n",
    "print()\n",
    "print(f\"x = {x[:10]}, y = {y[:10]}\")\n",
    "\n",
    "# Converting them into tensors\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac59ab3",
   "metadata": {},
   "source": [
    "## Optimization of Model's Learnable Parameter (Weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa9ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of weight (Model's Learnable Parameter)\n",
    "g = torch.Generator().manual_seed(42)\n",
    "weight = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b63b6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 3.696647882461548\n",
      "Loss at epoch 100: 2.4729576110839844\n",
      "Loss at epoch 200: 2.4625625610351562\n",
      "Loss at epoch 300: 2.459355592727661\n",
      "Loss at epoch 400: 2.4578323364257812\n"
     ]
    }
   ],
   "source": [
    "# Training our Neural Network\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(x, num_classes = 27).float()\n",
    "\n",
    "for i in range(500):\n",
    "    # Forward pass\n",
    "    logits = xenc @ weight\n",
    "    count = torch.exp(logits)\n",
    "    probs = count / count.sum(dim = 1, keepdims = True) # Probabilities for next character assigned by the Model\n",
    "    loss = -probs[torch.arange(len(y)), y].log().mean()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Loss at epoch {i}: {loss}\")\n",
    "\n",
    "    # Backward pass\n",
    "    weight.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the Model's Parameter\n",
    "    lr = 50\n",
    "    weight.data -= lr * weight.grad\n",
    "\n",
    "# For storing it into a dict\n",
    "neural_network_loss = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9901d6",
   "metadata": {},
   "source": [
    "## Predictions of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2770c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya.\n",
      "syahavilin.\n",
      "dleekahmangonya.\n",
      "tryahe.\n",
      "chen.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "import torch.nn.functional as F\n",
    "for i in range(5):\n",
    "    models_output = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([idx]), num_classes=27).float()\n",
    "        logits = xenc @ weight\n",
    "        exp_logits = torch.exp(logits)\n",
    "        y_preds = exp_logits / exp_logits.sum(dim = 1, keepdims = True)\n",
    "\n",
    "        idx = torch.multinomial(y_preds, num_samples=1, replacement=True, generator=g).item()\n",
    "        models_output.append(itos[idx])\n",
    "\n",
    "        if idx == 0:\n",
    "            break\n",
    "    print(''.join(models_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c4490",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "1. Statistical Approach → Using co-occurrence counts + probability distributions.\n",
    "2. Neural Network Approach → Using trainable parameters + optimization via backpropagation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
