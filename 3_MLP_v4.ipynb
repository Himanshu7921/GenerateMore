{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48ba0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72b55e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the words from the '.txt' file\n",
    "words = open('names.txt', mode = 'r', encoding='utf-8').read().splitlines()\n",
    "words[:10]\n",
    "\n",
    "# Encoder and Decoder\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {c:i+1 for i, c in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:c for c, i in stoi.items()}\n",
    "\n",
    "# Generate train, test and validation Dataset\n",
    "def generate_dataset(words, block_size):\n",
    "    x, y = [], []\n",
    "    for w in words:\n",
    "        # print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            idx = stoi[ch]\n",
    "            x.append(context)\n",
    "            y.append(idx)\n",
    "            # print(f\"{''.join([itos[i] for i in context])} --> {itos[idx]}\")\n",
    "            context = context[1:] + [idx]\n",
    "    x, y = torch.tensor(x), torch.tensor(y)\n",
    "    return x, y\n",
    "\n",
    "def get_split(data, train_split: float, test_split: float, val_split: float, block_size: int):\n",
    "    import random\n",
    "    random.seed(42)\n",
    "\n",
    "    if (train_split + test_split + val_split) != 1:\n",
    "        raise ValueError(\"All splits must sum to 100% of the data\")\n",
    "    else: \n",
    "        random.shuffle(data)\n",
    "        n1 = int(train_split* len(data))\n",
    "        n2 = int((train_split + val_split) * len(data))\n",
    "        x_train, y_train = generate_dataset(data[:n1], block_size)\n",
    "        x_val, y_val = generate_dataset(data[n1:n2], block_size)\n",
    "        x_test, y_test = generate_dataset(data[n2:], block_size)\n",
    "\n",
    "        return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = get_split(data = words, train_split = 0.8, test_split = 0.1, val_split = 0.1, block_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58ad2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias: bool = True):\n",
    "        self.weight = torch.nn.Parameter(torch.empty(in_features, out_features))\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(out_features)) if bias else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x @ self.weight + self.bias\n",
    "    \n",
    "    def __call__(self, x): \n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, in_features, training: bool = True, momentum = 0.1, eps = 1e-05):\n",
    "        self.in_features = in_features\n",
    "        self.training = training\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(1, self.in_features), requires_grad = True)\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(1, self.in_features), requires_grad = True)\n",
    "        self.running_mean = torch.zeros(1, self.in_features)\n",
    "        self.running_var = torch.ones(1, self.in_features)\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            batch_mean = x.mean(dim = 0, keepdim = True)\n",
    "            batch_var = x.var(dim = 0, keepdim = True, unbiased = False)\n",
    "\n",
    "        else: \n",
    "            batch_mean = self.running_mean \n",
    "            batch_var = self.running_var\n",
    "        \n",
    "        x_hat = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
    "        output = x_hat * self.gamma + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, input):\n",
    "        return torch.tanh(input)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Embeddings:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = torch.nn.Parameter(torch.randn(in_features, out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb = self.weight[x]\n",
    "        emb_cat = emb.view(-1, x.shape[1] * self.out_features)\n",
    "        return emb_cat\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "class Flatten:\n",
    "    def forward(self, x):\n",
    "        output = x.view(x.shape[0], -1)\n",
    "        return output\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bb88f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.layers = self.model.layers\n",
    "        self.parameters = model.parameters()\n",
    "        self.n_parameters = sum([p.nelement() for p in self.parameters])\n",
    "        self.model_type = self.check_model()\n",
    "        print(f\"{self.model_type} registered with Learnable Parameters: {self.n_parameters}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def generate_names(self, num_names: int = 5, block_size: int = 3):\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Generating names from {self.model_type}\")\n",
    "        g = torch.Generator().manual_seed(42)\n",
    "        for _ in range(num_names):\n",
    "            out = []\n",
    "            context = [0] * block_size\n",
    "            while True:\n",
    "                for layer in self.layers:\n",
    "                    if isinstance(layer, BatchNorm1d):\n",
    "                        layer.training = False\n",
    "                \n",
    "                x = torch.tensor([context])\n",
    "                for layer in self.layers:\n",
    "                    x = layer(x)\n",
    "                    \n",
    "                probs = F.softmax(x, dim = 1)\n",
    "                idx = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "                context = context[1:] + [idx]\n",
    "                out.append(idx)\n",
    "                if idx == 0:\n",
    "                    break\n",
    "        \n",
    "            print(''.join(itos[i] for i in out))\n",
    "    \n",
    "    def train_model(self, lr: float = 0.01, epochs: int = 200000):\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Training {self.model_type} | Epochs: {200000} | lr: {lr}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            # mini-batch processing\n",
    "            rand_idx = torch.randint(0, x_train.shape[0], (32,))\n",
    "\n",
    "            x = x_train[rand_idx]\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "\n",
    "            loss = F.cross_entropy(x, y_train[rand_idx])\n",
    "\n",
    "            # Backward pass\n",
    "            for p in self.parameters:\n",
    "                p.grad = None\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            for p in self.parameters:\n",
    "                p.data -= lr * p.grad\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                print(f\"{i} / {epochs} Loss: {loss}\")\n",
    "            \n",
    "            # break\n",
    "    \n",
    "    def check_model(self):\n",
    "        self.model_type = \"Plain_MLP_Model\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BatchNorm1d):\n",
    "                self.model_type = \"BatchNorm_MLP_Model\"\n",
    "                break\n",
    "        return self.model_type\n",
    "    \n",
    "    # Evaluate the loss on validation test\n",
    "    def eval_loss(self, split):\n",
    "        if split == \"train\":\n",
    "            x_data, y_data = x_train, y_train\n",
    "        elif split == \"test\":\n",
    "            x_data, y_data = x_test, y_test\n",
    "        elif split == \"val\":\n",
    "            x_data, y_data = x_val, y_val\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train', 'test', or 'val'\")\n",
    "\n",
    "        x = x_data\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BatchNorm1d):\n",
    "                layer.training = False\n",
    "                break\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        loss_val = F.cross_entropy(x, y_data)\n",
    "        print(f\"Loss on {split} split = {loss_val}\")\n",
    "        return loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ba3af",
   "metadata": {},
   "source": [
    "### Defining Model's architecture (Without batch normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c14a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain_MLP_Model registered with Learnable Parameters: 212897\n",
      "BatchNorm_MLP_Model registered with Learnable Parameters: 215297\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "n_embedings = 10\n",
    "vocab_size = len(chars) + 1\n",
    "block_size = 3\n",
    "in_features = n_embedings * block_size\n",
    "out_features = 200\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Embeddings(vocab_size, n_embedings),\n",
    "    Flatten(),\n",
    "    Linear(in_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, vocab_size)\n",
    "])\n",
    "\n",
    "model_2 = Sequential([\n",
    "    Embeddings(vocab_size, n_embedings),\n",
    "    Flatten(),\n",
    "    Linear(in_features, out_features),  BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, vocab_size)\n",
    "])\n",
    "\n",
    "\n",
    "# Register the Model for further tracking\n",
    "model_1 = MLPModel(model = model_1)\n",
    "model_2 = MLPModel(model = model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c97b7a",
   "metadata": {},
   "source": [
    "### Evaluating Loss on Test and Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1b99d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Training Plain_MLP_Model | Epochs: 200000 | lr: 0.01\n",
      "----------------------------------------\n",
      "0 / 200000 Loss: 3.3732125759124756\n",
      "10000 / 200000 Loss: 2.1990609169006348\n",
      "20000 / 200000 Loss: 1.8497204780578613\n",
      "30000 / 200000 Loss: 2.3937573432922363\n",
      "40000 / 200000 Loss: 2.212120771408081\n",
      "50000 / 200000 Loss: 2.159064769744873\n",
      "60000 / 200000 Loss: 1.6576730012893677\n",
      "70000 / 200000 Loss: 1.8349465131759644\n",
      "80000 / 200000 Loss: 1.96515953540802\n",
      "90000 / 200000 Loss: 1.7478300333023071\n",
      "100000 / 200000 Loss: 2.0020923614501953\n",
      "110000 / 200000 Loss: 2.100942611694336\n",
      "120000 / 200000 Loss: 2.4784464836120605\n",
      "130000 / 200000 Loss: 1.7515544891357422\n",
      "140000 / 200000 Loss: 2.178866147994995\n",
      "150000 / 200000 Loss: 2.39397931098938\n",
      "160000 / 200000 Loss: 2.372723340988159\n",
      "170000 / 200000 Loss: 1.6358914375305176\n",
      "180000 / 200000 Loss: 1.974360466003418\n",
      "190000 / 200000 Loss: 2.4713988304138184\n",
      "----------------------------------------\n",
      "Training BatchNorm_MLP_Model | Epochs: 200000 | lr: 0.05\n",
      "----------------------------------------\n",
      "0 / 200000 Loss: 3.8490710258483887\n",
      "10000 / 200000 Loss: 2.245934009552002\n",
      "20000 / 200000 Loss: 2.227781295776367\n",
      "30000 / 200000 Loss: 2.654351234436035\n",
      "40000 / 200000 Loss: 1.9573155641555786\n",
      "50000 / 200000 Loss: 2.0092215538024902\n",
      "60000 / 200000 Loss: 2.0451886653900146\n",
      "70000 / 200000 Loss: 1.912800908088684\n",
      "80000 / 200000 Loss: 1.8628212213516235\n",
      "90000 / 200000 Loss: 2.224370002746582\n",
      "100000 / 200000 Loss: 1.9548746347427368\n",
      "110000 / 200000 Loss: 2.397010087966919\n",
      "120000 / 200000 Loss: 1.906723976135254\n",
      "130000 / 200000 Loss: 2.094362258911133\n",
      "140000 / 200000 Loss: 1.8623313903808594\n",
      "150000 / 200000 Loss: 1.9223629236221313\n",
      "160000 / 200000 Loss: 2.139148235321045\n",
      "170000 / 200000 Loss: 1.84064519405365\n",
      "180000 / 200000 Loss: 1.9800231456756592\n",
      "190000 / 200000 Loss: 1.8885347843170166\n"
     ]
    }
   ],
   "source": [
    "# Train the Models\n",
    "model_1.train_model()\n",
    "model_2.train_model(lr = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3459f85",
   "metadata": {},
   "source": [
    "### Generating names from our Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3f60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Generating names from Plain_MLP_Model\n",
      "yeosyah.\n",
      "malin.\n",
      "dloey.\n",
      "skylinny.\n",
      "nicyannachel.\n",
      "----------------------------------------\n",
      "Generating names from BatchNorm_MLP_Model\n",
      "yessy.\n",
      "theodor.\n",
      "dece.\n",
      "khalei.\n",
      "nya.\n"
     ]
    }
   ],
   "source": [
    "# generate some words from different Models\n",
    "model_1.generate_names(block_size)\n",
    "model_2.generate_names(block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5caef877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain_MLP_Model\n",
      "Loss on train split = 2.0000603199005127\n",
      "Loss on test split = 2.0938944816589355\n",
      "Loss on val split = 2.100276231765747\n",
      "----------------------------------------\n",
      "BatchNorm_MLP_Model\n",
      "Loss on train split = 2.0005698204040527\n",
      "Loss on test split = 2.103998899459839\n",
      "Loss on val split = 2.1080944538116455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.1081, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test and Validation loss checking logic: will implemnet later; because i need to re-train the entire model\n",
    "print(\"Plain_MLP_Model\")\n",
    "model_1.eval_loss('train')\n",
    "model_1.eval_loss('test')\n",
    "model_1.eval_loss('val')\n",
    "print(\"-\" * 40)\n",
    "print(\"BatchNorm_MLP_Model\")\n",
    "model_2.eval_loss('train')\n",
    "model_2.eval_loss('test')\n",
    "model_2.eval_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399273e",
   "metadata": {},
   "source": [
    "## **Summary of 3\\_MLP\\_v4.ipynb**\n",
    "\n",
    "### **Objective**\n",
    "\n",
    "To train **deep MLPs** for character-level name generation using a **from-scratch implementation** of neural network layers, explicitly handling parameter initialization, forward passes, backpropagation, and optimization.\n",
    "\n",
    "### **Implemented Components**\n",
    "\n",
    "* **Linear layer (custom)**: with Xavier initialization and optional bias.\n",
    "* **BatchNorm1d (custom)**: with running mean/variance tracking and learnable `γ` (scale) and `β` (shift).\n",
    "* **Tanh (custom)**: simple wrapper around `torch.tanh`.\n",
    "* **MLPModel (custom)**:\n",
    "\n",
    "  * Tracks parameters.\n",
    "  * Defines `forward`, `train_model`, and `generate_names`.\n",
    "  * Differentiates automatically between `Plain_MLP_Model` and `BatchNorm_MLP_Model`.\n",
    "\n",
    "### **Experiment Setup**\n",
    "\n",
    "* **Embedding table (`C`)** initialized as trainable parameters.\n",
    "* Two MLP variants trained:\n",
    "\n",
    "  1. **Plain\\_MLP\\_Model** → deep stack of Linear + Tanh.\n",
    "  2. **BatchNorm\\_MLP\\_Model** → same, but with BatchNorm1d between every Linear and Tanh.\n",
    "* Both models trained with **mini-batch gradient descent** and tested for **name generation quality**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "* `v4` represents the **first full from-scratch deep learning framework** built in the project (Linear, BN, Activation, Loss, Training Loop).\n",
    "* **Batch Normalization integration** works seamlessly even in this low-level implementation, demonstrating:\n",
    "\n",
    "  * More stable gradient flow,\n",
    "  * Better convergence when training with a higher learning rate (`lr = 0.05` vs `0.01` for plain MLP).\n",
    "* By abstracting layers into composable modules, this notebook sets the stage for:\n",
    "\n",
    "  * Easier **extension into RNNs, GRUs, Transformers**,\n",
    "  * Explicit experimentation with **weight initialization, activations, and normalization strategies** at the code level.\n",
    "* Compared to `v3`, this notebook transitions from **“experimenting with PyTorch MLPs”** → to **“building a neural network framework by hand.”**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
