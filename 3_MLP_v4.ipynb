{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48ba0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b55e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the words from the '.txt' file\n",
    "words = open('names.txt', mode = 'r', encoding='utf-8').read().splitlines()\n",
    "words[:10]\n",
    "\n",
    "# Encoder and Decoder\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {c:i+1 for i, c in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:c for c, i in stoi.items()}\n",
    "\n",
    "# Get the dataset in torch.tensor() format\n",
    "x, y = [], []\n",
    "block_size = 3\n",
    "for w in words:\n",
    "    # print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        idx = stoi[ch]\n",
    "        x.append(context)\n",
    "        y.append(idx)\n",
    "        # print(f\"{''.join([itos[i] for i in context])} --> {itos[idx]}\")\n",
    "        context = context[1:] + [idx]\n",
    "x, y = torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "# Generate train, test and validation Dataset\n",
    "def generate_dataset(words):\n",
    "    x, y = [], []\n",
    "    block_size = 3\n",
    "    for w in words:\n",
    "        # print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            idx = stoi[ch]\n",
    "            x.append(context)\n",
    "            y.append(idx)\n",
    "            # print(f\"{''.join([itos[i] for i in context])} --> {itos[idx]}\")\n",
    "            context = context[1:] + [idx]\n",
    "    x, y = torch.tensor(x), torch.tensor(y)\n",
    "    return x, y\n",
    "\n",
    "def get_split(data, train_split: float, test_split: float, val_split: float):\n",
    "    import random\n",
    "    random.seed(42)\n",
    "\n",
    "    if (train_split + test_split + val_split) != 1:\n",
    "        raise ValueError(\"All splits must sum to 100% of the data\")\n",
    "    else: \n",
    "        random.shuffle(data)\n",
    "        n1 = int(train_split* len(data))\n",
    "        n2 = int((train_split + val_split) * len(data))\n",
    "        x_train, y_train = generate_dataset(data[:n1])\n",
    "        x_val, y_val = generate_dataset(data[n1:n2])\n",
    "        x_test, y_test = generate_dataset(data[n2:])\n",
    "\n",
    "        return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = get_split(data = words, train_split = 0.8, test_split = 0.1, val_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9285b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ad2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias: bool = True):\n",
    "        self.weight = torch.nn.Parameter(torch.empty(in_features, out_features))\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(out_features)) if bias else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x @ self.weight + self.bias\n",
    "    \n",
    "    def __call__(self, x): \n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, in_features, training: bool = True, momentum = 0.1, eps = 1e-05):\n",
    "        self.in_features = in_features\n",
    "        self.training = training\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(1, self.in_features), requires_grad = True)\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(1, self.in_features), requires_grad = True)\n",
    "        self.running_mean = torch.zeros(1, self.in_features)\n",
    "        self.running_var = torch.ones(1, self.in_features)\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            batch_mean = x.mean(dim = 0, keepdim = True)\n",
    "            batch_var = x.var(dim = 0, keepdim = True, unbiased = False)\n",
    "\n",
    "        else: \n",
    "            batch_mean = self.running_mean \n",
    "            batch_var = self.running_var\n",
    "        \n",
    "        x_hat = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
    "        output = x_hat * self.gamma + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, input):\n",
    "        return torch.tanh(input)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb88f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel:\n",
    "    def __init__(self, layers, C):\n",
    "        self.layers = layers\n",
    "        self.C = C\n",
    "        self.parameters = [self.C] + [p for layer in self.layers for p in layer.parameters()]\n",
    "        self.n_parameters = sum([p.nelement() for p in self.parameters])\n",
    "        self.model_type = self.check_model()\n",
    "        print(f\"{self.model_type} Learnable Parameters: {self.n_parameters}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb = self.C[x]\n",
    "        emb_cat = emb.view(1, -1)\n",
    "        for layer in self.layers:\n",
    "            emb_cat = layer(emb_cat)\n",
    "        return emb_cat\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def generate_names(self, num_names: int = 5):\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Generating names from {self.model_type}\")\n",
    "        g = torch.Generator().manual_seed(42)\n",
    "        for _ in range(num_names):\n",
    "            out = []\n",
    "            context = [0] * block_size\n",
    "            while True:\n",
    "                emb = self.C[torch.tensor([context])]\n",
    "                x = emb.view(1, -1)\n",
    "\n",
    "                for layer in self.layers:\n",
    "                    if isinstance(layer, BatchNorm1d):\n",
    "                        layer.training = False\n",
    "\n",
    "                for layer in self.layers:\n",
    "                    x = layer(x)\n",
    "                    \n",
    "                probs = F.softmax(x, dim = 1)\n",
    "                idx = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "                context = context[1:] + [idx]\n",
    "                out.append(idx)\n",
    "                if idx == 0:\n",
    "                    break\n",
    "        \n",
    "            print(''.join(itos[i] for i in out))\n",
    "    \n",
    "    def train_model(self, lr: float = 0.01, epochs: int = 200000):\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Training {self.model_type} | Epochs: {200000} | lr: {lr}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            # mini-batch processing\n",
    "            rand_idx = torch.randint(0, x_train.shape[0], (32,))\n",
    "\n",
    "            emb = self.C[x_train[rand_idx]]\n",
    "            x = emb.view(-1, 30)\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "\n",
    "            loss = F.cross_entropy(x, y_train[rand_idx])\n",
    "\n",
    "            # Backward pass\n",
    "            for p in self.parameters:\n",
    "                p.grad = None\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            for p in self.parameters:\n",
    "                p.data -= lr * p.grad\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                print(f\"{i} / {epochs} Loss: {loss}\")\n",
    "    \n",
    "    def check_model(self):\n",
    "        self.model_type = \"Plain_MLP_Model\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BatchNorm1d):\n",
    "                self.model_type = \"BatchNorm_MLP_Model\"\n",
    "                break\n",
    "        return self.model_type\n",
    "    \n",
    "    # Evaluate the loss on validation test\n",
    "def eval_loss(self, split):\n",
    "    \n",
    "    if split == \"train\":\n",
    "        x_data, y_data = x_train, y_train\n",
    "    elif split == \"test\":\n",
    "        x_data, y_data = x_test, y_test\n",
    "    elif split == \"val\":\n",
    "        x_data, y_data = x_val, y_val\n",
    "    else:\n",
    "        raise ValueError(\"split must be 'train', 'test', or 'val'\")\n",
    "\n",
    "    emb = self.C[x_data]\n",
    "    x = emb.view(-1, 30)\n",
    "\n",
    "    for layer in self.layers:\n",
    "        if isinstance(layer, BatchNorm1d):\n",
    "            layer.training = False\n",
    "\n",
    "    for layer in self.layers:\n",
    "        x = layer(x)\n",
    "\n",
    "    loss_val = F.cross_entropy(x, y_data)\n",
    "    print(f\"Loss on {split} split = {loss_val}\")\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ba3af",
   "metadata": {},
   "source": [
    "### Defining Model's architecture (Without batch normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c14a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain_MLP_Model Learnable Parameters: 212897\n",
      "BatchNorm_MLP_Model Learnable Parameters: 215297\n",
      "----------------------------------------\n",
      "Training Plain_MLP_Model | Epochs: 200000 | lr: 0.01\n",
      "----------------------------------------\n",
      "0 / 200000 Loss: 3.294309616088867\n",
      "10000 / 200000 Loss: 2.2997920513153076\n",
      "20000 / 200000 Loss: 2.566303014755249\n",
      "30000 / 200000 Loss: 2.108135223388672\n",
      "40000 / 200000 Loss: 2.0908284187316895\n",
      "50000 / 200000 Loss: 2.281618356704712\n",
      "60000 / 200000 Loss: 2.090207099914551\n",
      "70000 / 200000 Loss: 2.5185492038726807\n",
      "80000 / 200000 Loss: 1.8580081462860107\n",
      "90000 / 200000 Loss: 2.0318069458007812\n",
      "100000 / 200000 Loss: 2.019204616546631\n",
      "110000 / 200000 Loss: 1.8186008930206299\n",
      "120000 / 200000 Loss: 2.270770788192749\n",
      "130000 / 200000 Loss: 2.2174289226531982\n",
      "140000 / 200000 Loss: 2.046259880065918\n",
      "150000 / 200000 Loss: 1.8387391567230225\n",
      "160000 / 200000 Loss: 2.1483726501464844\n",
      "170000 / 200000 Loss: 1.9504220485687256\n",
      "180000 / 200000 Loss: 1.7631959915161133\n",
      "190000 / 200000 Loss: 1.6307967901229858\n",
      "----------------------------------------\n",
      "Training BatchNorm_MLP_Model | Epochs: 200000 | lr: 0.05\n",
      "----------------------------------------\n",
      "0 / 200000 Loss: 3.807652235031128\n",
      "10000 / 200000 Loss: 2.160982131958008\n",
      "20000 / 200000 Loss: 1.8110496997833252\n",
      "30000 / 200000 Loss: 2.059680938720703\n",
      "40000 / 200000 Loss: 2.3754451274871826\n",
      "50000 / 200000 Loss: 1.9718176126480103\n",
      "60000 / 200000 Loss: 2.112159252166748\n",
      "70000 / 200000 Loss: 2.341688871383667\n",
      "80000 / 200000 Loss: 2.0581610202789307\n",
      "90000 / 200000 Loss: 2.3249144554138184\n",
      "100000 / 200000 Loss: 1.5690383911132812\n",
      "110000 / 200000 Loss: 2.194580554962158\n",
      "120000 / 200000 Loss: 1.9504462480545044\n",
      "130000 / 200000 Loss: 2.150409460067749\n",
      "140000 / 200000 Loss: 2.153879165649414\n",
      "150000 / 200000 Loss: 2.2079455852508545\n",
      "160000 / 200000 Loss: 1.6327544450759888\n",
      "170000 / 200000 Loss: 1.9477609395980835\n",
      "180000 / 200000 Loss: 2.148622989654541\n",
      "190000 / 200000 Loss: 1.7587885856628418\n",
      "----------------------------------------\n",
      "Generating names from Plain_MLP_Model\n",
      "yansyanne.\n",
      "\n",
      "lin.\n",
      "\n",
      "dlogk.\n",
      "\n",
      "skylinny.\n",
      "\n",
      "nicyannah.\n",
      "\n",
      "----------------------------------------\n",
      "Generating names from BatchNorm_MLP_Model\n",
      "yaesyah.\n",
      "\n",
      "malin.\n",
      "\n",
      "dlayking.\n",
      "\n",
      "manny.\n",
      "\n",
      "nicyaidan.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "n_embedings = 10\n",
    "vocab_size = len(chars) + 1\n",
    "C = torch.nn.Parameter(torch.randn((vocab_size, n_embedings), generator = g) * 0.01) # Look-up table\n",
    "emb = C[x_train]\n",
    "emb_cat = emb.view(-1, x_train.shape[1] * n_embedings)\n",
    "\n",
    "in_features = emb_cat.shape[1]\n",
    "out_features = 200\n",
    "\n",
    "model_1_layers = [\n",
    "    Linear(in_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, vocab_size)\n",
    "]\n",
    "\n",
    "model_2_layers = [\n",
    "    Linear(in_features, out_features),  BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, vocab_size)\n",
    "]\n",
    "\n",
    "\n",
    "# Register the Model for further tracking\n",
    "model_1 = MLPModel(layers = model_1_layers, C = C)\n",
    "model_2 = MLPModel(layers = model_2_layers, C = C)\n",
    "\n",
    "# Train the Models\n",
    "model_1.train_model()\n",
    "model_2.train_model(lr = 0.05)\n",
    "\n",
    "\n",
    "# generate some words from different Models\n",
    "model_1.generate_names()\n",
    "model_2.generate_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c97b7a",
   "metadata": {},
   "source": [
    "### Evaluating Loss on Test and Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caef877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and Validation loss checking logic: will implemnet later; because i need to re-train the entire model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde01226",
   "metadata": {},
   "source": [
    "### Generating names from our Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "057af6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Generating names from Plain_MLP_Model\n",
      "yansyanne.\n",
      "\n",
      "lin.\n",
      "\n",
      "dlogk.\n",
      "\n",
      "skylinny.\n",
      "\n",
      "nicyannah.\n",
      "\n",
      "eliena.\n",
      "\n",
      "daya.\n",
      "\n",
      "jian.\n",
      "\n",
      "hakelea.\n",
      "\n",
      "lyn.\n",
      "\n",
      "topheovani.\n",
      "\n",
      "pul.\n",
      "\n",
      "lannezis.\n",
      "\n",
      "jassilah.\n",
      "\n",
      "waina.\n",
      "\n",
      "luna.\n",
      "\n",
      "adari.\n",
      "\n",
      "zion.\n",
      "\n",
      "arten.\n",
      "\n",
      "kira.\n",
      "\n",
      "----------------------------------------\n",
      "Generating names from BatchNorm_MLP_Model\n",
      "yaesyah.\n",
      "\n",
      "malin.\n",
      "\n",
      "dlayking.\n",
      "\n",
      "manny.\n",
      "\n",
      "nicyaidan.\n",
      "\n",
      "eliena.\n",
      "\n",
      "daya.\n",
      "\n",
      "jian.\n",
      "\n",
      "yakeles.\n",
      "\n",
      "lon.\n",
      "\n",
      "toy.\n",
      "\n",
      "javani.\n",
      "\n",
      "pepolannezio.\n",
      "\n",
      "jasellah.\n",
      "\n",
      "xaina.\n",
      "\n",
      "luna.\n",
      "\n",
      "anaya.\n",
      "\n",
      "zerianna.\n",
      "\n",
      "nah.\n",
      "\n",
      "baumaim.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate some words from different Models\n",
    "model_1.generate_names(num_names=20)\n",
    "model_2.generate_names(num_names=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d8904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# write the training logic of the model in different block of code\n",
    "# eval loss on validation, test and training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399273e",
   "metadata": {},
   "source": [
    "## **Summary of 3\\_MLP\\_v4.ipynb**\n",
    "\n",
    "### **Objective**\n",
    "\n",
    "To train **deep MLPs** for character-level name generation using a **from-scratch implementation** of neural network layers, explicitly handling parameter initialization, forward passes, backpropagation, and optimization.\n",
    "\n",
    "### **Implemented Components**\n",
    "\n",
    "* **Linear layer (custom)**: with Xavier initialization and optional bias.\n",
    "* **BatchNorm1d (custom)**: with running mean/variance tracking and learnable `γ` (scale) and `β` (shift).\n",
    "* **Tanh (custom)**: simple wrapper around `torch.tanh`.\n",
    "* **MLPModel (custom)**:\n",
    "\n",
    "  * Tracks parameters.\n",
    "  * Defines `forward`, `train_model`, and `generate_names`.\n",
    "  * Differentiates automatically between `Plain_MLP_Model` and `BatchNorm_MLP_Model`.\n",
    "\n",
    "### **Experiment Setup**\n",
    "\n",
    "* **Embedding table (`C`)** initialized as trainable parameters.\n",
    "* Two MLP variants trained:\n",
    "\n",
    "  1. **Plain\\_MLP\\_Model** → deep stack of Linear + Tanh.\n",
    "  2. **BatchNorm\\_MLP\\_Model** → same, but with BatchNorm1d between every Linear and Tanh.\n",
    "* Both models trained with **mini-batch gradient descent** and tested for **name generation quality**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "* `v4` represents the **first full from-scratch deep learning framework** built in the project (Linear, BN, Activation, Loss, Training Loop).\n",
    "* **Batch Normalization integration** works seamlessly even in this low-level implementation, demonstrating:\n",
    "\n",
    "  * More stable gradient flow,\n",
    "  * Better convergence when training with a higher learning rate (`lr = 0.05` vs `0.01` for plain MLP).\n",
    "* By abstracting layers into composable modules, this notebook sets the stage for:\n",
    "\n",
    "  * Easier **extension into RNNs, GRUs, Transformers**,\n",
    "  * Explicit experimentation with **weight initialization, activations, and normalization strategies** at the code level.\n",
    "* Compared to `v3`, this notebook transitions from **“experimenting with PyTorch MLPs”** → to **“building a neural network framework by hand.”**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
