{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48ba0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b55e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the words from the '.txt' file\n",
    "words = open('names.txt', mode = 'r', encoding='utf-8').read().splitlines()\n",
    "words[:10]\n",
    "\n",
    "# Encoder and Decoder\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {c:i+1 for i, c in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:c for c, i in stoi.items()}\n",
    "\n",
    "# Get the dataset in torch.tensor() format\n",
    "x, y = [], []\n",
    "block_size = 3\n",
    "for w in words:\n",
    "    # print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        idx = stoi[ch]\n",
    "        x.append(context)\n",
    "        y.append(idx)\n",
    "        # print(f\"{''.join([itos[i] for i in context])} --> {itos[idx]}\")\n",
    "        context = context[1:] + [idx]\n",
    "x, y = torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "# Generate train, test and validation Dataset\n",
    "def generate_dataset(words):\n",
    "    x, y = [], []\n",
    "    block_size = 3\n",
    "    for w in words:\n",
    "        # print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            idx = stoi[ch]\n",
    "            x.append(context)\n",
    "            y.append(idx)\n",
    "            # print(f\"{''.join([itos[i] for i in context])} --> {itos[idx]}\")\n",
    "            context = context[1:] + [idx]\n",
    "    x, y = torch.tensor(x), torch.tensor(y)\n",
    "    return x, y\n",
    "\n",
    "def get_split(data, train_split: float, test_split: float, val_split: float):\n",
    "    import random\n",
    "    random.seed(42)\n",
    "\n",
    "    if (train_split + test_split + val_split) != 1:\n",
    "        raise ValueError(\"All splits must sum to 100% of the data\")\n",
    "    else: \n",
    "        random.shuffle(data)\n",
    "        n1 = int(train_split* len(data))\n",
    "        n2 = int((train_split + val_split) * len(data))\n",
    "        x_train, y_train = generate_dataset(data[:n1])\n",
    "        x_val, y_val = generate_dataset(data[n1:n2])\n",
    "        x_test, y_test = generate_dataset(data[n2:])\n",
    "\n",
    "        return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = get_split(data = words, train_split = 0.8, test_split = 0.1, val_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9285b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ad2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias: bool = True):\n",
    "        self.weight = torch.nn.Parameter(torch.empty(in_features, out_features))\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(out_features)) if bias else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x @ self.weight + self.bias\n",
    "    \n",
    "    def __call__(self, x): \n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, in_features, training: bool = True, momentum = 0.1, eps = 1e-05):\n",
    "        self.in_features = in_features\n",
    "        self.training = training\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(1, self.in_features), requires_grad = True)\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(1, self.in_features), requires_grad = True)\n",
    "        self.running_mean = torch.zeros(1, self.in_features)\n",
    "        self.running_var = torch.ones(1, self.in_features)\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            batch_mean = x.mean(dim = 0, keepdim = True)\n",
    "            batch_var = x.var(dim = 0, keepdim = True, unbiased = False)\n",
    "\n",
    "        else: \n",
    "            batch_mean = self.running_mean \n",
    "            batch_var = self.running_var\n",
    "        \n",
    "        x_hat = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
    "        output = x_hat * self.gamma + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, input):\n",
    "        return torch.tanh(input)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb88f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel:\n",
    "    def __init__(self, layers, C):\n",
    "        self.layers = layers\n",
    "        self.C = C\n",
    "        self.parameters = [self.C] + [p for layer in self.layers for p in layer.parameters()]\n",
    "        self.n_parameters = sum([p.nelement() for p in self.parameters])\n",
    "        self.model_type = self.check_model()\n",
    "        print(f\"{self.model_type} registered with Learnable Parameters: {self.n_parameters}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb = self.C[x]\n",
    "        emb_cat = emb.view(1, -1)\n",
    "        for layer in self.layers:\n",
    "            emb_cat = layer(emb_cat)\n",
    "        return emb_cat\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def generate_names(self, num_names: int = 5):\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Generating names from {self.model_type}\")\n",
    "        g = torch.Generator().manual_seed(42)\n",
    "        for _ in range(num_names):\n",
    "            out = []\n",
    "            context = [0] * block_size\n",
    "            while True:\n",
    "                emb = self.C[torch.tensor([context])]\n",
    "                x = emb.view(1, -1)\n",
    "\n",
    "                for layer in self.layers:\n",
    "                    if isinstance(layer, BatchNorm1d):\n",
    "                        layer.training = False\n",
    "\n",
    "                for layer in self.layers:\n",
    "                    x = layer(x)\n",
    "                    \n",
    "                probs = F.softmax(x, dim = 1)\n",
    "                idx = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "                context = context[1:] + [idx]\n",
    "                out.append(idx)\n",
    "                if idx == 0:\n",
    "                    break\n",
    "        \n",
    "            print(''.join(itos[i] for i in out))\n",
    "    \n",
    "    def train_model(self, lr: float = 0.01, epochs: int = 200000):\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Training {self.model_type} | Epochs: {200000} | lr: {lr}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            # mini-batch processing\n",
    "            rand_idx = torch.randint(0, x_train.shape[0], (32,))\n",
    "\n",
    "            emb = self.C[x_train[rand_idx]]\n",
    "            x = emb.view(-1, 30)\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "\n",
    "            loss = F.cross_entropy(x, y_train[rand_idx])\n",
    "\n",
    "            # Backward pass\n",
    "            for p in self.parameters:\n",
    "                p.grad = None\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            for p in self.parameters:\n",
    "                p.data -= lr * p.grad\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                print(f\"{i} / {epochs} Loss: {loss}\")\n",
    "    \n",
    "    def check_model(self):\n",
    "        self.model_type = \"Plain_MLP_Model\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BatchNorm1d):\n",
    "                self.model_type = \"BatchNorm_MLP_Model\"\n",
    "                break\n",
    "        return self.model_type\n",
    "    \n",
    "    # Evaluate the loss on validation test\n",
    "    def eval_loss(self, split):\n",
    "        if split == \"train\":\n",
    "            x_data, y_data = x_train, y_train\n",
    "        elif split == \"test\":\n",
    "            x_data, y_data = x_test, y_test\n",
    "        elif split == \"val\":\n",
    "            x_data, y_data = x_val, y_val\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train', 'test', or 'val'\")\n",
    "\n",
    "        emb = self.C[x_data]\n",
    "        x = emb.view(-1, 30)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BatchNorm1d):\n",
    "                layer.training = False\n",
    "                break\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        loss_val = F.cross_entropy(x, y_data)\n",
    "        print(f\"Loss on {split} split = {loss_val}\")\n",
    "        return loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ba3af",
   "metadata": {},
   "source": [
    "### Defining Model's architecture (Without batch normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46c14a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain_MLP_Model registered with Learnable Parameters: 212897\n",
      "BatchNorm_MLP_Model registered with Learnable Parameters: 215297\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "n_embedings = 10\n",
    "vocab_size = len(chars) + 1\n",
    "C = torch.nn.Parameter(torch.randn((vocab_size, n_embedings), generator = g) * 0.01) # Look-up table\n",
    "emb = C[x_train]\n",
    "emb_cat = emb.view(-1, x_train.shape[1] * n_embedings)\n",
    "\n",
    "in_features = emb_cat.shape[1]\n",
    "out_features = 200\n",
    "\n",
    "model_1_layers = [\n",
    "    Linear(in_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, out_features), Tanh(),\n",
    "    Linear(out_features, vocab_size)\n",
    "]\n",
    "\n",
    "model_2_layers = [\n",
    "    Linear(in_features, out_features),  BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, out_features), BatchNorm1d(out_features), Tanh(),\n",
    "    Linear(out_features, vocab_size)\n",
    "]\n",
    "\n",
    "\n",
    "# Register the Model for further tracking\n",
    "model_1 = MLPModel(layers = model_1_layers, C = C)\n",
    "model_2 = MLPModel(layers = model_2_layers, C = C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c97b7a",
   "metadata": {},
   "source": [
    "### Evaluating Loss on Test and Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b99d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Training Plain_MLP_Model | Epochs: 200000 | lr: 0.01\n",
      "----------------------------------------\n",
      "0 / 200000 Loss: 3.294497489929199\n",
      "10000 / 200000 Loss: 2.404792547225952\n",
      "20000 / 200000 Loss: 2.1788437366485596\n",
      "30000 / 200000 Loss: 2.3839800357818604\n",
      "40000 / 200000 Loss: 2.065843343734741\n",
      "50000 / 200000 Loss: 2.5495574474334717\n",
      "60000 / 200000 Loss: 2.3854641914367676\n",
      "70000 / 200000 Loss: 1.9497584104537964\n",
      "80000 / 200000 Loss: 2.0401358604431152\n",
      "90000 / 200000 Loss: 2.23378324508667\n",
      "100000 / 200000 Loss: 2.252897024154663\n",
      "110000 / 200000 Loss: 2.1235134601593018\n",
      "120000 / 200000 Loss: 2.16164493560791\n",
      "130000 / 200000 Loss: 2.059861183166504\n",
      "140000 / 200000 Loss: 2.4819765090942383\n",
      "150000 / 200000 Loss: 2.261061906814575\n",
      "160000 / 200000 Loss: 2.1779606342315674\n",
      "170000 / 200000 Loss: 1.9697216749191284\n",
      "180000 / 200000 Loss: 2.0542755126953125\n",
      "190000 / 200000 Loss: 1.8965195417404175\n",
      "----------------------------------------\n",
      "Training BatchNorm_MLP_Model | Epochs: 200000 | lr: 0.05\n",
      "----------------------------------------\n",
      "0 / 200000 Loss: 3.7086849212646484\n",
      "10000 / 200000 Loss: 2.4959278106689453\n",
      "20000 / 200000 Loss: 1.8715029954910278\n",
      "30000 / 200000 Loss: 1.9668010473251343\n",
      "40000 / 200000 Loss: 2.066955327987671\n",
      "50000 / 200000 Loss: 2.1916472911834717\n",
      "60000 / 200000 Loss: 1.9284411668777466\n",
      "70000 / 200000 Loss: 2.0691347122192383\n",
      "80000 / 200000 Loss: 2.201362371444702\n",
      "90000 / 200000 Loss: 1.9536067247390747\n",
      "100000 / 200000 Loss: 1.8210843801498413\n",
      "110000 / 200000 Loss: 1.9832346439361572\n",
      "120000 / 200000 Loss: 1.8369609117507935\n",
      "130000 / 200000 Loss: 2.314251184463501\n",
      "140000 / 200000 Loss: 2.281003952026367\n",
      "150000 / 200000 Loss: 2.0064001083374023\n",
      "160000 / 200000 Loss: 1.8648731708526611\n",
      "170000 / 200000 Loss: 2.0739479064941406\n",
      "180000 / 200000 Loss: 1.9089430570602417\n",
      "190000 / 200000 Loss: 2.156566619873047\n"
     ]
    }
   ],
   "source": [
    "# Train the Models\n",
    "model_1.train_model()\n",
    "model_2.train_model(lr = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3459f85",
   "metadata": {},
   "source": [
    "### Generating names from our Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcd3f60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Generating names from Plain_MLP_Model\n",
      "ya.\n",
      "syah.\n",
      "milin.\n",
      "dleekshmani.\n",
      "nys.\n",
      "----------------------------------------\n",
      "Generating names from BatchNorm_MLP_Model\n",
      "yessy.\n",
      "haviah.\n",
      "noluwatso.\n",
      "manny.\n",
      "neryckeniter.\n"
     ]
    }
   ],
   "source": [
    "# generate some words from different Models\n",
    "model_1.generate_names()\n",
    "model_2.generate_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5caef877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain_MLP_Model\n",
      "Loss on train split = 2.183703660964966\n",
      "Loss on test split = 2.2381961345672607\n",
      "Loss on val split = 2.236504554748535\n",
      "----------------------------------------\n",
      "BatchNorm_MLP_Model\n",
      "Loss on train split = 2.005396604537964\n",
      "Loss on test split = 2.107797861099243\n",
      "Loss on val split = 2.1099600791931152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.1100, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test and Validation loss checking logic: will implemnet later; because i need to re-train the entire model\n",
    "print(\"Plain_MLP_Model\")\n",
    "model_1.eval_loss('train')\n",
    "model_1.eval_loss('test')\n",
    "model_1.eval_loss('val')\n",
    "print(\"-\" * 40)\n",
    "print(\"BatchNorm_MLP_Model\")\n",
    "model_2.eval_loss('train')\n",
    "model_2.eval_loss('test')\n",
    "model_2.eval_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399273e",
   "metadata": {},
   "source": [
    "## **Summary of 3\\_MLP\\_v4.ipynb**\n",
    "\n",
    "### **Objective**\n",
    "\n",
    "To train **deep MLPs** for character-level name generation using a **from-scratch implementation** of neural network layers, explicitly handling parameter initialization, forward passes, backpropagation, and optimization.\n",
    "\n",
    "### **Implemented Components**\n",
    "\n",
    "* **Linear layer (custom)**: with Xavier initialization and optional bias.\n",
    "* **BatchNorm1d (custom)**: with running mean/variance tracking and learnable `γ` (scale) and `β` (shift).\n",
    "* **Tanh (custom)**: simple wrapper around `torch.tanh`.\n",
    "* **MLPModel (custom)**:\n",
    "\n",
    "  * Tracks parameters.\n",
    "  * Defines `forward`, `train_model`, and `generate_names`.\n",
    "  * Differentiates automatically between `Plain_MLP_Model` and `BatchNorm_MLP_Model`.\n",
    "\n",
    "### **Experiment Setup**\n",
    "\n",
    "* **Embedding table (`C`)** initialized as trainable parameters.\n",
    "* Two MLP variants trained:\n",
    "\n",
    "  1. **Plain\\_MLP\\_Model** → deep stack of Linear + Tanh.\n",
    "  2. **BatchNorm\\_MLP\\_Model** → same, but with BatchNorm1d between every Linear and Tanh.\n",
    "* Both models trained with **mini-batch gradient descent** and tested for **name generation quality**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "* `v4` represents the **first full from-scratch deep learning framework** built in the project (Linear, BN, Activation, Loss, Training Loop).\n",
    "* **Batch Normalization integration** works seamlessly even in this low-level implementation, demonstrating:\n",
    "\n",
    "  * More stable gradient flow,\n",
    "  * Better convergence when training with a higher learning rate (`lr = 0.05` vs `0.01` for plain MLP).\n",
    "* By abstracting layers into composable modules, this notebook sets the stage for:\n",
    "\n",
    "  * Easier **extension into RNNs, GRUs, Transformers**,\n",
    "  * Explicit experimentation with **weight initialization, activations, and normalization strategies** at the code level.\n",
    "* Compared to `v3`, this notebook transitions from **“experimenting with PyTorch MLPs”** → to **“building a neural network framework by hand.”**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
