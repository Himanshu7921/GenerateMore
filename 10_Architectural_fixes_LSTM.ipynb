{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "918fe1b1",
   "metadata": {},
   "source": [
    "## 1. Gradient Lens → Architectural Fix\n",
    "\n",
    "### Limitation Recap (from Gradient Lens)\n",
    "\n",
    "From the gradient-flow analysis, we observed that in LSTMs the backward signal must propagate through every intermediate timestep. The dependency chain takes the form:\n",
    "\n",
    "$$\n",
    "c_t \\rightarrow c_{t-1} \\rightarrow c_{t-2} \\rightarrow \\dots \\rightarrow c_{t-k}\n",
    "$$\n",
    "\n",
    "As a result, gradients are repeatedly multiplied across timesteps, causing them to gradually shrink as sequence length increases. The core issue is therefore not optimization, but the **length of the dependency path** itself.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Architectural Question\n",
    "\n",
    "Instead of asking *how to strengthen gradients*, we ask a more fundamental question:\n",
    "\n",
    "> Why must information (and gradients) travel through all intermediate timesteps to reach distant positions?\n",
    "\n",
    "This reveals that the limitation arises from a **forced sequential dependency**, not from learning dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "### Required Architectural Property\n",
    "\n",
    "To address this limitation, an architecture must allow:\n",
    "\n",
    "> **Direct interaction between distant positions without passing through all intermediate timesteps.**\n",
    "\n",
    "This immediately implies:\n",
    "- Shorter gradient paths\n",
    "- Gradient strength determined by relevance, not temporal distance\n",
    "\n",
    "---\n",
    "\n",
    "### Structural Implications\n",
    "\n",
    "Such an architecture must:\n",
    "- Maintain explicit representations for individual timesteps\n",
    "- Allow position $i$ to directly influence position $j$\n",
    "- Avoid forcing information through a single recurrent chain\n",
    "\n",
    "In contrast to a linear chain structure, this introduces **flexible connectivity** between sequence elements.\n",
    "\n",
    "---\n",
    "\n",
    "### Effect on Gradient Propagation\n",
    "\n",
    "With direct position-to-position interactions, gradients can flow as:\n",
    "\n",
    "$$\n",
    "\\text{Loss at time } t \\;\\rightarrow\\; \\text{Representation at time } t-k\n",
    "$$\n",
    "\n",
    "without traversing all intermediate states. Consequently:\n",
    "- Long multiplicative chains are eliminated\n",
    "- Gradient decay no longer scales with sequence length\n",
    "- Dependency distance becomes independent of gradient distance\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "> The solution to gradient instability is not stronger memory or better gating, but **shorter dependency paths**.\n",
    "\n",
    "This reframes the gradient problem as a **connectivity problem**, rather than a memory problem.\n",
    "\n",
    "---\n",
    "\n",
    "### Terminology (Revealed After Derivation)\n",
    "\n",
    "An architectural design in which every position in a sequence can directly interact with every other position creates gradient paths that are independent of sequence length.\n",
    "\n",
    "This concept is known as **self-attention**.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "By removing the requirement that information must flow through all intermediate timesteps, this architectural change directly addresses the gradient-flow limitation identified in LSTMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f2b66",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d84d784",
   "metadata": {},
   "source": [
    "## 2. Capacity / Compression Lens → Architectural Fix\n",
    "\n",
    "### Limitation Recap (from Capacity / Compression Lens)\n",
    "\n",
    "In LSTMs, all past information is stored in a single fixed-size cell state:\n",
    "\n",
    "$$\n",
    "c_t \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "At every timestep, this memory is updated as:\n",
    "\n",
    "$$\n",
    "c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t\n",
    "$$\n",
    "\n",
    "This update rule forces all historical information—regardless of sequence length—to be repeatedly compressed into the same fixed-dimensional vector. As new information arrives, older information must be weakened or mixed to make room, leading to progressive information loss.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Architectural Question\n",
    "\n",
    "Rather than asking *how to store information better in a fixed vector*, we ask:\n",
    "\n",
    "> Why must all past information be compressed into a single fixed-size memory at all?\n",
    "\n",
    "This question exposes the true limitation: **memory capacity is independent of sequence length**.\n",
    "\n",
    "---\n",
    "\n",
    "### Required Architectural Property\n",
    "\n",
    "To overcome this limitation, an architecture must allow:\n",
    "\n",
    "> **Memory capacity to scale with the length of the sequence, rather than remaining fixed.**\n",
    "\n",
    "This implies:\n",
    "- Past information should not be repeatedly merged into a single state\n",
    "- Individual timesteps should retain their own representations\n",
    "- Memory growth should be proportional to input length\n",
    "\n",
    "---\n",
    "\n",
    "### Structural Implications\n",
    "\n",
    "Instead of representing history as:\n",
    "\n",
    "$$\n",
    "\\text{History} \\rightarrow c_t \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "the architecture should represent history as:\n",
    "\n",
    "$$\n",
    "\\text{History} \\rightarrow \\{ h_1, h_2, \\dots, h_t \\}\n",
    "$$\n",
    "\n",
    "where each timestep maintains its own state without being forced into a shared memory slot.\n",
    "\n",
    "This removes the need for repeated compression and preserves fine-grained information from earlier timesteps.\n",
    "\n",
    "---\n",
    "\n",
    "### Effect on Information Preservation\n",
    "\n",
    "With per-timestep representations:\n",
    "- Older information does not need to be weakened to store new information\n",
    "- Memory does not degrade simply due to sequence length\n",
    "- Long-range details remain available without repeated compression\n",
    "\n",
    "Information loss becomes a **choice**, not a structural requirement.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "> The limitation is not that LSTM memory is too small, but that it is **singular**.\n",
    "\n",
    "By removing the requirement that all information must live in one vector, the compression bottleneck disappears.\n",
    "\n",
    "---\n",
    "\n",
    "### Terminology (Revealed After Derivation)\n",
    "\n",
    "An architectural design in which each timestep retains its own representation, and memory capacity grows with sequence length, avoids forced compression.\n",
    "\n",
    "This idea underlies **self-attention–based architectures**, where sequence elements are stored explicitly rather than merged into a single state.\n",
    "\n",
    "---\n",
    "\n",
    "> Question: Why did LSTM struggle, and why didn’t the new design?\n",
    "\n",
    "> Answer: Because LSTM was forced to repeatedly compress past information into a fixed-size memory, while the new design preserves per-token representations and avoids information loss.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "The capacity limitation of LSTMs arises from compressing unbounded sequence information into a fixed-size memory. Allowing memory to scale with sequence length directly removes this bottleneck and preserves long-range information without repeated loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8faf0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07ab1a2c",
   "metadata": {},
   "source": [
    "## 3. Temporal Locality Lens → Architectural Fix\n",
    "\n",
    "### Limitation Recap (from Temporal Locality Lens)\n",
    "\n",
    "In LSTMs, the importance of information is determined **at the moment it is written into memory**.  \n",
    "The memory update rule is:\n",
    "\n",
    "$$\n",
    "c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t\n",
    "$$\n",
    "\n",
    "Here, the input gate $i_t$ and forget gate $f_t$ decide how strongly current information is stored and how much past information is retained.  \n",
    "Crucially, these decisions are made using **only past and current context**, with no access to future information.\n",
    "\n",
    "Once the memory is updated, earlier representations cannot be revisited or revised.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Architectural Question\n",
    "\n",
    "Instead of asking *how to store information more carefully*, we ask:\n",
    "\n",
    "> Why must the model decide the importance of information **before** seeing the future context in which it may be needed?\n",
    "\n",
    "This reveals a mismatch between **local memory decisions** and **global sequence requirements**.\n",
    "\n",
    "---\n",
    "\n",
    "### Required Architectural Property\n",
    "\n",
    "To overcome this limitation, an architecture must allow:\n",
    "\n",
    "> **The importance of past information to be decided at the time it is used, not at the time it is stored.**\n",
    "\n",
    "This implies:\n",
    "- Memory writing should not permanently fix importance\n",
    "- Past information should remain accessible in its original form\n",
    "- Relevance should be computed dynamically based on current needs\n",
    "\n",
    "---\n",
    "\n",
    "### Structural Implications\n",
    "\n",
    "Instead of memory being written as:\n",
    "\n",
    "$$\n",
    "\\text{store now} \\;\\Rightarrow\\; \\text{importance fixed forever}\n",
    "$$\n",
    "\n",
    "the architecture should support:\n",
    "\n",
    "$$\n",
    "\\text{store first} \\;\\Rightarrow\\; \\text{decide importance later}\n",
    "$$\n",
    "\n",
    "This requires separating:\n",
    "- **memory storage** from\n",
    "- **memory usage**\n",
    "\n",
    "so that future context can influence which past information matters.\n",
    "\n",
    "---\n",
    "\n",
    "### Effect on Information Usage\n",
    "\n",
    "With deferred importance decisions:\n",
    "- Past information does not need to be guessed as important or unimportant early\n",
    "- The model can revisit earlier inputs when sufficient context is available\n",
    "- Long-range dependencies can be resolved using full sequence information\n",
    "\n",
    "This removes the need for premature forgetting.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "> The limitation is not forgetting itself, but **being forced to decide too early what should be remembered**.\n",
    "\n",
    "Allowing the model to postpone relevance decisions fundamentally changes how long-range dependencies are handled.\n",
    "\n",
    "---\n",
    "\n",
    "### Terminology (Revealed After Derivation)\n",
    "\n",
    "An architectural design that allows past information to be stored without committing to its importance, and evaluates relevance only when needed, enables dynamic reuse of memory.\n",
    "\n",
    "This principle is realized in **self-attention–based architectures**, where relevance is computed at read time rather than write time.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Temporal locality in LSTMs arises because memory importance is fixed at the time of storage, without access to future context. By deferring importance decisions until memory is accessed, this architectural change enables more reliable handling of long-range dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716b13e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15aa1ad9",
   "metadata": {},
   "source": [
    "## 4. Credit Assignment Lens → Architectural Fix\n",
    "\n",
    "### Limitation Recap (from Credit Assignment Lens)\n",
    "\n",
    "In sequence modeling, prediction error is computed at a specific timestep $t$, but the cause of that error may originate from inputs far in the past.  \n",
    "In LSTMs, the loss at time $t$ propagates backward through the cell state as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_t}{\\partial c_{t-k}}\n",
    "= \\frac{\\partial \\mathcal{L}_t}{\\partial c_t}\n",
    "\\cdot \\prod_{j=t-k+1}^{t} f_j\n",
    "$$\n",
    "\n",
    "Because the cell state $c_t$ is a compressed mixture of all previous inputs, the backward error signal is distributed across many timesteps and scaled primarily by temporal distance rather than semantic relevance.\n",
    "\n",
    "As a result, the model cannot precisely identify which specific past input was responsible for the prediction error.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Architectural Question\n",
    "\n",
    "Instead of asking *how to propagate gradients more stably*, we ask:\n",
    "\n",
    "> Why must error signals be distributed through a compressed memory rather than being routed directly to the responsible past information?\n",
    "\n",
    "This exposes the central issue: **credit is assigned based on position in time, not based on content relevance**.\n",
    "\n",
    "---\n",
    "\n",
    "### Required Architectural Property\n",
    "\n",
    "To address this limitation, an architecture must allow:\n",
    "\n",
    "> **Errors to be routed directly to the specific past representations that influenced the prediction.**\n",
    "\n",
    "This implies:\n",
    "- Error attribution should be content-based, not time-based\n",
    "- Relevant past inputs should receive stronger learning signals\n",
    "- Irrelevant inputs should receive minimal or no credit\n",
    "\n",
    "---\n",
    "\n",
    "### Structural Implications\n",
    "\n",
    "Instead of relying on a single compressed state:\n",
    "\n",
    "$$\n",
    "\\text{All past inputs} \\;\\rightarrow\\; c_t\n",
    "$$\n",
    "\n",
    "the architecture should maintain explicit representations:\n",
    "\n",
    "$$\n",
    "\\{h_1, h_2, \\dots, h_t\\}\n",
    "$$\n",
    "\n",
    "and allow the prediction at time $t$ to be computed as a function of **selected past representations** rather than the entire mixture.\n",
    "\n",
    "This enables the backward error to follow **direct responsibility paths**.\n",
    "\n",
    "---\n",
    "\n",
    "### Effect on Learning Dynamics\n",
    "\n",
    "With content-based routing:\n",
    "- Errors flow directly to the representations that contributed to the prediction\n",
    "- Learning signals become sharper and more informative\n",
    "- Long-range dependencies receive precise supervision rather than diluted gradients\n",
    "\n",
    "This significantly improves the model’s ability to learn grammatical and semantic relationships spanning long distances.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "> The difficulty is not that errors cannot reach the past, but that they **cannot reach the right part of the past**.\n",
    "\n",
    "Effective credit assignment requires selective, content-aware gradient paths.\n",
    "\n",
    "---\n",
    "\n",
    "### Terminology (Revealed After Derivation)\n",
    "\n",
    "An architectural design that computes explicit relevance scores between current predictions and past representations enables direct, content-based error routing.\n",
    "\n",
    "This principle is implemented in **self-attention–based architectures**, where attention weights act as responsibility signals during both forward and backward passes.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Credit assignment in LSTMs is limited by compressed memory and time-based gradient propagation. By enabling direct, content-based connections between predictions and past inputs, this architectural change allows errors to be assigned precisely to the information that caused them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e26255",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6cd6b38",
   "metadata": {},
   "source": [
    "## 5. Representational Bias Lens → Architectural Fix\n",
    "\n",
    "### Limitation Recap (from Representational Bias Lens)\n",
    "\n",
    "In LSTMs, sequence modeling is performed by repeatedly applying the same transition function at each timestep:\n",
    "\n",
    "$$\n",
    "(h_t, c_t) = F(h_{t-1}, c_{t-1}, x_t)\n",
    "$$\n",
    "\n",
    "This enforces a strictly **linear, left-to-right processing order**, where all structure must be encoded implicitly within a single evolving state.  \n",
    "As a result, hierarchical and relational patterns—such as nested clauses or subject–verb dependencies—must be flattened into a sequential representation.\n",
    "\n",
    "The architecture therefore exhibits a strong **bias toward linear order**, rather than structured relationships.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Architectural Question\n",
    "\n",
    "Instead of asking *how to encode structure into a sequence*, we ask:\n",
    "\n",
    "> Why should the model assume that sequence order is the primary organizing principle for relationships in the data?\n",
    "\n",
    "This exposes the deeper issue: **the architecture hard-codes a sequential worldview**, even when the underlying structure is hierarchical or relational.\n",
    "\n",
    "---\n",
    "\n",
    "### Required Architectural Property\n",
    "\n",
    "To overcome this bias, an architecture must allow:\n",
    "\n",
    "> **Relationships between elements to be modeled independently of their positions in a sequence.**\n",
    "\n",
    "This implies:\n",
    "- Structure should not be forced into a single temporal chain\n",
    "- Dependencies should be based on relationships, not just order\n",
    "- Multiple elements should interact directly, regardless of distance\n",
    "\n",
    "---\n",
    "\n",
    "### Structural Implications\n",
    "\n",
    "Instead of representing the sequence as a chain:\n",
    "\n",
    "$$\n",
    "h_1 \\rightarrow h_2 \\rightarrow \\dots \\rightarrow h_t\n",
    "$$\n",
    "\n",
    "the architecture should treat representations as a **set**:\n",
    "\n",
    "$$\n",
    "\\{h_1, h_2, \\dots, h_t\\}\n",
    "$$\n",
    "\n",
    "and allow interactions to be defined by learned relationships rather than fixed temporal adjacency.\n",
    "\n",
    "This removes the assumption that “next in time” implies “most relevant”.\n",
    "\n",
    "---\n",
    "\n",
    "### Effect on Learning Structured Patterns\n",
    "\n",
    "With relational rather than sequential bias:\n",
    "- Hierarchical dependencies can be modeled explicitly\n",
    "- Nested structures no longer need to be encoded indirectly\n",
    "- Grammar and long-range relations become natural outcomes, not edge cases\n",
    "\n",
    "The model is free to learn **structure first**, rather than reconstructing it from linear order.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "> The limitation is not the absence of structure in the data, but the assumption that structure must be expressed sequentially.\n",
    "\n",
    "Removing this assumption fundamentally changes what the model can represent easily.\n",
    "\n",
    "---\n",
    "\n",
    "### Terminology (Revealed After Derivation)\n",
    "\n",
    "An architectural design that models relationships between elements based on learned relevance, rather than fixed sequence order, supports flexible and relational representations.\n",
    "\n",
    "This principle is realized in **self-attention–based architectures**, where interactions are defined by content-based relationships rather than strict temporal progression.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Representational bias in LSTMs arises from enforcing a linear, sequential view of data. By allowing elements to interact based on learned relationships rather than fixed order, this architectural change enables more natural modeling of hierarchical and structured patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef542fb7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66366e89",
   "metadata": {},
   "source": [
    "## 6. Memory Access Lens → Architectural Fix\n",
    "\n",
    "### Limitation Recap (from Memory Access Lens)\n",
    "\n",
    "In LSTMs, long-term memory is stored entirely in a single cell state:\n",
    "\n",
    "$$\n",
    "c_t \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "Memory is **read** via the hidden state:\n",
    "\n",
    "$$\n",
    "h_t = o_t \\cdot \\tanh(c_t)\n",
    "$$\n",
    "\n",
    "and **written** at the same timestep using:\n",
    "\n",
    "$$\n",
    "c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t\n",
    "$$\n",
    "\n",
    "This design tightly couples memory reading and writing. As a result, memory access is **global** (the entire memory is exposed at once) and **destructive** (reading necessarily modifies memory through the update).\n",
    "\n",
    "There is no mechanism to selectively access, reuse, or preserve specific past information.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Architectural Question\n",
    "\n",
    "Instead of asking *how to store memory more carefully*, we ask:\n",
    "\n",
    "> Why must reading from memory necessarily overwrite or modify it?\n",
    "\n",
    "This reveals a fundamental constraint: **memory storage and memory access are inseparable** in the architecture.\n",
    "\n",
    "---\n",
    "\n",
    "### Required Architectural Property\n",
    "\n",
    "To overcome this limitation, an architecture must allow:\n",
    "\n",
    "> **Selective, non-destructive access to stored information.**\n",
    "\n",
    "This implies:\n",
    "- Memory can be read without being overwritten\n",
    "- Specific pieces of information can be accessed independently\n",
    "- Stored representations remain intact across multiple accesses\n",
    "\n",
    "---\n",
    "\n",
    "### Structural Implications\n",
    "\n",
    "Instead of a single mutable memory:\n",
    "\n",
    "$$\n",
    "\\text{Memory} \\;\\rightarrow\\; c_t\n",
    "$$\n",
    "\n",
    "the architecture should maintain a collection of stored representations:\n",
    "\n",
    "$$\n",
    "\\{h_1, h_2, \\dots, h_t\\}\n",
    "$$\n",
    "\n",
    "and provide a mechanism to:\n",
    "- select relevant representations\n",
    "- combine them for computation\n",
    "- leave the stored memory unchanged\n",
    "\n",
    "This separates **memory storage** from **memory usage**.\n",
    "\n",
    "---\n",
    "\n",
    "### Effect on Information Reuse\n",
    "\n",
    "With selective, non-destructive access:\n",
    "- The same past information can be reused multiple times\n",
    "- Reasoning over memory does not degrade stored content\n",
    "- Multiple distant memories can be compared or combined explicitly\n",
    "\n",
    "This enables iterative reasoning without memory corruption.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "> The limitation is not memory size, but the inability to **read memory without rewriting it**.\n",
    "\n",
    "Separating access from modification is essential for reliable long-term reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "### Terminology (Revealed After Derivation)\n",
    "\n",
    "An architectural design that retrieves stored representations selectively, without modifying them, supports content-based and non-destructive memory access.\n",
    "\n",
    "This principle is implemented in **self-attention–based architectures**, where stored representations are read-only and relevance is computed dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Memory access in LSTMs is global and destructive due to the coupling of reading and writing operations. By decoupling memory storage from memory access and enabling selective, non-destructive reads, this architectural change allows reliable reuse and comparison of past information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79001bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec396134",
   "metadata": {},
   "source": [
    "## 7. Computational Scaling Lens → Architectural Fix\n",
    "\n",
    "### Limitation Recap (from Computational Scaling Lens)\n",
    "\n",
    "In LSTMs, computation at each timestep depends strictly on the previous timestep:\n",
    "\n",
    "$$\n",
    "(h_t, c_t) = F(h_{t-1}, c_{t-1}, x_t)\n",
    "$$\n",
    "\n",
    "This recurrence enforces a hard sequential dependency. To compute the representation at timestep $t$, all previous timesteps $\\{1, \\dots, t-1\\}$ must be processed in order.\n",
    "\n",
    "As a consequence:\n",
    "- Forward computation is strictly sequential\n",
    "- Backpropagation through time (BPTT) is also sequential\n",
    "- Computation time scales linearly with sequence length\n",
    "\n",
    "---\n",
    "\n",
    "### Core Architectural Question\n",
    "\n",
    "Rather than asking *how to make recurrent computation faster*, we ask:\n",
    "\n",
    "> Why must sequence elements be processed one after another, instead of simultaneously?\n",
    "\n",
    "This reveals that the scalability issue arises from **temporal dependency in computation**, not from model size or optimization inefficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### Required Architectural Property\n",
    "\n",
    "To overcome this limitation, an architecture must allow:\n",
    "\n",
    "> **Sequence elements to be processed in parallel, without relying on timestep-to-timestep recurrence.**\n",
    "\n",
    "This implies:\n",
    "- No mandatory dependency on previous states for computation\n",
    "- Representations for all positions can be computed simultaneously\n",
    "- Learning does not require unrolling through time\n",
    "\n",
    "---\n",
    "\n",
    "### Structural Implications\n",
    "\n",
    "Instead of enforcing a computation chain:\n",
    "\n",
    "$$\n",
    "h_1 \\rightarrow h_2 \\rightarrow \\dots \\rightarrow h_t\n",
    "$$\n",
    "\n",
    "the architecture should allow representations:\n",
    "\n",
    "$$\n",
    "\\{h_1, h_2, \\dots, h_t\\}\n",
    "$$\n",
    "\n",
    "to be computed as a group, with interactions defined independently of processing order.\n",
    "\n",
    "This removes the need for sequential execution during both training and inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Effect on Training and Inference\n",
    "\n",
    "With parallelizable computation:\n",
    "- Training time no longer scales linearly with sequence length in wall-clock time\n",
    "- Long sequences become feasible without truncation\n",
    "- Models can leverage modern hardware efficiently\n",
    "\n",
    "This enables learning from full-context sequences rather than shortened approximations.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "> The bottleneck is not model complexity, but the assumption that time must dictate computation order.\n",
    "\n",
    "Removing recurrence removes the primary barrier to scalability.\n",
    "\n",
    "---\n",
    "\n",
    "### Terminology (Revealed After Derivation)\n",
    "\n",
    "An architectural design that removes timestep-to-timestep dependencies and allows parallel computation across sequence elements enables scalable sequence modeling.\n",
    "\n",
    "This principle is realized in **self-attention–based architectures**, where all positions are processed simultaneously using shared relational computations.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Computational scaling limitations in LSTMs arise from strict recurrence, which enforces sequential execution. By eliminating mandatory temporal dependencies and enabling parallel computation across sequence elements, this architectural change allows efficient training and inference on long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2072baf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8acffa73",
   "metadata": {},
   "source": [
    "## Concluding Synthesis: From LSTM Limitations to Architectural Principles\n",
    "\n",
    "Through a lens-based analysis of LSTMs, we identified a set of fundamental limitations that arise not from training procedures or data scarcity, but from the architectural assumptions embedded in recurrent computation. Each limitation exposes a specific constraint on how information is stored, accessed, propagated, and scaled across time.\n",
    "\n",
    "From the **gradient-flow lens**, we observed that long dependency chains force gradients to traverse many intermediate timesteps, leading to gradual attenuation. This revealed the need for architectures with shorter dependency paths between relevant sequence elements.  \n",
    "From the **capacity and compression lens**, we showed that compressing unbounded sequence information into a fixed-size memory inevitably causes information loss, motivating memory representations that scale with sequence length.  \n",
    "The **temporal locality lens** highlighted that LSTMs must decide the importance of information at the moment it is written, without access to future context, suggesting that relevance should instead be determined dynamically at the time of use.  \n",
    "Through the **credit assignment lens**, we demonstrated that errors propagate through compressed memory mixtures, preventing precise attribution of responsibility to specific past inputs and motivating content-based routing of learning signals.  \n",
    "The **representational bias lens** revealed that enforcing a strictly linear, sequential view of data makes hierarchical and relational structures unnatural to represent, indicating the need for architectures that model relationships independently of temporal order.  \n",
    "The **memory access lens** showed that coupling memory reading and writing leads to global, destructive access, motivating a separation between memory storage and selective, non-destructive retrieval.  \n",
    "Finally, the **computational scaling lens** exposed how strict recurrence enforces sequential execution, limiting parallelism and scalability, and motivating architectures that remove timestep-to-timestep computational dependencies.\n",
    "\n",
    "Viewed together, these lenses converge on a consistent set of architectural principles: memory should scale with input length, relevance should be computed dynamically, access to stored information should be selective and non-destructive, dependencies should be content-based rather than time-based, and computation should be parallelizable across sequence elements.\n",
    "\n",
    "Architectures that embody these principles do not emerge as arbitrary replacements for recurrent models, but as natural responses to the structural constraints revealed by this analysis. In this sense, modern sequence models can be understood not as incremental improvements over LSTMs, but as systematic architectural resolutions to the limitations imposed by recurrence itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb6d55",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
