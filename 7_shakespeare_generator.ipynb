{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e93fd3",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook presents a systematic experimental study on recurrent neural network architectures—specifically a family of custom-built Gated Recurrent Unit (GRU) models collectively referred to as the **Astra-GRU Series**.\n",
    "The objective of this work is to evaluate the representational capacity, learning dynamics, and generative performance of three architectural variants (Astra-α, Astra-β, and Astra-γ) when trained on a **character-level language modeling task** using the *Tiny Shakespeare* corpus.\n",
    "\n",
    "All models in this study are implemented **entirely from first principles**, relying on a manually constructed GRUCell class defined previously in *6_GRU_PyTorch_version.ipynb*.\n",
    "No PyTorch-native recurrent modules (e.g., `nn.GRU`, `nn.GRUCell`) are used. Instead, every gating mechanism, affine transformation, nonlinear activation, and recurrent update rule follows the original GRU formulation while deliberately diverging from PyTorch’s implementation in several key aspects:\n",
    "\n",
    "* **Single-bias design**, in contrast to PyTorch’s dual-bias formulation for each gate.\n",
    "* **Reset-gate interaction applied directly to the previous hidden state** prior to affine transformation, matching the theoretical GRU definition rather than PyTorch’s optimized variant.\n",
    "\n",
    "These modifications allow for a transparent examination of the GRU’s internal mechanics and yield an instructive comparison between canonical formulations and optimized library implementations.\n",
    "\n",
    "The models are trained on the full Tiny Shakespeare dataset, a compact yet stylistically rich corpus containing dramatic dialogues, stage cues, and poetic structures. An excerpt is shown below:\n",
    "\n",
    "```\n",
    "First Citizen:\n",
    "Before we proceed any further, hear me speak.\n",
    "\n",
    "All:\n",
    "Speak, speak.\n",
    "\n",
    "First Citizen:\n",
    "You are all resolved rather to die than to famish?\n",
    "\n",
    "All:\n",
    "Resolved. resolved.\n",
    "```\n",
    "\n",
    "To investigate the effect of architectural depth and hidden-state dimensionality on sequence modeling performance, three GRU configurations of increasing complexity are trained:\n",
    "\n",
    "* **Astra-α** — a lightweight baseline model\n",
    "* **Astra-β** — a medium-capacity two-layer GRU\n",
    "* **Astra-γ** — a large three-layer recurrent model with significantly expanded hidden representation\n",
    "\n",
    "Each model is evaluated on its ability to learn long-range dependencies within Shakespearean text and generate coherent, stylistically consistent sequences under various sampling regimes.\n",
    "Comprehensive summaries—including architecture breakdown, trainable parameter counts, and hyperparameter settings—are provided for all three trained variants.\n",
    "Model weights can be supplied upon request.\n",
    "\n",
    "This notebook documents the full experimental workflow, from data preprocessing and model construction to training, evaluation, and autoregressive text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70659dc1",
   "metadata": {},
   "source": [
    "## Preparing Training and Validation Dataset for Tiny Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f672707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "text = open(\"tiny_shakespeare.txt\", 'r', encoding='utf-8').read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "\n",
    "data = torch.tensor([stoi[c] for c in text], dtype=torch.long)\n",
    "seq_length = 128  # sequence length\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split='train', batch_size=64):\n",
    "    source = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(source) - seq_length - 1, (batch_size,))\n",
    "    X = torch.stack([source[i:i+seq_length] for i in ix])\n",
    "    Y = torch.stack([source[i+1:i+seq_length+1] for i in ix])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a02129",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = get_batch(split = 'train', batch_size = 64)\n",
    "x_val, y_val = get_batch(split = 'validation', batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0811b14c",
   "metadata": {},
   "source": [
    "## GRUCell's Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dad063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    \"\"\" \n",
    "    This Implementation is Differ from the PyTorch's Official Implementation in 2 Different Ways\n",
    "    NOTE-1:\n",
    "        This is not the official Implementation of PyTorch's GRUcell Because they use 2 biases per Gate\n",
    "        and i'm only using 1 bias per Gate\n",
    "\n",
    "        PyTorch's Official Implementation: \n",
    "        r = σ(W_ir x + b_ir + W_hr h + b_hr)\n",
    "        z = σ(W_iz x + b_iz + W_hz h + b_hz)\n",
    "        n = tanh(W_in x + b_in + r ⊙ (W_hn h + b_hn))\n",
    "\n",
    "        They Use 2 Bias per Gate\n",
    "    \n",
    "    NOTE-2: \n",
    "        They apply the reset gate (r) after the Multiplication of W_hn and addition of b_hn on the h_prev\n",
    "        2. Original implementation: Apply the Hadamard product (⊙) between r_t and h_prev and then apply the\n",
    "            Matrix Transformation and bias addition\n",
    "        3. What PyTorch does is, they apply the Matrix Transformation (Matrix Multiplication and bias addition) 1st and then\n",
    "            they apply the Hadamard product (⊙) between (W_hn h + b_hn)\n",
    "    \"\"\"\n",
    "    def __init__(self, embd_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        -> Bias only on x: input\n",
    "        -> No Bias on Hidden States\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Candidate transformation\n",
    "        self.Wx = nn.Linear(embd_dim, hidden_dim, bias = True)\n",
    "        self.Wh = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "\n",
    "        # Update Gate Specific Parameters\n",
    "        self.Wzx = nn.Linear(embd_dim, hidden_dim, bias = True)\n",
    "        self.Wzh = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.bias_z = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        # Reset Gate Specific Parameters\n",
    "        self.Wrx = nn.Linear(embd_dim, hidden_dim, bias = True)\n",
    "        self.Wrh = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.bias_r = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        --> A proposed update → candidate (h̃_t)\n",
    "        --> A decision gate → update gate (z_t)\n",
    "        --> A final controlled update → h_t\n",
    "\n",
    "        NOTE:   1. Reset Gate Filters h_prev\n",
    "                    - r_t = sigmoid( ( (x_t @ W_rx) + (h_prev @ W_rh) + b_r) )\n",
    "                2. Apply filter to h_prev\n",
    "                    - filtered_h_prev = r_t * h_prev [NOTE: (where * is element-wise multiplication)]\n",
    "                        - Meaning:\n",
    "                            - If r_t ≈ 0 → ignore old memory when forming candidate\n",
    "                            - If r_t ≈ 1 → use old memory fully\n",
    "                3. Compute candidate\n",
    "                    - h̃_t = tanh( ( (x_t @ W_hx) + (filtered_h_prev @ W_hh) + b_h) )\n",
    "                        - Meaning: \n",
    "                            - This produces a new memory proposal: A proposed update\n",
    "                4. Final hidden state\n",
    "                    - h_t = (1 - z_t) * h_prev + z_t * h̃_t [NOTE: (where * is element-wise multiplication)]\n",
    "        \"\"\"\n",
    "        r_t = torch.sigmoid((self.Wrx(x)) + (self.Wrh(h_prev)) + self.bias_r)\n",
    "        z_t = torch.sigmoid((self.Wzx(x)) + (self.Wzh(h_prev)) + self.bias_z)\n",
    "        h_tilde = torch.tanh(self.Wx(x) + self.Wh(r_t * h_prev))\n",
    "\n",
    "        h = (1 - z_t) * h_prev + z_t * h_tilde\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a45bc",
   "metadata": {},
   "source": [
    "## GRULayer's Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efdf23cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULayer(nn.Module):\n",
    "    def __init__(self, embd_dim, hidden_dim, dropout = 0.0):\n",
    "        super().__init__()\n",
    "        self.grucell = GRUCell(embd_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, h_prev = None):\n",
    "        batch, seq_length, _ = x.shape # x.shape --> batch, seq_length, embd_dim\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(batch, self.grucell.hidden_dim, device = x.device)\n",
    "\n",
    "        hidden_states = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h_prev = self.grucell(x_t, h_prev)\n",
    "            h_prev = self.dropout(h_prev)\n",
    "            hidden_states.append(h_prev)\n",
    "        \n",
    "        # Stack list into tensor\n",
    "        hidden_states = torch.stack(hidden_states, dim=1)\n",
    "        return hidden_states\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81255bb6",
   "metadata": {},
   "source": [
    "## Linear Layer Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ff016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_classes):\n",
    "        \"\"\"\n",
    "        This layer performs a linear projection on the GRU hidden states.\n",
    "        It maps the hidden vector (of size hidden_dim) into the vocabulary space (n_classes)\n",
    "        by applying a learnable affine transformation:\n",
    "\n",
    "            logits = W h + b\n",
    "\n",
    "        This is used to convert each GRU hidden state into class probabilities\n",
    "        (e.g., next-character prediction in a name generation model).\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.linear_projection = nn.Linear(in_features = hidden_dim, out_features = n_classes, bias = True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_projection(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51269fd3",
   "metadata": {},
   "source": [
    "## Custom GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc1ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_dim, hidden_dim, num_layers, model_name, dropout=0.0, ):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_classes = vocab_size\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embd_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(GRULayer(embd_dim, hidden_dim, dropout))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(GRULayer(hidden_dim, hidden_dim, dropout))\n",
    "\n",
    "        self.fc = Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_length)\n",
    "        x = self.embedding(x)   # (batch, seq, embd_dim)\n",
    "\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)  # (batch, seq, hidden_dim)\n",
    "\n",
    "        logits = self.fc(h)     # (batch, seq, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c4ad78",
   "metadata": {},
   "source": [
    "## Function for Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7af9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def train_model(\n",
    "        model: MyGRUModel,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss_fn,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        device,\n",
    "        clip_value=1.0,\n",
    "        val_interval=1,\n",
    "        steps = 200\n",
    "    ):\n",
    "    \n",
    "    print(f\"\\n---------------- Training Started for {model.model_name} Model ----------------\\n\")\n",
    "    # steps ---> How many batches will get involve in forwardpass and backward pass\n",
    "    # steps = 200, and batch_size = 64 meaning 200 batches of each size = 64 will get involved in forwardpass and backward pass\n",
    "    # 200 * 64 * seq_length = 200 * 64 * 128 = 1.64M tokens/epoch for forward pass and 1.46M token/epoch for backward pass\n",
    "    # so for larger models, keep larger steps, for Astra-gamma step = 400 \n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for _ in range(steps):\n",
    "            X, Y = get_batch(split=\"train\", batch_size=batch_size)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                Y.reshape(-1)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= steps\n",
    "\n",
    "        # Validation\n",
    "        val_loss = None\n",
    "        if epoch % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Xv, Yv = get_batch(split=\"val\", batch_size=batch_size)\n",
    "                Xv, Yv = Xv.to(device), Yv.to(device)\n",
    "\n",
    "                logits = model(Xv)\n",
    "                val_loss = loss_fn(\n",
    "                    logits.reshape(-1, logits.size(-1)),\n",
    "                    Yv.reshape(-1)\n",
    "                ).item()\n",
    "\n",
    "        # Lr Scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Epoch and Loss Details\n",
    "        if val_loss is not None:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\n---------------- Training Completed for {model.model_name} Model ----------------\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427fdaae",
   "metadata": {},
   "source": [
    "## Sampling Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a549e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_greedy(model, stoi, itos, start_text=\"A\", max_new_tokens=200):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Encode start text\n",
    "    input_ids = torch.tensor([stoi[c] for c in start_text], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids[:, -1:])  \n",
    "        # logits: (1, 1, vocab_size)\n",
    "\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1)  # greedy pick\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_id.unsqueeze(0)], dim=1)\n",
    "\n",
    "    return ''.join(itos[i] for i in input_ids[0].tolist())\n",
    "\n",
    "\n",
    "def sample_with_temperature(model, stoi, itos, start_text=\"A\", max_new_tokens=200, temperature=1.0):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_ids = torch.tensor([stoi[c] for c in start_text], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids[:, -1:])\n",
    "        logits = logits[:, -1, :] / temperature  \n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_id], dim=1)\n",
    "\n",
    "    return ''.join(itos[i] for i in input_ids[0].tolist())\n",
    "\n",
    "\n",
    "def sample_top_k(model, stoi, itos, start_text=\"A\", max_new_tokens=200, k=20, temperature=1.0):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_ids = torch.tensor([stoi[c] for c in start_text], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids[:, -1:])\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        # Keep only top-k logits\n",
    "        topk_vals, topk_idx = torch.topk(logits, k)\n",
    "        \n",
    "        probs = F.softmax(topk_vals, dim=-1)\n",
    "\n",
    "        # Sample from top-k\n",
    "        sampled_idx = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        next_id = topk_idx.gather(-1, sampled_idx)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_id], dim=1)\n",
    "\n",
    "    return ''.join(itos[i] for i in input_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80577a",
   "metadata": {},
   "source": [
    "## Function for Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11ca2c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model(model: MyGRUModel, base_name=\"Astra\", path=\"./saved_models/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # versioning\n",
    "    existing = [f for f in os.listdir(path) if f.startswith(base_name) and f.endswith(\".pth\")]\n",
    "    versions = []\n",
    "    for f in existing:\n",
    "        parts = f.replace(\".pth\", \"\").split(\"_v\")\n",
    "        if len(parts) == 2 and parts[1].isdigit():\n",
    "            versions.append(int(parts[1]))\n",
    "    next_version = max(versions, default=0) + 1\n",
    "\n",
    "    filename = f\"{base_name}_v{next_version}.pth\"\n",
    "    save_path = os.path.join(path, filename)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"model_class\": model.__class__.__name__,\n",
    "        \"model_name\": model.model_name,\n",
    "        \"n_classes\": model.n_classes,\n",
    "        \"embd_dim\": model.embd_dim,\n",
    "        \"hidden_dim\": model.hidden_dim,\n",
    "        \"dropout\": model.dropout,\n",
    "        \"vocab_size\": model.vocab_size,\n",
    "        \"num_layers\": model.num_layers,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"version\": next_version,\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, save_path)\n",
    "    torch.save(checkpoint, os.path.join(path, f\"{base_name}_latest.pth\"))\n",
    "\n",
    "    print(f\"\\nModel saved at: {save_path}\")\n",
    "    print(f\"Also updated: {base_name}_latest.pth\\n\")\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd8516",
   "metadata": {},
   "source": [
    "## Function for Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def load_model(filepath, device):\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "\n",
    "    # extract architecture parameters from checkpoint\n",
    "    model_name  = checkpoint[\"model_name\"]\n",
    "    vocab_size  = checkpoint[\"vocab_size\"]\n",
    "    n_classes   = checkpoint[\"n_classes\"]\n",
    "    embd_dim    = checkpoint[\"embd_dim\"]\n",
    "    hidden_dim  = checkpoint[\"hidden_dim\"]\n",
    "    dropout     = checkpoint[\"dropout\"]\n",
    "\n",
    "    # instantiate model using all saved metadata\n",
    "    model = MyGRUModel(\n",
    "        vocab_size = vocab_size,\n",
    "        embd_dim   = embd_dim,\n",
    "        hidden_dim = hidden_dim,\n",
    "        dropout    = dropout,\n",
    "        model_name = model_name\n",
    "    ).to(device)\n",
    "\n",
    "    # load weights\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "    # pretty print metadata\n",
    "    print(\"\\n================ MODEL LOADED ================\")\n",
    "    print(f\"Loaded File      : {filepath}\")\n",
    "    print(f\"Model Name       : {model_name}\")\n",
    "    print(f\"Model Class      : {checkpoint['model_class']}\")\n",
    "    print(f\"Version          : v{checkpoint['version']}\")\n",
    "    print(f\"Timestamp        : {checkpoint['timestamp']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(\"Model Architecture:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if name != \"\":\n",
    "            print(f\"  └── {name}: {module.__class__.__name__}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(f\"Total Parameters : {count_parameters(model):,}\")\n",
    "    print(f\"Loaded on Device : {device}\")\n",
    "    print(\"==============================================\\n\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e7fe7",
   "metadata": {},
   "source": [
    "## Function for Printing the Summary of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c909797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_summary(model, model_name, epochs, lr, device):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ASTRA-GRU MODEL SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Model Name       : {model_name}\")\n",
    "    print(f\"Device           : {device}\")\n",
    "    print(f\"Total Epochs     : {epochs}\")\n",
    "    print(f\"Learning Rate    : {lr}\")\n",
    "\n",
    "    print(\"\\nMODEL ARCHITECTURE\")\n",
    "    print(\"-\"*100)\n",
    "    for name, module in model.named_modules():\n",
    "        if name == \"\":\n",
    "            continue\n",
    "        print(f\"  └── {name}: {module.__class__.__name__}()\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model_size_mb = n_params * 4 / (1024**2)\n",
    "\n",
    "    print(f\"\\nTrainable Parameters : {n_params:,}\")\n",
    "    print(f\"Model Size : {model_size_mb:.2f} MB\")\n",
    "\n",
    "    print(\"\\nPARAMETER BREAKDOWN\")\n",
    "    print(\"-\"*100)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name:40s} : {param.numel():,}\")\n",
    "\n",
    "    print(\"=\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b4491",
   "metadata": {},
   "source": [
    "# **Astra-GRU Model Family: Architectural Variants and Training Configurations**\n",
    "\n",
    "The **Astra-GRU** series defines three progressively scaled recurrent architectures—**Astra-α**, **Astra-β**, and **Astra-γ**—designed for character-level language modeling on the Tiny Shakespeare corpus.\n",
    "Each model explores a different point on the capacity–efficiency trade-off, enabling controlled experiments in representational depth, sequential reasoning ability, and generalization performance.\n",
    "\n",
    "The following sections describe the architecture and training configuration for each model variant.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Astra-α Model (Small Configuration)**\n",
    "\n",
    "### **Architectural Description**\n",
    "\n",
    "Astra-α is the minimal configuration of the series, intended for rapid experimentation, debugging, and establishing performance baselines.\n",
    "It consists of:\n",
    "\n",
    "* A character embedding layer\n",
    "* A single handcrafted GRU layer (built using the custom GRUCell implementation)\n",
    "* A linear projection head for next-character prediction\n",
    "\n",
    "This configuration prioritizes efficiency, low memory footprint, and training speed.\n",
    "\n",
    "### **Hyperparameter Configuration**\n",
    "\n",
    "```\n",
    "embedding_dim = 64\n",
    "hidden_dim    = 128\n",
    "num_layers    = 1\n",
    "epochs        = 10\n",
    "learning_rate = 3e-3\n",
    "weight_decay  = 0.01\n",
    "optimizer     = AdamW\n",
    "scheduler     = CosineAnnealingLR\n",
    "dropout       = 0.1\n",
    "batch_size    = 64\n",
    "seq_length    = 128\n",
    "```\n",
    "\n",
    "### **Purpose and Expected Behavior**\n",
    "\n",
    "Astra-α serves as a compact baseline for understanding convergence behavior and validating training mechanics.\n",
    "It captures short-range patterns, punctuation structure, and local dependencies, but is expected to underfit the full Shakespeare corpus due to limited representational capacity.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Astra-β Model (Medium Configuration)**\n",
    "\n",
    "### **Architectural Description**\n",
    "\n",
    "Astra-β increases depth and expressiveness by stacking **two GRULayer blocks**, enabling the model to integrate information over longer contexts.\n",
    "This architecture provides a balanced midpoint—substantially more capable than Astra-α while remaining computationally accessible.\n",
    "\n",
    "The model includes:\n",
    "\n",
    "* Embedding layer\n",
    "* GRU Layer 1 → GRU Layer 2\n",
    "* Linear projection layer\n",
    "\n",
    "The deeper recurrence allows richer temporal abstractions and more coherent multi-line generation.\n",
    "\n",
    "### **Hyperparameter Configuration**\n",
    "\n",
    "```\n",
    "embedding_dim = 128\n",
    "hidden_dim    = 256\n",
    "num_layers    = 2\n",
    "epochs        = 15\n",
    "learning_rate = 2e-3\n",
    "weight_decay  = 0.01\n",
    "optimizer     = AdamW\n",
    "scheduler     = CosineAnnealingLR\n",
    "dropout       = 0.1\n",
    "batch_size    = 64\n",
    "seq_length    = 128\n",
    "```\n",
    "\n",
    "### **Purpose and Expected Behavior**\n",
    "\n",
    "Astra-β is a strong general-purpose model, delivering noticeably better syntactic consistency and character-to-character coherence.\n",
    "It serves as the main reference point for qualitative text generation within the Astra-GRU family.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Astra-γ Model (Large Configuration)**\n",
    "\n",
    "### **Architectural Description**\n",
    "\n",
    "Astra-γ is the largest and most expressive model in the series, extending the architecture to **three stacked GRU layers** with wider embedding and hidden dimensions.\n",
    "This design maximizes temporal modeling capacity and enables robust generation across multi-sentence Shakespeare-style passages.\n",
    "\n",
    "The model architecture comprises:\n",
    "\n",
    "* Embedding layer\n",
    "* GRU Layer 1 → GRU Layer 2 → GRU Layer 3\n",
    "* Linear projection head\n",
    "\n",
    "The increased depth and width allow Astra-γ to learn long-range dependencies, dialogue structure, and character-consistent phrasing.\n",
    "\n",
    "### **Hyperparameter Configuration**\n",
    "\n",
    "```\n",
    "embedding_dim = 256\n",
    "hidden_dim    = 512\n",
    "num_layers    = 3\n",
    "epochs        = 20\n",
    "learning_rate = 1e-3\n",
    "weight_decay  = 0.01\n",
    "optimizer     = AdamW\n",
    "scheduler     = CosineAnnealingLR\n",
    "dropout       = 0.1\n",
    "batch_size    = 64\n",
    "seq_length    = 128\n",
    "```\n",
    "\n",
    "### **Purpose and Expected Behavior**\n",
    "\n",
    "Astra-γ is optimized for high-quality generative performance, producing longer coherent passages and capturing stylistic nuances of Shakespearean dialogue.\n",
    "This configuration leverages depth, wider hidden channels, and stable optimization to achieve the strongest sampling quality in the series.\n",
    "\n",
    "---\n",
    "\n",
    "# **Summary**\n",
    "\n",
    "The Astra-GRU family—**Astra-α**, **Astra-β**, and **Astra-γ**—provides a structured progression of model capacities tailored for controlled experimentation in recurrent sequence modeling.\n",
    "\n",
    "| Model       | Layers | Hidden | Embedding | Expected Behavior                                |\n",
    "| ----------- | ------ | ------ | --------- | ------------------------------------------------ |\n",
    "| **Astra-α** | 1      | 128    | 64        | Efficient baseline, learns local structure       |\n",
    "| **Astra-β** | 2      | 256    | 128       | Balanced model, strong coherence                 |\n",
    "| **Astra-γ** | 3      | 512    | 256       | Highest quality generation and context retention |\n",
    "\n",
    "These architectures form a scalable framework for evaluating the effect of depth and representational power on character-level language modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321f7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78164269",
   "metadata": {},
   "source": [
    "## Astra-GRU Architecture Family: Defining the Astra-α, Astra-β, and Astra-γ Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5688c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------------------------------------------------- Small GRU Model -------------------------------------------------------- \n",
    "small_model_name = \"Astra-α\"\n",
    "model_small = MyGRUModel(\n",
    "    vocab_size = len(stoi),\n",
    "    embd_dim = 64,\n",
    "    hidden_dim = 128,\n",
    "    num_layers = 1,\n",
    "    model_name = small_model_name,\n",
    "    dropout = 0.1\n",
    ").to(device)\n",
    "lr_small_model = 3e-3\n",
    "weight_decay_small = 0.01\n",
    "optimizer_small = torch.optim.AdamW(\n",
    "    model_small.parameters(),\n",
    "    lr = lr_small_model,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "epochs_small_model = 10\n",
    "scheduler_small = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_small,\n",
    "    T_max = epochs_small_model\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------- Medium GRU Model -------------------------------------------------------- \n",
    "medium_model_name = \"Astra-β\"\n",
    "model_medium = MyGRUModel(\n",
    "    vocab_size = len(stoi),\n",
    "    embd_dim = 128,\n",
    "    hidden_dim = 256,\n",
    "    num_layers = 2,\n",
    "    model_name = medium_model_name,\n",
    "    dropout = 0.1\n",
    ").to(device)\n",
    "lr_medium_model = 2e-3\n",
    "weight_decay_medium = 0.01\n",
    "optimizer_medium = torch.optim.AdamW(\n",
    "    model_medium.parameters(),\n",
    "    lr = lr_medium_model,\n",
    "    weight_decay = weight_decay_medium\n",
    ")\n",
    "epochs_medium_model = 15\n",
    "scheduler_medium = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_medium,\n",
    "    T_max=epochs_medium_model\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------- Large GRU Model -------------------------------------------------------- \n",
    "large_model_name = \"Astra-γ\"\n",
    "model_large = MyGRUModel(\n",
    "    vocab_size = len(stoi),\n",
    "    embd_dim = 256,\n",
    "    hidden_dim = 512,\n",
    "    num_layers = 3,\n",
    "    model_name = large_model_name,\n",
    "    dropout = 0.1\n",
    ").to(device)\n",
    "\n",
    "lr_large_model = 1e-3\n",
    "epochs_large_model = 20\n",
    "weight_decay_large = 0.01\n",
    "\n",
    "optimizer_large = torch.optim.AdamW(\n",
    "    model_large.parameters(),\n",
    "    lr = lr_large_model,\n",
    "    weight_decay = weight_decay_large\n",
    ")\n",
    "\n",
    "# CosineAnnealingLR scheduler: It will produce cleaner convergence and noticeably better text quality.\n",
    "scheduler_large = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_large,\n",
    "    T_max = epochs_large_model\n",
    ")\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528fc96f",
   "metadata": {},
   "source": [
    "### Astra-α Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2c8794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ASTRA-GRU MODEL SUMMARY\n",
      "====================================================================================================\n",
      "Model Name       : Astra-α\n",
      "Device           : cuda\n",
      "Total Epochs     : 10\n",
      "Learning Rate    : 0.003\n",
      "\n",
      "MODEL ARCHITECTURE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  └── embedding: Embedding()\n",
      "  └── layers: ModuleList()\n",
      "  └── layers.0: GRULayer()\n",
      "  └── layers.0.grucell: GRUCell()\n",
      "  └── layers.0.grucell.Wx: Linear()\n",
      "  └── layers.0.grucell.Wh: Linear()\n",
      "  └── layers.0.grucell.Wzx: Linear()\n",
      "  └── layers.0.grucell.Wzh: Linear()\n",
      "  └── layers.0.grucell.Wrx: Linear()\n",
      "  └── layers.0.grucell.Wrh: Linear()\n",
      "  └── layers.0.dropout: Dropout()\n",
      "  └── fc: Linear()\n",
      "  └── fc.linear_projection: Linear()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trainable Parameters : 86,913\n",
      "Model Size : 0.33 MB\n",
      "\n",
      "PARAMETER BREAKDOWN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "embedding.weight                         : 4,160\n",
      "layers.0.grucell.bias_z                  : 128\n",
      "layers.0.grucell.bias_r                  : 128\n",
      "layers.0.grucell.Wx.weight               : 8,192\n",
      "layers.0.grucell.Wx.bias                 : 128\n",
      "layers.0.grucell.Wh.weight               : 16,384\n",
      "layers.0.grucell.Wzx.weight              : 8,192\n",
      "layers.0.grucell.Wzx.bias                : 128\n",
      "layers.0.grucell.Wzh.weight              : 16,384\n",
      "layers.0.grucell.Wrx.weight              : 8,192\n",
      "layers.0.grucell.Wrx.bias                : 128\n",
      "layers.0.grucell.Wrh.weight              : 16,384\n",
      "fc.linear_projection.weight              : 8,320\n",
      "fc.linear_projection.bias                : 65\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_model_summary(model = model_small,\n",
    "                    model_name = small_model_name,\n",
    "                    epochs = epochs_small_model,\n",
    "                    lr = lr_small_model,\n",
    "                    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c71882",
   "metadata": {},
   "source": [
    "### Astra-α Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a8beca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- Training Started for Astra-α Model ----------------\n",
      "\n",
      "Epoch 01/10 | Train Loss: 2.2413 | Val Loss: 1.8872\n",
      "Epoch 02/10 | Train Loss: 1.8197 | Val Loss: 1.7809\n",
      "Epoch 03/10 | Train Loss: 1.7299 | Val Loss: 1.7230\n",
      "Epoch 04/10 | Train Loss: 1.6936 | Val Loss: 1.7377\n",
      "Epoch 05/10 | Train Loss: 1.6695 | Val Loss: 1.7098\n",
      "Epoch 06/10 | Train Loss: 1.6551 | Val Loss: 1.7342\n",
      "Epoch 07/10 | Train Loss: 1.6400 | Val Loss: 1.6682\n",
      "Epoch 08/10 | Train Loss: 1.6335 | Val Loss: 1.7225\n",
      "Epoch 09/10 | Train Loss: 1.6333 | Val Loss: 1.6444\n",
      "Epoch 10/10 | Train Loss: 1.6276 | Val Loss: 1.7015\n",
      "\n",
      "---------------- Training Completed for Astra-α Model ----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_small = train_model(\n",
    "    model=model_small,\n",
    "    optimizer=optimizer_small,\n",
    "    scheduler=scheduler_small,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=epochs_small_model,\n",
    "    batch_size=64,\n",
    "    device=device,\n",
    "    steps = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dbe80a",
   "metadata": {},
   "source": [
    "### Astra-α Autoregressive Generation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386df16",
   "metadata": {},
   "source": [
    "#### Greedy Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58af481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n"
     ]
    }
   ],
   "source": [
    "print(sample_greedy(model_small, stoi, itos, \"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5184c0bd",
   "metadata": {},
   "source": [
    "#### Temperature Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fcc9a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING: lind se alivearalatare' chorineeres ary, withing hande lar wligisas wivenngositheng, me wis.\n",
      "STI ans oue toainourd,\n",
      "I vethiconong, weth'sountlalivent mes'spri'thesunr y seancke me oncerelenttelinswere\n"
     ]
    }
   ],
   "source": [
    "print(sample_with_temperature(model_small, stoi, itos, \"KING: \", temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ff89d",
   "metadata": {},
   "source": [
    "#### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a94ebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: wiserakend mplanenttham fre?\n",
      "I: I'MBlentoor\n",
      "Wendowinotwan fldspbon\n",
      "Win haces:\n",
      "Whid, wasamime r\n",
      "WI\n",
      "Amonisulabounoun,\n",
      "I:\n",
      "WARI:\n",
      "Whenas iknwhithod ldard:\n",
      "LOr I aist.\n",
      "TI; ide t Reralan d h\n",
      "HE, hy me\n",
      "UF n a\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sample_top_k(model_small, stoi, itos, \"FIRST CITIZEN: \", k=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d557668e",
   "metadata": {},
   "source": [
    "### Astra-α Model Checkpoint Saving and Archival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3be745b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at: ./astra_saved_models\\Astra_alpha_v1.pth\n",
      "Also updated: Astra_alpha_latest.pth\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./astra_saved_models\\\\Astra_alpha_v1.pth'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(model = model_small, base_name = \"Astra_alpha\", path = \"./astra_saved_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77131d9b",
   "metadata": {},
   "source": [
    "# **Astra-α: Results Summary**\n",
    "\n",
    "### **Model Overview**\n",
    "\n",
    "Astra-α represents the smallest configuration within the Astra-GRU family.\n",
    "It utilizes a single GRU layer (hidden size 128, embedding size 64) and serves as a computationally lightweight baseline for assessing the effect of recurrent capacity on character-level language modeling.\n",
    "Despite its constrained parameter budget, Astra-α exhibits clear evidence of learning the statistical and structural regularities embedded within the Tiny Shakespeare corpus.\n",
    "\n",
    "---\n",
    "\n",
    "# **Training Dynamics**\n",
    "\n",
    "Training converged smoothly, with the loss decreasing across epochs and validation loss stabilizing around ~1.66–1.72.\n",
    "The loss curve reflects a stable learning trajectory without signs of divergence or gradient instability—consistent with small-scale GRU architectures.\n",
    "\n",
    "Astra-α successfully internalized:\n",
    "\n",
    "* Local character-to-character transitions\n",
    "* Basic syntactic rhythms\n",
    "* Dialogue structure patterns (e.g., prefixes like “ROMEO:” or “FIRST CITIZEN:”)\n",
    "* Punctuation placement and line-break behavior\n",
    "\n",
    "However, due to the limited representational capacity of a single recurrent layer, the model displays expected constraints in long-range dependency modeling and semantic consistency.\n",
    "\n",
    "---\n",
    "\n",
    "# **Qualitative Generation Analysis**\n",
    "\n",
    "### **Greedy Sampling**\n",
    "\n",
    "Greedy decoding rapidly collapses into highly repetitive sequences:\n",
    "\n",
    "```\n",
    "ROMEO: the the the the the the the the the ...\n",
    "```\n",
    "\n",
    "This behavior is characteristic of small autoregressive models, where the most probable next token dominates the distribution, driving the model into a loop of locally optimal but globally degenerate predictions.\n",
    "This failure mode is expected and does not reflect poor training, but rather the intrinsic limitations of the greedy strategy for underparameterized models.\n",
    "\n",
    "---\n",
    "\n",
    "### **Temperature-Controlled Sampling**\n",
    "\n",
    "At moderate temperatures (e.g., 0.8), Astra-α produces text with considerably more structural variety:\n",
    "\n",
    "```\n",
    "KING: lind se alivearalatare' chorineeres ary, withing hande lar wligisas...\n",
    "```\n",
    "\n",
    "Key observations:\n",
    "\n",
    "* Generated tokens resemble English phonotactics.\n",
    "* Sentence-level flow exhibits quasi-syntactic coherence.\n",
    "* Punctuation, spacing, and capitalization are reproduced accurately.\n",
    "* Word-like constructs appear regularly, despite the absence of explicit word modeling.\n",
    "\n",
    "This demonstrates that the model learned meaningful local structure and stylistic signatures characteristic of Shakespearean text.\n",
    "\n",
    "---\n",
    "\n",
    "### **Top-K Sampling**\n",
    "\n",
    "Top-K decoding further stabilizes the output by restricting sampling to plausible character candidates:\n",
    "\n",
    "```\n",
    "FIRST CITIZEN: wiserakend mplanenttham fre ?\n",
    "...\n",
    "```\n",
    "\n",
    "This mode yields:\n",
    "\n",
    "* Improved formatting consistency\n",
    "* Better preservation of dialogue patterns\n",
    "* More controlled creativity and reduced noise\n",
    "* Frequent production of Shakespeare-like pseudo-words\n",
    "\n",
    "Under these constraints, Astra-α displays its strongest generative performance, revealing its ability to encode stylistic priors even with limited capacity.\n",
    "\n",
    "---\n",
    "\n",
    "# **Overall Assessment**\n",
    "\n",
    "Astra-α achieves the primary goals for a baseline recurrent architecture:\n",
    "\n",
    "* It learns the character-level distribution of a stylistically rich corpus.\n",
    "* It generalizes sufficiently to produce coherent local sequences.\n",
    "* It reproduces formatting conventions and phonetic structure.\n",
    "* It demonstrates expected limitations in global coherence and semantic fidelity.\n",
    "\n",
    "The model functions as a robust baseline for comparing deeper or wider GRU variants. Its performance provides clear evidence that architectural scaling (Astra-β and Astra-γ) will yield substantial improvements in long-range consistency, diversity, and stylistic imitation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c866261b",
   "metadata": {},
   "source": [
    "### Astra-β Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96c5ba66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ASTRA-GRU MODEL SUMMARY\n",
      "====================================================================================================\n",
      "Model Name       : Astra-β\n",
      "Device           : cuda\n",
      "Total Epochs     : 15\n",
      "Learning Rate    : 0.002\n",
      "\n",
      "MODEL ARCHITECTURE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  └── embedding: Embedding()\n",
      "  └── layers: ModuleList()\n",
      "  └── layers.0: GRULayer()\n",
      "  └── layers.0.grucell: GRUCell()\n",
      "  └── layers.0.grucell.Wx: Linear()\n",
      "  └── layers.0.grucell.Wh: Linear()\n",
      "  └── layers.0.grucell.Wzx: Linear()\n",
      "  └── layers.0.grucell.Wzh: Linear()\n",
      "  └── layers.0.grucell.Wrx: Linear()\n",
      "  └── layers.0.grucell.Wrh: Linear()\n",
      "  └── layers.0.dropout: Dropout()\n",
      "  └── layers.1: GRULayer()\n",
      "  └── layers.1.grucell: GRUCell()\n",
      "  └── layers.1.grucell.Wx: Linear()\n",
      "  └── layers.1.grucell.Wh: Linear()\n",
      "  └── layers.1.grucell.Wzx: Linear()\n",
      "  └── layers.1.grucell.Wzh: Linear()\n",
      "  └── layers.1.grucell.Wrx: Linear()\n",
      "  └── layers.1.grucell.Wrh: Linear()\n",
      "  └── layers.1.dropout: Dropout()\n",
      "  └── fc: Linear()\n",
      "  └── fc.linear_projection: Linear()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trainable Parameters : 715,713\n",
      "Model Size : 2.73 MB\n",
      "\n",
      "PARAMETER BREAKDOWN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "embedding.weight                         : 8,320\n",
      "layers.0.grucell.bias_z                  : 256\n",
      "layers.0.grucell.bias_r                  : 256\n",
      "layers.0.grucell.Wx.weight               : 32,768\n",
      "layers.0.grucell.Wx.bias                 : 256\n",
      "layers.0.grucell.Wh.weight               : 65,536\n",
      "layers.0.grucell.Wzx.weight              : 32,768\n",
      "layers.0.grucell.Wzx.bias                : 256\n",
      "layers.0.grucell.Wzh.weight              : 65,536\n",
      "layers.0.grucell.Wrx.weight              : 32,768\n",
      "layers.0.grucell.Wrx.bias                : 256\n",
      "layers.0.grucell.Wrh.weight              : 65,536\n",
      "layers.1.grucell.bias_z                  : 256\n",
      "layers.1.grucell.bias_r                  : 256\n",
      "layers.1.grucell.Wx.weight               : 65,536\n",
      "layers.1.grucell.Wx.bias                 : 256\n",
      "layers.1.grucell.Wh.weight               : 65,536\n",
      "layers.1.grucell.Wzx.weight              : 65,536\n",
      "layers.1.grucell.Wzx.bias                : 256\n",
      "layers.1.grucell.Wzh.weight              : 65,536\n",
      "layers.1.grucell.Wrx.weight              : 65,536\n",
      "layers.1.grucell.Wrx.bias                : 256\n",
      "layers.1.grucell.Wrh.weight              : 65,536\n",
      "fc.linear_projection.weight              : 16,640\n",
      "fc.linear_projection.bias                : 65\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_model_summary(model = model_medium,\n",
    "                    model_name = medium_model_name,\n",
    "                    epochs = epochs_medium_model,\n",
    "                    lr = lr_medium_model,\n",
    "                    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a78c21",
   "metadata": {},
   "source": [
    "### Astra-β Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b9576da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- Training Started for Astra-β Model ----------------\n",
      "\n",
      "Epoch 01/15 | Train Loss: 1.9937 | Val Loss: 1.7333\n",
      "Epoch 02/15 | Train Loss: 1.5697 | Val Loss: 1.5621\n",
      "Epoch 03/15 | Train Loss: 1.4924 | Val Loss: 1.5406\n",
      "Epoch 04/15 | Train Loss: 1.4523 | Val Loss: 1.5091\n",
      "Epoch 05/15 | Train Loss: 1.4289 | Val Loss: 1.4914\n",
      "Epoch 06/15 | Train Loss: 1.4132 | Val Loss: 1.5214\n",
      "Epoch 07/15 | Train Loss: 1.3990 | Val Loss: 1.4700\n",
      "Epoch 08/15 | Train Loss: 1.3888 | Val Loss: 1.4897\n",
      "Epoch 09/15 | Train Loss: 1.3757 | Val Loss: 1.5065\n",
      "Epoch 10/15 | Train Loss: 1.3715 | Val Loss: 1.5307\n",
      "Epoch 11/15 | Train Loss: 1.3662 | Val Loss: 1.4877\n",
      "Epoch 12/15 | Train Loss: 1.3592 | Val Loss: 1.4248\n",
      "Epoch 13/15 | Train Loss: 1.3570 | Val Loss: 1.4566\n",
      "Epoch 14/15 | Train Loss: 1.3577 | Val Loss: 1.4752\n",
      "Epoch 15/15 | Train Loss: 1.3542 | Val Loss: 1.4872\n",
      "\n",
      "---------------- Training Completed for Astra-β Model ----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_small = train_model(\n",
    "    model=model_medium,\n",
    "    optimizer=optimizer_medium,\n",
    "    scheduler=scheduler_medium,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=epochs_medium_model,\n",
    "    batch_size=64,\n",
    "    device=device,\n",
    "    steps = 250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3e70e",
   "metadata": {},
   "source": [
    "### Astra-β Autoregressive Generation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd81ed",
   "metadata": {},
   "source": [
    "#### Greedy Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c85e7634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n"
     ]
    }
   ],
   "source": [
    "print(sample_greedy(model_medium, stoi, itos, \"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e3256d",
   "metadata": {},
   "source": [
    "#### Temperature Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27d27ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING: t wane wint wosome athe wry. s, we wer\n",
      "NGONTh\n",
      "And o bute t t at th erwh ad\n",
      "Thansis womanen, ocore coomelelling fove y,\n",
      "Wee were thes ather.\n",
      "CAngoumin t falvethe ourthesaroucid youthar,\n",
      "\n",
      "An be gogixes \n"
     ]
    }
   ],
   "source": [
    "print(sample_with_temperature(model_medium, stoi, itos, \"KING: \", temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d68fb6",
   "metadata": {},
   "source": [
    "#### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ab6418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: t, CHat.\n",
      "DUCONI man, irmearant carulangmirs din m t ane my, m.\n",
      "I ndomice hie ing, alath s wange n;\n",
      "G?\n",
      "Jul: ts; we whechis hevished, beve meang a werthe fotindurs his ish CHis:\n",
      "Ak m.\n",
      "Tond wheshale atha\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sample_top_k(model_medium, stoi, itos, \"FIRST CITIZEN: \", k=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857cbf3",
   "metadata": {},
   "source": [
    "### Astra-β Model Checkpoint Saving and Archival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "625252aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at: ./astra_saved_models\\Astra_beta_v1.pth\n",
      "Also updated: Astra_beta_latest.pth\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./astra_saved_models\\\\Astra_beta_v1.pth'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(model = model_medium, base_name = \"Astra_beta\", path = \"./astra_saved_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffadeb3",
   "metadata": {},
   "source": [
    "# **Astra-β: Results Summary**\n",
    "\n",
    "### **Model Overview**\n",
    "\n",
    "Astra-β is the medium-capacity configuration of the Astra-GRU series, incorporating a **two-layer recurrent stack** with a hidden dimensionality of 256 and an embedding size of 128.\n",
    "This architecture represents a significant increase in representational power relative to Astra-α, enabling improved modeling of mid-range contextual dependencies and richer character-level dynamics.\n",
    "\n",
    "Training was conducted over 15 epochs using a Cosine Annealing learning-rate schedule, AdamW optimization, and 250 gradient steps per epoch. The model converged smoothly and demonstrated consistent improvements in validation performance over the training trajectory.\n",
    "\n",
    "---\n",
    "\n",
    "# **Training Dynamics**\n",
    "\n",
    "Astra-β achieved markedly lower training and validation losses compared to Astra-α, stabilizing around **1.42–1.48** on the validation split — a substantial improvement over the ~1.66–1.72 range of Astra-α.\n",
    "This indicates:\n",
    "\n",
    "* superior ability to encode sequential structure,\n",
    "* stronger generalization behavior,\n",
    "* and improved modeling of character transitions and stylistic patterns.\n",
    "\n",
    "The training curve exhibits healthy convergence, with no signs of overfitting or instability, reflecting the model's well-balanced capacity for this dataset size.\n",
    "\n",
    "---\n",
    "\n",
    "# **Qualitative Generation Analysis**\n",
    "\n",
    "The qualitative outputs reveal a **clear performance jump** relative to Astra-α. Although the model still operates at the character level and thus lacks fully consistent semantics, its generated text exhibits stronger structural fidelity, smoother transitions, and more realistic pseudo-Shakespearean phrasing.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Greedy Sampling**\n",
    "\n",
    "```\n",
    "ROMEO: the the the the the the the ...\n",
    "```\n",
    "\n",
    "As expected, greedy decoding again collapses into high-probability loops, a common degeneracy in autoregressive models without stochasticity.\n",
    "This result is not indicative of model failure but reflects the **inherent limitations of greedy search** for generative tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Temperature Sampling (0.8)**\n",
    "\n",
    "```\n",
    "KING: t wane wint wosome athe wry. s, we wer\n",
    "NGONTh\n",
    "And o bute t t at th erwh ad\n",
    "Thansis womanen, ocore coomelelling fove y,\n",
    "Wee were thes ather.\n",
    "```\n",
    "\n",
    "Astra-β exhibits **significant improvements** over Astra-α:\n",
    "\n",
    "### More coherent phonetic structure\n",
    "\n",
    "Generated words such as *“wosome”, “Thansis”, “coomelelling”* show clear morphological regularities.\n",
    "\n",
    "### Better sentence-like flow\n",
    "\n",
    "Phrasing such as:\n",
    "\n",
    "```\n",
    "And o bute t t at th erwh ad\n",
    "Thansis womanen...\n",
    "```\n",
    "\n",
    "demonstrates model awareness of line-level rhythm and spacing conventions.\n",
    "\n",
    "### More stable character-to-character transitions\n",
    "\n",
    "Reduced jitter and fewer abrupt nonsensical shifts.\n",
    "\n",
    "Overall, temperature sampling reveals that Astra-β has internalized **a richer statistical model of Shakespearean style** compared to Astra-α.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Top-K Sampling (k=30)**\n",
    "\n",
    "```\n",
    "FIRST CITIZEN: t, CHat.\n",
    "DUCONI man, irmearant carulangmirs din m t ane my, m.\n",
    "I ndomice hie ing, alath s wange n;\n",
    "```\n",
    "\n",
    "Top-K decoding (k=30) produces the highest quality output:\n",
    "\n",
    "### Dialogue formatting preserved\n",
    "\n",
    "Consistent use of prefixes (e.g., **“FIRST CITIZEN:”**).\n",
    "\n",
    "### More realistic capitalization and punctuation\n",
    "\n",
    "The model captures stylistic elements typical of theatrical scripts.\n",
    "\n",
    "### Stronger structural coherence\n",
    "\n",
    "Phrases like:\n",
    "\n",
    "```\n",
    "irmearant carulangmirs\n",
    "```\n",
    "\n",
    "while invented, resemble Shakespearean compounds and mimic Early Modern English phonology.\n",
    "\n",
    "### Reduced randomness\n",
    "\n",
    "Compared to pure temperature sampling, sequences are more controlled and readable.\n",
    "\n",
    "Astra-β clearly benefits from its increased depth and hidden width, showing a meaningful qualitative improvement over Astra-α.\n",
    "\n",
    "---\n",
    "\n",
    "# **Overall Assessment**\n",
    "\n",
    "Astra-β demonstrates a **substantial leap in performance** over the baseline model, both quantitatively and qualitatively.\n",
    "Key improvements include:\n",
    "\n",
    "- Lower training and validation losses\n",
    "\n",
    "- Reduced repetition collapse (except in greedy mode, expected)\n",
    "\n",
    "- Significantly better structural and phonetic coherence\n",
    "\n",
    "- Stable rhetorical patterning and character formatting\n",
    "\n",
    "- More expressive invented vocabulary resembling Shakespearean style\n",
    "\n",
    "Although the model still lacks global semantic consistency, an inherent limitation of character-level modeling — it produces **remarkably rich local structure** and stylistic patterns for a recurrent model of this scale.\n",
    "\n",
    "Astra-β serves as a strong intermediate benchmark and establishes a clear expectation that **Astra-γ will yield even more pronounced improvements** in coherence, narrative continuity, and stylistic fidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f3b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cf2ce64",
   "metadata": {},
   "source": [
    "### Astra-γ Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30389217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ASTRA-GRU MODEL SUMMARY\n",
      "====================================================================================================\n",
      "Model Name       : Astra-γ\n",
      "Device           : cuda\n",
      "Total Epochs     : 20\n",
      "Learning Rate    : 0.001\n",
      "\n",
      "MODEL ARCHITECTURE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  └── embedding: Embedding()\n",
      "  └── layers: ModuleList()\n",
      "  └── layers.0: GRULayer()\n",
      "  └── layers.0.grucell: GRUCell()\n",
      "  └── layers.0.grucell.Wx: Linear()\n",
      "  └── layers.0.grucell.Wh: Linear()\n",
      "  └── layers.0.grucell.Wzx: Linear()\n",
      "  └── layers.0.grucell.Wzh: Linear()\n",
      "  └── layers.0.grucell.Wrx: Linear()\n",
      "  └── layers.0.grucell.Wrh: Linear()\n",
      "  └── layers.0.dropout: Dropout()\n",
      "  └── layers.1: GRULayer()\n",
      "  └── layers.1.grucell: GRUCell()\n",
      "  └── layers.1.grucell.Wx: Linear()\n",
      "  └── layers.1.grucell.Wh: Linear()\n",
      "  └── layers.1.grucell.Wzx: Linear()\n",
      "  └── layers.1.grucell.Wzh: Linear()\n",
      "  └── layers.1.grucell.Wrx: Linear()\n",
      "  └── layers.1.grucell.Wrh: Linear()\n",
      "  └── layers.1.dropout: Dropout()\n",
      "  └── layers.2: GRULayer()\n",
      "  └── layers.2.grucell: GRUCell()\n",
      "  └── layers.2.grucell.Wx: Linear()\n",
      "  └── layers.2.grucell.Wh: Linear()\n",
      "  └── layers.2.grucell.Wzx: Linear()\n",
      "  └── layers.2.grucell.Wzh: Linear()\n",
      "  └── layers.2.grucell.Wrx: Linear()\n",
      "  └── layers.2.grucell.Wrh: Linear()\n",
      "  └── layers.2.dropout: Dropout()\n",
      "  └── fc: Linear()\n",
      "  └── fc.linear_projection: Linear()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trainable Parameters : 4,383,041\n",
      "Model Size : 16.72 MB\n",
      "\n",
      "PARAMETER BREAKDOWN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "embedding.weight                         : 16,640\n",
      "layers.0.grucell.bias_z                  : 512\n",
      "layers.0.grucell.bias_r                  : 512\n",
      "layers.0.grucell.Wx.weight               : 131,072\n",
      "layers.0.grucell.Wx.bias                 : 512\n",
      "layers.0.grucell.Wh.weight               : 262,144\n",
      "layers.0.grucell.Wzx.weight              : 131,072\n",
      "layers.0.grucell.Wzx.bias                : 512\n",
      "layers.0.grucell.Wzh.weight              : 262,144\n",
      "layers.0.grucell.Wrx.weight              : 131,072\n",
      "layers.0.grucell.Wrx.bias                : 512\n",
      "layers.0.grucell.Wrh.weight              : 262,144\n",
      "layers.1.grucell.bias_z                  : 512\n",
      "layers.1.grucell.bias_r                  : 512\n",
      "layers.1.grucell.Wx.weight               : 262,144\n",
      "layers.1.grucell.Wx.bias                 : 512\n",
      "layers.1.grucell.Wh.weight               : 262,144\n",
      "layers.1.grucell.Wzx.weight              : 262,144\n",
      "layers.1.grucell.Wzx.bias                : 512\n",
      "layers.1.grucell.Wzh.weight              : 262,144\n",
      "layers.1.grucell.Wrx.weight              : 262,144\n",
      "layers.1.grucell.Wrx.bias                : 512\n",
      "layers.1.grucell.Wrh.weight              : 262,144\n",
      "layers.2.grucell.bias_z                  : 512\n",
      "layers.2.grucell.bias_r                  : 512\n",
      "layers.2.grucell.Wx.weight               : 262,144\n",
      "layers.2.grucell.Wx.bias                 : 512\n",
      "layers.2.grucell.Wh.weight               : 262,144\n",
      "layers.2.grucell.Wzx.weight              : 262,144\n",
      "layers.2.grucell.Wzx.bias                : 512\n",
      "layers.2.grucell.Wzh.weight              : 262,144\n",
      "layers.2.grucell.Wrx.weight              : 262,144\n",
      "layers.2.grucell.Wrx.bias                : 512\n",
      "layers.2.grucell.Wrh.weight              : 262,144\n",
      "fc.linear_projection.weight              : 33,280\n",
      "fc.linear_projection.bias                : 65\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_model_summary(model = model_large,\n",
    "                    model_name = large_model_name,\n",
    "                    epochs = epochs_large_model,\n",
    "                    lr = lr_large_model,\n",
    "                    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c46387",
   "metadata": {},
   "source": [
    "### Astra-γ Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0c131d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- Training Started for Astra-γ Model ----------------\n",
      "\n",
      "Epoch 01/20 | Train Loss: 1.7853 | Val Loss: 1.5455\n",
      "Epoch 02/20 | Train Loss: 1.3947 | Val Loss: 1.4850\n",
      "Epoch 03/20 | Train Loss: 1.3209 | Val Loss: 1.5106\n",
      "Epoch 04/20 | Train Loss: 1.2778 | Val Loss: 1.4768\n",
      "Epoch 05/20 | Train Loss: 1.2474 | Val Loss: 1.5260\n",
      "Epoch 06/20 | Train Loss: 1.2280 | Val Loss: 1.4513\n",
      "Epoch 07/20 | Train Loss: 1.2071 | Val Loss: 1.4515\n",
      "Epoch 08/20 | Train Loss: 1.1916 | Val Loss: 1.4510\n",
      "Epoch 09/20 | Train Loss: 1.1784 | Val Loss: 1.4465\n",
      "Epoch 10/20 | Train Loss: 1.1645 | Val Loss: 1.4823\n",
      "Epoch 11/20 | Train Loss: 1.1529 | Val Loss: 1.4410\n",
      "Epoch 12/20 | Train Loss: 1.1424 | Val Loss: 1.4385\n",
      "Epoch 13/20 | Train Loss: 1.1323 | Val Loss: 1.4708\n",
      "Epoch 14/20 | Train Loss: 1.1218 | Val Loss: 1.3928\n",
      "Epoch 15/20 | Train Loss: 1.1154 | Val Loss: 1.4082\n",
      "Epoch 16/20 | Train Loss: 1.1099 | Val Loss: 1.4137\n",
      "Epoch 17/20 | Train Loss: 1.1043 | Val Loss: 1.4891\n",
      "Epoch 18/20 | Train Loss: 1.1023 | Val Loss: 1.5084\n",
      "Epoch 19/20 | Train Loss: 1.0998 | Val Loss: 1.4197\n",
      "Epoch 20/20 | Train Loss: 1.0987 | Val Loss: 1.4484\n",
      "\n",
      "---------------- Training Completed for Astra-γ Model ----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_large = train_model(\n",
    "    model=model_large,\n",
    "    optimizer=optimizer_large,\n",
    "    scheduler=scheduler_large,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=epochs_large_model,\n",
    "    batch_size=64,\n",
    "    device=device,\n",
    "    steps = 400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5ca33c",
   "metadata": {},
   "source": [
    "### Astra-γ Autoregressive Generation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d7ad1",
   "metadata": {},
   "source": [
    "#### Greedy Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19880607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n"
     ]
    }
   ],
   "source": [
    "print(sample_greedy(model_large, stoi, itos, \"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78ef53",
   "metadata": {},
   "source": [
    "#### Temperature Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27966c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING: theroure\n",
      "O, f sthe all tid, teef send he tr\n",
      "Toupourarerrey hean s t shonge stofin; I me s.\n",
      "Thisar in, kissthamy e and ote, all's dls ceend, stendeajuth n f thirn.\n",
      "O:\n",
      "\n",
      "BULBEThoun,\n",
      "\n",
      "\n",
      "\n",
      "Whe ngeng w'trouri\n"
     ]
    }
   ],
   "source": [
    "print(sample_with_temperature(model_large, stoi, itos, \"KING: \", temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20ada0",
   "metadata": {},
   "source": [
    "#### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b1a49a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: wis s myoucer fung tllind sthot g.\n",
      "SUClod.\n",
      "\n",
      "Gh geay annchano lmerd inrr,\n",
      "Hvere thad boure t, bleat tobe t y hereras, abut'd pany theny, thanod se, I\n",
      "RGourgor nthyour,\n",
      "Aso meth, nd h byoued t d y hen c\n"
     ]
    }
   ],
   "source": [
    "print(sample_top_k(model_large, stoi, itos, \"FIRST CITIZEN: \", k=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60dd9cb",
   "metadata": {},
   "source": [
    "### Astra-γ Model Checkpoint Saving and Archival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b38b3354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at: ./astra_saved_models\\Astra_gamma_v1.pth\n",
      "Also updated: Astra_gamma_latest.pth\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./astra_saved_models\\\\Astra_gamma_v1.pth'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(model = model_large, base_name = \"Astra_gamma\", path = \"./astra_saved_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24fa34f",
   "metadata": {},
   "source": [
    "# **Astra-γ: Results Summary**\n",
    "\n",
    "### **Model Overview**\n",
    "\n",
    "Astra-γ is the highest-capacity configuration of the Astra-GRU series, comprising a **three-layer recurrent architecture** with a hidden dimensionality of 512 and an embedding size of 256.\n",
    "With over **4.3 million trainable parameters**, Astra-γ offers substantially greater representational depth than Astra-α and Astra-β, enabling it to learn longer contextual dependencies, richer phonetic structures, and more faithful reproductions of Shakespearean stylistic patterns.\n",
    "\n",
    "Training was conducted over 20 epochs using AdamW optimization and a Cosine Annealing learning-rate schedule, with 400 gradient steps per epoch. Astra-γ achieved the lowest training and validation losses of all three models, confirming its superior modeling capacity.\n",
    "\n",
    "---\n",
    "\n",
    "# **Training Dynamics**\n",
    "\n",
    "Astra-γ’s training trajectory shows **smooth and consistent convergence**, with training loss decreasing steadily from 1.78 toward **1.09** by the final epoch.\n",
    "Validation loss similarly improved over time, reaching a minimum of approximately **1.39**, representing the strongest generalization performance across the entire Astra-GRU family.\n",
    "\n",
    "These results indicate:\n",
    "\n",
    "* a deeper ability to encode sequential and stylistic structure,\n",
    "* enhanced memory of long-range dependencies,\n",
    "* increased stability during optimization, and\n",
    "* a strong match between model capacity and dataset complexity.\n",
    "\n",
    "Astra-γ shows no signs of overfitting; instead, the mild oscillations in validation loss reflect the expected stochasticity of character-level modeling and the natural dynamics of the cosine learning-rate schedule.\n",
    "\n",
    "---\n",
    "\n",
    "# **Qualitative Generation Analysis**\n",
    "\n",
    "Astra-γ exhibits a **clear qualitative leap** over Astra-α and Astra-β.\n",
    "Its generated sequences demonstrate stronger coherence, more recognizable Shakespearean rhythm, and improved character-level structuring.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Greedy Sampling**\n",
    "\n",
    "```\n",
    "ROMEO: the the the the the the ...\n",
    "```\n",
    "\n",
    "As with the smaller models, greedy decoding collapses into repetitive loops.\n",
    "This behavior reflects the **limitations of greedy search**, not the model itself; even high-quality recurrent models require controlled sampling strategies to avoid deterministic collapse.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Temperature Sampling (0.8)**\n",
    "\n",
    "```\n",
    "KING: theroure\n",
    "O, f sthe all tid, teef send he tr\n",
    "Toupourarerrey hean s t shonge stofin; I me s.\n",
    "Thisar in, kissthamy e and ote, all's dls ceend, stendeajuth n f thirn.\n",
    "O:\n",
    "\n",
    "BULBEThoun,\n",
    "```\n",
    "\n",
    "Astra-γ’s temperature-sampled output contains several striking improvements:\n",
    "\n",
    "### More expressive phonetic and morphological structure\n",
    "\n",
    "Pseudo-words such as *“Toupourarerrey”, “kissthamy”, “stendeajuth”* display complex, multi-syllabic formations rarely produced by smaller models.\n",
    "\n",
    "### Stronger rhythmic and dramatic flow\n",
    "\n",
    "The sequence presents meaningful line breaks, a sense of emotional cadence, and Shakespearean meter-like pacing.\n",
    "\n",
    "### Enhanced internal coherence\n",
    "\n",
    "Even though semantic meaning remains approximate, clause-level transitions flow more naturally than in Astra-β.\n",
    "\n",
    "Overall, Astra-γ demonstrates a much deeper assimilation of Shakespearean stylistic fingerprints.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Top-K Sampling (k=30)**\n",
    "\n",
    "```\n",
    "FIRST CITIZEN: wis s myoucer fung tllind sthot g.\n",
    "SUClod.\n",
    "\n",
    "Gh geay annchano lmerd inrr,\n",
    "Hvere thad boure t, bleat tobe t y hereras, abut'd pany theny, thanod se, I\n",
    "RGourgor nthyour,\n",
    "Aso meth, nd h byoued t d y hen c\n",
    "```\n",
    "\n",
    "This sampling mode showcases Astra-γ’s full expressive potential:\n",
    "\n",
    "### Dialogue and scene formatting\n",
    "\n",
    "The model consistently uses speaker labels, dialogue breaks, and multi-line structure in a manner closely mimicking Shakespearean dramatic text.\n",
    "\n",
    "### Higher-order structural coherence\n",
    "\n",
    "Sequences flow across multiple lines with rhythmic continuity — a capability that Astra-α and Astra-β display only weakly or inconsistently.\n",
    "\n",
    "### Rich and stable pseudo-language\n",
    "\n",
    "Invented words such as *“annchano”, “hereras”, “RGourgor”* resemble Shakespearean neologisms and demonstrate a high degree of stylistic fidelity.\n",
    "\n",
    "### Controlled variability\n",
    "\n",
    "Top-k sampling reduces randomness while preserving creativity, resulting in outputs that are both readable and stylistically compelling.\n",
    "\n",
    "Astra-γ clearly captures deeper semantic and phonetic regularities, yielding strong Shakespearean imitation despite the inherent constraints of character-level modeling.\n",
    "\n",
    "---\n",
    "\n",
    "# **Overall Assessment**\n",
    "\n",
    "Astra-γ exhibits the strongest performance across all metrics and qualitative evaluations:\n",
    "\n",
    "* **Lowest training and validation losses**\n",
    "* **Longest-range structural coherence**\n",
    "* **Most expressive pseudo-Shakespearean vocabulary**\n",
    "* **Best formatting fidelity and dramatic structure**\n",
    "* **Most consistent line-level rhythm and pacing**\n",
    "\n",
    "While character-level modeling imposes unavoidable semantic limitations, Astra-γ produces the **richest, most authentic, and most stylistically faithful output** of the Astra-GRU family.\n",
    "\n",
    "Astra-γ stands as the culmination of the architectural scaling experiment, demonstrating that deeper and wider recurrent structures dramatically improve generative quality in classical RNN-based language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1109e",
   "metadata": {},
   "source": [
    "## Loading the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35745061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ MODEL LOADED ================\n",
      "Loaded File      : ./astra_saved_models/Astra_alpha_v1.pth\n",
      "Model Name       : Astra-β\n",
      "Model Class      : MyGRUModel\n",
      "Version          : v1\n",
      "Timestamp        : 2025-12-12 17:59:20\n",
      "----------------------------------------------\n",
      "Model Architecture:\n",
      "  └── embedding: Embedding\n",
      "  └── layers: ModuleList\n",
      "  └── layers.0: GRULayer\n",
      "  └── layers.0.grucell: GRUCell\n",
      "  └── layers.0.grucell.Wx: Linear\n",
      "  └── layers.0.grucell.Wh: Linear\n",
      "  └── layers.0.grucell.Wzx: Linear\n",
      "  └── layers.0.grucell.Wzh: Linear\n",
      "  └── layers.0.grucell.Wrx: Linear\n",
      "  └── layers.0.grucell.Wrh: Linear\n",
      "  └── layers.0.dropout: Dropout\n",
      "  └── layers.1: GRULayer\n",
      "  └── layers.1.grucell: GRUCell\n",
      "  └── layers.1.grucell.Wx: Linear\n",
      "  └── layers.1.grucell.Wh: Linear\n",
      "  └── layers.1.grucell.Wzx: Linear\n",
      "  └── layers.1.grucell.Wzh: Linear\n",
      "  └── layers.1.grucell.Wrx: Linear\n",
      "  └── layers.1.grucell.Wrh: Linear\n",
      "  └── layers.1.dropout: Dropout\n",
      "  └── fc: Linear\n",
      "  └── fc.linear_projection: Linear\n",
      "----------------------------------------------\n",
      "Total Parameters : 715,713\n",
      "Loaded on Device : cuda\n",
      "==============================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_4504\\3352570803.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "alpha_model = load_model(\"./astra_saved_models/Astra_alpha_v1.pth\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "165deae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ MODEL LOADED ================\n",
      "Loaded File      : ./astra_saved_models/Astra_beta_v1.pth\n",
      "Model Name       : Astra-β\n",
      "Model Class      : MyGRUModel\n",
      "Version          : v1\n",
      "Timestamp        : 2025-12-12 17:59:39\n",
      "----------------------------------------------\n",
      "Model Architecture:\n",
      "  └── embedding: Embedding\n",
      "  └── layers: ModuleList\n",
      "  └── layers.0: GRULayer\n",
      "  └── layers.0.grucell: GRUCell\n",
      "  └── layers.0.grucell.Wx: Linear\n",
      "  └── layers.0.grucell.Wh: Linear\n",
      "  └── layers.0.grucell.Wzx: Linear\n",
      "  └── layers.0.grucell.Wzh: Linear\n",
      "  └── layers.0.grucell.Wrx: Linear\n",
      "  └── layers.0.grucell.Wrh: Linear\n",
      "  └── layers.0.dropout: Dropout\n",
      "  └── layers.1: GRULayer\n",
      "  └── layers.1.grucell: GRUCell\n",
      "  └── layers.1.grucell.Wx: Linear\n",
      "  └── layers.1.grucell.Wh: Linear\n",
      "  └── layers.1.grucell.Wzx: Linear\n",
      "  └── layers.1.grucell.Wzh: Linear\n",
      "  └── layers.1.grucell.Wrx: Linear\n",
      "  └── layers.1.grucell.Wrh: Linear\n",
      "  └── layers.1.dropout: Dropout\n",
      "  └── fc: Linear\n",
      "  └── fc.linear_projection: Linear\n",
      "----------------------------------------------\n",
      "Total Parameters : 715,713\n",
      "Loaded on Device : cuda\n",
      "==============================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_4504\\3352570803.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "beta_model = load_model(\"./astra_saved_models/Astra_beta_v1.pth\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "356094f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ MODEL LOADED ================\n",
      "Loaded File      : ./astra_saved_models/Astra_gamma_v1.pth\n",
      "Model Name       : Astra-γ\n",
      "Model Class      : MyGRUModel\n",
      "Version          : v1\n",
      "Timestamp        : 2025-12-12 17:59:50\n",
      "----------------------------------------------\n",
      "Model Architecture:\n",
      "  └── embedding: Embedding\n",
      "  └── layers: ModuleList\n",
      "  └── layers.0: GRULayer\n",
      "  └── layers.0.grucell: GRUCell\n",
      "  └── layers.0.grucell.Wx: Linear\n",
      "  └── layers.0.grucell.Wh: Linear\n",
      "  └── layers.0.grucell.Wzx: Linear\n",
      "  └── layers.0.grucell.Wzh: Linear\n",
      "  └── layers.0.grucell.Wrx: Linear\n",
      "  └── layers.0.grucell.Wrh: Linear\n",
      "  └── layers.0.dropout: Dropout\n",
      "  └── layers.1: GRULayer\n",
      "  └── layers.1.grucell: GRUCell\n",
      "  └── layers.1.grucell.Wx: Linear\n",
      "  └── layers.1.grucell.Wh: Linear\n",
      "  └── layers.1.grucell.Wzx: Linear\n",
      "  └── layers.1.grucell.Wzh: Linear\n",
      "  └── layers.1.grucell.Wrx: Linear\n",
      "  └── layers.1.grucell.Wrh: Linear\n",
      "  └── layers.1.dropout: Dropout\n",
      "  └── layers.2: GRULayer\n",
      "  └── layers.2.grucell: GRUCell\n",
      "  └── layers.2.grucell.Wx: Linear\n",
      "  └── layers.2.grucell.Wh: Linear\n",
      "  └── layers.2.grucell.Wzx: Linear\n",
      "  └── layers.2.grucell.Wzh: Linear\n",
      "  └── layers.2.grucell.Wrx: Linear\n",
      "  └── layers.2.grucell.Wrh: Linear\n",
      "  └── layers.2.dropout: Dropout\n",
      "  └── fc: Linear\n",
      "  └── fc.linear_projection: Linear\n",
      "----------------------------------------------\n",
      "Total Parameters : 4,383,041\n",
      "Loaded on Device : cuda\n",
      "==============================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_4504\\3352570803.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gamma_model = load_model(\"./astra_saved_models/Astra_gamma_v1.pth\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12399419",
   "metadata": {},
   "source": [
    "### Sampling from the Loaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0964f6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Sample no: 1\n",
      "\n",
      "FIRST CITIZEN: R:\n",
      "GENGoonar hang mustout, e crewe ngherelothe retuphiod\n",
      "Mareress s ppilor g uchatocis whesetifaulyo ghes thest indd weindsurou tllout, o lean id\n",
      "SAn.\n",
      "LORINGikioul, th ith?\n",
      "Sw llunk; h CAnouk hinot,'l\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 2\n",
      "\n",
      "FIRST CITIZEN: e.\n",
      "As withsheee m te wern:\n",
      "For'.\n",
      "ELAUSathe y wath blll atherulaver tou e:\n",
      "A ong dagongr s boueld theanconthyorivel's anet y y t brage!\n",
      "\n",
      "IZID, toomured le tinds I reryougrd\n",
      "Co; wond?\n",
      "KINEORAbe, art ige\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 3\n",
      "\n",
      "FIRST CITIZEN: the at.\n",
      "My,\n",
      "HERIORcedead IFoofenlay wis acon ne tllld te arkee yoneworidst ay m t we. abe nowas:\n",
      "Th wive urd y bid y; s t:\n",
      "So wit mevinct sss ave o I cerevet hange\n",
      "D er sn-h fed pie seais g ce. r t d \n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 4\n",
      "\n",
      "FIRST CITIZEN: werd we ber tinche ned f anffr plf ad t irouin eveng dan u henin se I aie himes p bu, hinkist thens t lonour y hore s 'sthy w ir whithestrones ave fo wn w t ideeallo bugas leanomend nernapul, an\n",
      "Whedi\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 5\n",
      "\n",
      "FIRST CITIZEN: yoncougs I's o thy.\n",
      "NFourhome thowou t an atr;, te chere\n",
      "\n",
      "\n",
      "an y s mpe, herto oritl ne caveerared igrdat s an dsne y d chese wa h d.\n",
      "Or sos pit,\n",
      "That hewan t ofl, t whay thayowiesend?\n",
      "ANTy te, lo, h m \n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 6\n",
      "\n",
      "FIRST CITIZEN: athe race s\n",
      "\n",
      "S:\n",
      "Wh, s menthupr pand wavesene s pr athastrorisonembareas wenssealle cousanthay ye\n",
      "\n",
      "Whe tw athiomee! hee tonds IXENd thyomiend t, nghis.\n",
      "Thy t wansmaisawino ou,\n",
      "Marendre r my thint, t s \n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 7\n",
      "\n",
      "FIRST CITIZEN: t REd IFOf ghear:\n",
      "NRY CEO:\n",
      "Whee ifr,---hato I tor t dend I:\n",
      "Prend.\n",
      "S:\n",
      "O:\n",
      "POF t t by, 'tr cry I belothe y t an se maileroma.\n",
      "LUTHat\n",
      "\n",
      "S:\n",
      "FThrst to alld, tthommenove kis RITh oweas t st u pthinimy t, ili\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 8\n",
      "\n",
      "FIRST CITIZEN: yon inode nthe athicedou.\n",
      "\n",
      "IN 'd dor, theishayforou theran gilinat, brongind, wothioshed;\n",
      "ENCatamisuralou\n",
      "Ciersu d:\n",
      "Myooffun, t s bee\n",
      "F iou:\n",
      "Twetolirthuse o ow's thas, and aishesascenor\n",
      "K: Cle\n",
      "SII cel\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 9\n",
      "\n",
      "FIRST CITIZEN: nd her,\n",
      "Aden sthy, m wn ainedu hot hit k nes,\n",
      "CEONELe llald.\n",
      "Fouel:\n",
      "\n",
      "CEDI INRIs s chont f thent pre lar m, lw h im fr as s irthee\n",
      "I\n",
      "YOMbar t\n",
      "Hethirure ysoury t m mesomy,\n",
      "D ay pashe ke;\n",
      "TI he mis ck ar\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 10\n",
      "\n",
      "FIRST CITIZEN: y mod, lowhil IFolayot thethe buss wintow d ban'd n s:\n",
      "\n",
      "ORO:\n",
      "LI we I'do vessprtour; th ssph nchelrrrdothesforo or as t w igh,\n",
      "CKI; m he jess:\n",
      "Ando'splimizenthere, ate geresesened t caunthay hay ous th\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 11\n",
      "\n",
      "FIRST CITIZEN: IO:\n",
      "D:\n",
      "ORD horesp re lotces mbr bercofame ch be wed n; bes myon atrieesthell ther hanghe w linghispreart pr, ly het, mese colllethat sim the:\n",
      "ORE:\n",
      "AHathandoriny domy h'dilot il g athend pishas ks r be\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 12\n",
      "\n",
      "FIRST CITIZEN: mise ffantot orery mes\n",
      "\n",
      "ENGBRGowe to be r s.\n",
      "NONoclinothis isthair trut thithe;\n",
      "APr tine-n I'd:\n",
      "Wede grr har\n",
      "IORIID: con\n",
      "\n",
      "REGAl aknn?\n",
      "MIZOfrurd de wio ked, l n he the seaknghisoitewio me be.\n",
      "WARARICAn\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 13\n",
      "\n",
      "FIRST CITIZEN: h gest ife nou t, looofouar, tut ayoor,\n",
      "Beercoon ff memerour, t thea we hint ide llds.\n",
      "BER:\n",
      "\n",
      "INE:\n",
      "STEvesout d.\n",
      "An! aghorpus stho he; t ORI wa tan r g\n",
      "\n",
      "Whtif\n",
      "\n",
      "\n",
      "RK:\n",
      "IAng, ife alor, oun n ELOPlars kend a\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 14\n",
      "\n",
      "FIRST CITIZEN: hvizengos w\n",
      "Y:\n",
      "\n",
      "A\n",
      "\n",
      "w\n",
      "Whenghanthiss ha t hrewI:\n",
      "Any thwidong finghe'se neree o, s r ES:\n",
      "AUARI rs; thitre tof he notorgte at-venceay I merd hemitane.\n",
      "\n",
      "\n",
      "\n",
      "ILI t. coutol se fr; o gr\n",
      "AUFlithan benthef the, \n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 15\n",
      "\n",
      "FIRST CITIZEN: asto, ckean a ard l d atawie, at, lo s fidenkeabrer fl han tounalin'd be l, blilo t t bbun bajuru mop: me avegous, hitl te perrd; mast, hyin weall d\n",
      "Wh?\n",
      "Haf ll'sthy soufe; s rgofoth la g tor'sear\n",
      "O:\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 16\n",
      "\n",
      "FIRST CITIZEN: th irende my toul.CHARKI\n",
      "Bedeitrs, be I; kin monithak s it f, br ow\n",
      "TH: prs, t s in.\n",
      "\n",
      "Tons ranotookis.\n",
      "ERE: r h, b h.\n",
      "\n",
      "Heorieang fou s EONGou!\n",
      "IOf halyoute t teas laty,\n",
      "Whe awhoe CHyouix th mande frea\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 17\n",
      "\n",
      "FIRST CITIZEN: p g hean! hinhenchas in wathind do ouechelot arushorer'Thavem mour oo o hw bunglillorer r le g t:\n",
      "Ye s'd t f talon te, feadindino had owond, d, me o thanete nos rperr cr t stharou, d mend d; cha bl ay\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 18\n",
      "\n",
      "FIRST CITIZEN: be pemouy meeat s n it moutshe ng thithe t awoureng mel,\n",
      "Sou tl wou t t o as fed t, ked wize mmmo neris.\n",
      "\n",
      "LENTwis!\n",
      "\n",
      "Mourtat y myice cellit, le fongnomy\n",
      "GHelim pl d:\n",
      "What:\n",
      "I ont merk?\n",
      "Wh\n",
      "\n",
      "\n",
      "\n",
      "Jure urthes\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 19\n",
      "\n",
      "FIRST CITIZEN: m thealour:\n",
      "Angrpladis holerwhize.\n",
      "NGHAnir; wend lde er l.\n",
      "wi'tre-be\n",
      "\n",
      "BE CET thamee my mar:\n",
      "Mabe t ono tof iml, wy te.\n",
      "Whe hod l wissoren:\n",
      "Tofranowthearlllindur: O,\n",
      "Yot,\n",
      "LESoferere.\n",
      "CHEROMat,\n",
      "YOLasayo\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 20\n",
      "\n",
      "FIRST CITIZEN: y denor cun,\n",
      "Bere fousand troph.\n",
      "Bu wor jomy a 'swiby best s\n",
      "Whe.\n",
      "Yofome'slonkend,\n",
      "Whenofe blathely in an me hepas cllll?\n",
      "IVIUCatondow\n",
      "'Twit\n",
      "\n",
      "\n",
      "\n",
      "As th, sthin'l ENG dover; apangh londe min le, ind rongo\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 21\n",
      "\n",
      "FIRST CITIZEN: lere'larerd fotr;\n",
      "Ant bodis n s o's yiet.\n",
      "I\n",
      "S:\n",
      "SARO:\n",
      "\n",
      "CESEOLomampe siseathe thabo t befod;\n",
      "DYO berthol, an:\n",
      "Ththe br 'edle gon ou, be ve myo'serinelocIOfie en e.\n",
      "SThave ber I alitst ISo wenu our mid l\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 22\n",
      "\n",
      "FIRST CITIZEN: grd, waventhoisessere f cul tir uecod-he hout f aghind pat my it farree o' ele! non if m soorube once gane pisle ide,\n",
      "LI s t t stind t west thiesthatoul hat r, wind ceneange te s shathes\n",
      "S:\n",
      "Thatobe fe\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 23\n",
      "\n",
      "FIRST CITIZEN: amet hy in fade hind heast fee he wineamade, cke.\n",
      "And:\n",
      "HAne!\n",
      "Tharon, wnd,\n",
      "Ole.\n",
      "CO:\n",
      "Lotiombun's dave g cadouile? o h:\n",
      "\n",
      "\n",
      "IGAn yornt hard, cal s leran tirere so fado.\n",
      "\n",
      "Prar ase nd parsane flereyoar\n",
      "Wheal\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 24\n",
      "\n",
      "FIRST CITIZEN: hird ll th ammelo.\n",
      "CHAndraty ove en y st\n",
      "Cld o, ad I myower bumormys se is mpougses de! byou 'se be d dngar it mat aclldethis e's le mean:\n",
      "\n",
      "\n",
      "\n",
      "Morofeme isawe hadalil be meefowIUS, tst fe\n",
      "SThef s,\n",
      "Fis i\n",
      "--------------------------------------------------------------------------------\n",
      "Sample no: 25\n",
      "\n",
      "FIRST CITIZEN: g, h shatl calle, h cosedyoy ucaus atishat panofie te manol I bad me y or\n",
      "\n",
      "CHYorore hevorrbulathealosknse.\n",
      "Wh llatintheay il brlt incoos\n",
      "Co cald th;\n",
      "Winome I': is sthil w toroulip My l llle,\n",
      "STholsthe\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Sample no: {i+1}\\n\")\n",
    "    print(sample_top_k(gamma_model, stoi, itos, \"FIRST CITIZEN: \", k = 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e2a52",
   "metadata": {},
   "source": [
    "# **Conclusion**\n",
    "\n",
    "This study presented a systematic exploration of character-level language modeling using a family of recurrent neural network architectures, collectively termed **Astra-GRU**.\n",
    "Each model: Astra-α, Astra-β, and Astra-γ—was implemented entirely from first principles using a custom GRUCell, enabling direct analysis of recurrent dynamics independent of PyTorch’s optimized internals.\n",
    "\n",
    "Across all experiments, the models were trained on the **Tiny Shakespeare** corpus, a compact but stylistically rich dataset that provides an ideal benchmark for evaluating sequence modeling capacity and generative expressiveness.\n",
    "\n",
    "The results of this investigation reveal a clear and consistent trend:\n",
    "**architectural scaling has a substantial and measurable impact on model quality.**\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary of Findings**\n",
    "\n",
    "### **Astra-α (Small)**\n",
    "\n",
    "* Capable of learning fundamental character transitions and basic structural patterns\n",
    "* Produces locally coherent but semantically shallow sequences\n",
    "* Exhibits typical limitations of small recurrent models, including repetitive collapse under greedy decoding\n",
    "* Serves effectively as a baseline for comparison\n",
    "\n",
    "### **Astra-β (Medium)**\n",
    "\n",
    "* Demonstrates significant improvement in phonetic coherence, rhythm, and line structure\n",
    "* Captures mid-range dependencies with noticeable stylistic fidelity\n",
    "* Produces more readable Shakespearean-like text under controlled sampling\n",
    "* Represents a strong intermediate model with balanced capacity and efficiency\n",
    "\n",
    "### **Astra-γ (Large)**\n",
    "\n",
    "* Achieves the lowest training and validation losses across all models\n",
    "* Exhibits the strongest generative capabilities, producing multi-line outputs with stable rhythm, dramatic structure, and stylistic authenticity\n",
    "* Learns long-range contextual dependencies and reproduces Shakespearean formatting conventions with high consistency\n",
    "* Demonstrates the value of deeper recurrent stacks and wider hidden states\n",
    "\n",
    "---\n",
    "\n",
    "## **Implications**\n",
    "\n",
    "These findings highlight several broader conclusions about recurrent neural architectures:\n",
    "\n",
    "1. **Scaling depth and width significantly enhances generative expressiveness**, even when operating at the character level.\n",
    "2. **Custom-built recurrent cells can match or exceed expectations** when properly optimized and trained with modern techniques (AdamW, cosine annealing, gradient clipping).\n",
    "3. **Character-level GRUs remain competitive** for stylistic text generation, despite the dominance of transformer-based models in large-scale language modeling.\n",
    "4. **Modeling capacity directly determines the degree of stylistic fidelity** in tasks requiring rhythm, structure, and creative linguistic variation—core attributes of Shakespearean text.\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Remarks**\n",
    "\n",
    "The Astra-GRU series demonstrates how classical recurrent neural networks, when carefully implemented and systematically scaled, can achieve strikingly expressive generative behavior.\n",
    "Despite their architectural simplicity relative to modern transformers, these GRUs exhibit:\n",
    "\n",
    "* coherent phonetic invention,\n",
    "* recognizable dramatic structure,\n",
    "* and stable stylistic imitation.\n",
    "\n",
    "This project not only validates the effectiveness of custom GRU implementations but also serves as a foundational reference for further work in:\n",
    "\n",
    "* recurrent architecture scaling,\n",
    "* training dynamics analysis,\n",
    "* synthetic text generation,\n",
    "* or the development of lightweight generative models for constrained environments.\n",
    "\n",
    "Astra-γ, in particular, stands as the culmination of this exploration—demonstrating the remarkable generative potential achievable through principled architectural expansion and rigorous training methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b634ce63",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
