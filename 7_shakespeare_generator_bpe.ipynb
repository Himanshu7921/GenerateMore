{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f102aa5d",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook presents an extended investigation into recurrent neural network architectures by advancing the original character-level Astra-GRU experiments into a **subword-level language modeling framework**.\n",
    "In this new study, we introduce the **Scribe-GRU Series**—a family of three recurrent models (Scribe-α, Scribe-β, and Scribe-γ) trained on the *Tiny Shakespeare* corpus using **SentencePiece-based subword tokenization** instead of raw characters.\n",
    "\n",
    "Where the Astra-GRU series examined the expressive limits of GRUs at the character level, the Scribe-GRU series explores how transitioning to **morpheme-like subword units** affects modeling capacity, compositional structure, and generative quality. Subword-level modeling provides a powerful intermediate granularity: tokens are more meaningful than characters but significantly more flexible than full words, allowing the model to learn syntax, phonetic structure, and stylistic patterns with far greater efficiency.\n",
    "\n",
    "Despite this shift in linguistic representation, **all core computational components remain unchanged**.\n",
    "Each model continues to use the same manually implemented GRUCell and GRULayer classes defined previously, preserving:\n",
    "\n",
    "* the **single-bias gate formulation**,\n",
    "* the **canonical reset-gate application to the raw hidden state**, and\n",
    "* the fully hand-written recurrent logic that diverges intentionally from PyTorch’s optimized GRU internals.\n",
    "\n",
    "This ensures that any observed improvements in depth, coherence, or stylistic fidelity arise solely from the **change in tokenization strategy**, not from modifications to the recurrent architecture itself.\n",
    "\n",
    "The Tiny Shakespeare dataset remains the underlying training corpus, but the text is now processed using a **SentencePiece BPE subword tokenizer**, enabling the model to operate over a compact vocabulary of semantically meaningful units. This allows GRUs to capture longer-range dependencies, more stable word-like structures, and richer stylistic patterns that are difficult to model at the character level.\n",
    "\n",
    "To systematically study scaling behavior under this new tokenization regime, three GRU architectures of increasing complexity are trained:\n",
    "\n",
    "* **Scribe-α** — a lightweight single-layer subword GRU\n",
    "* **Scribe-β** — a medium-capacity two-layer configuration\n",
    "* **Scribe-γ** — a high-capacity three-layer recurrent model with expanded hidden representation\n",
    "\n",
    "Each model is evaluated on its ability to perform autoregressive subword generation, reconstruct Shakespearean style, and maintain coherent linguistic structure across extended sequences. Comparative analyses are provided in terms of architecture, parameter count, training dynamics, and qualitative generation quality.\n",
    "\n",
    "This notebook documents the complete workflow—from subword tokenizer construction and dataset encoding to model training, evaluation, and sampling—thereby building upon and extending the original Astra-GRU experimental framework into a richer and more linguistically grounded modeling paradigm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b3c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_file = \"tiny_shakespeare.txt\"\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=input_file,\n",
    "    model_prefix=\"shakespeare_bpe\",\n",
    "    vocab_size=1000,\n",
    "    character_coverage=1.0,     \n",
    "    model_type=\"bpe\",           \n",
    "    bos_id=1,                   \n",
    "    eos_id=2,\n",
    "    unk_id=0                    \n",
    ")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"shakespeare_bpe.model\")\n",
    "\n",
    "data = sp.encode(open(\"tiny_shakespeare.txt\", \"r\", encoding=\"utf-8\").read(), out_type=int)\n",
    "n = int(0.9 * len(data))\n",
    "data = torch.tensor(data, dtype=torch.long)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "block_size = 128\n",
    "def get_batch(split=\"train\", batch_size=64):\n",
    "    source = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(source) - block_size - 1, (batch_size,))\n",
    "    X = torch.stack([source[i:i+block_size] for i in ix])\n",
    "    Y = torch.stack([source[i+1:i+block_size+1] for i in ix])\n",
    "    return X.to(device), Y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d05337",
   "metadata": {},
   "source": [
    "## GRUCell's Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cae64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    \"\"\" \n",
    "    This Implementation is Differ from the PyTorch's Official Implementation in 2 Different Ways\n",
    "    NOTE-1:\n",
    "        This is not the official Implementation of PyTorch's GRUcell Because they use 2 biases per Gate\n",
    "        and i'm only using 1 bias per Gate\n",
    "\n",
    "        PyTorch's Official Implementation: \n",
    "        r = σ(W_ir x + b_ir + W_hr h + b_hr)\n",
    "        z = σ(W_iz x + b_iz + W_hz h + b_hz)\n",
    "        n = tanh(W_in x + b_in + r ⊙ (W_hn h + b_hn))\n",
    "\n",
    "        They Use 2 Bias per Gate\n",
    "    \n",
    "    NOTE-2: \n",
    "        They apply the reset gate (r) after the Multiplication of W_hn and addition of b_hn on the h_prev\n",
    "        2. Original implementation: Apply the Hadamard product (⊙) between r_t and h_prev and then apply the\n",
    "            Matrix Transformation and bias addition\n",
    "        3. What PyTorch does is, they apply the Matrix Transformation (Matrix Multiplication and bias addition) 1st and then\n",
    "            they apply the Hadamard product (⊙) between (W_hn h + b_hn)\n",
    "    \"\"\"\n",
    "    def __init__(self, embd_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        -> Bias only on x: input\n",
    "        -> No Bias on Hidden States\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Candidate transformation\n",
    "        self.Wx = nn.Linear(embd_dim, hidden_dim, bias = True)\n",
    "        self.Wh = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "\n",
    "        # Update Gate Specific Parameters\n",
    "        self.Wzx = nn.Linear(embd_dim, hidden_dim, bias = True)\n",
    "        self.Wzh = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.bias_z = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        # Reset Gate Specific Parameters\n",
    "        self.Wrx = nn.Linear(embd_dim, hidden_dim, bias = True)\n",
    "        self.Wrh = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.bias_r = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        --> A proposed update → candidate (h̃_t)\n",
    "        --> A decision gate → update gate (z_t)\n",
    "        --> A final controlled update → h_t\n",
    "\n",
    "        NOTE:   1. Reset Gate Filters h_prev\n",
    "                    - r_t = sigmoid( ( (x_t @ W_rx) + (h_prev @ W_rh) + b_r) )\n",
    "                2. Apply filter to h_prev\n",
    "                    - filtered_h_prev = r_t * h_prev [NOTE: (where * is element-wise multiplication)]\n",
    "                        - Meaning:\n",
    "                            - If r_t ≈ 0 → ignore old memory when forming candidate\n",
    "                            - If r_t ≈ 1 → use old memory fully\n",
    "                3. Compute candidate\n",
    "                    - h̃_t = tanh( ( (x_t @ W_hx) + (filtered_h_prev @ W_hh) + b_h) )\n",
    "                        - Meaning: \n",
    "                            - This produces a new memory proposal: A proposed update\n",
    "                4. Final hidden state\n",
    "                    - h_t = (1 - z_t) * h_prev + z_t * h̃_t [NOTE: (where * is element-wise multiplication)]\n",
    "        \"\"\"\n",
    "        r_t = torch.sigmoid((self.Wrx(x)) + (self.Wrh(h_prev)) + self.bias_r)\n",
    "        z_t = torch.sigmoid((self.Wzx(x)) + (self.Wzh(h_prev)) + self.bias_z)\n",
    "        h_tilde = torch.tanh(self.Wx(x) + self.Wh(r_t * h_prev))\n",
    "\n",
    "        h = (1 - z_t) * h_prev + z_t * h_tilde\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00153af7",
   "metadata": {},
   "source": [
    "## GRULayer's Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7404df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULayer(nn.Module):\n",
    "    def __init__(self, embd_dim, hidden_dim, dropout = 0.0):\n",
    "        super().__init__()\n",
    "        self.grucell = GRUCell(embd_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, h_prev = None):\n",
    "        batch, seq_length, _ = x.shape # x.shape --> batch, seq_length, embd_dim\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(batch, self.grucell.hidden_dim, device = x.device)\n",
    "\n",
    "        hidden_states = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h_prev = self.grucell(x_t, h_prev)\n",
    "            h_prev = self.dropout(h_prev)\n",
    "            hidden_states.append(h_prev)\n",
    "        \n",
    "        # Stack list into tensor\n",
    "        hidden_states = torch.stack(hidden_states, dim=1)\n",
    "        return hidden_states, h_prev\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db3a14",
   "metadata": {},
   "source": [
    "## Linear Layer Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a23fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_classes):\n",
    "        \"\"\"\n",
    "        This layer performs a linear projection on the GRU hidden states.\n",
    "        It maps the hidden vector (of size hidden_dim) into the vocabulary space (n_classes)\n",
    "        by applying a learnable affine transformation:\n",
    "\n",
    "            logits = W h + b\n",
    "\n",
    "        This is used to convert each GRU hidden state into class probabilities\n",
    "        (e.g., next-character prediction in a name generation model).\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.linear_projection = nn.Linear(in_features = hidden_dim, out_features = n_classes, bias = True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_projection(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05612cbf",
   "metadata": {},
   "source": [
    "## Custom GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "986c395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_dim, hidden_dim, num_layers, model_name, dropout=0.0, ):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_classes = vocab_size\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embd_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(GRULayer(embd_dim, hidden_dim, dropout))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(GRULayer(hidden_dim, hidden_dim, dropout))\n",
    "\n",
    "        self.fc = Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, h_prev = None):\n",
    "        x = self.embedding(x)   # (batch, seq, embd_dim)\n",
    "\n",
    "        if h_prev is None:\n",
    "            h_prev = [None] * self.num_layers\n",
    "\n",
    "        new_h = []\n",
    "        h = x\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h, last_h = layer(h)  # (batch, seq, hidden_dim)\n",
    "            new_h.append(last_h)\n",
    "\n",
    "        logits = self.fc(h)     # (batch, seq, vocab_size)\n",
    "        return logits, new_h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b5abb",
   "metadata": {},
   "source": [
    "## Function for Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc4cf01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def train_model(\n",
    "        model: MyGRUModel,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss_fn,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        device,\n",
    "        clip_value=1.0,\n",
    "        val_interval=1,\n",
    "        steps = 200\n",
    "    ):\n",
    "    \n",
    "    print(f\"\\n---------------- Training Started for {model.model_name} Model ----------------\\n\")\n",
    "    \n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for _ in range(steps):\n",
    "            X, Y = get_batch(split=\"train\", batch_size=batch_size)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, _ = model(X)\n",
    "            loss = loss_fn(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                Y.reshape(-1)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= steps\n",
    "\n",
    "        # Validation\n",
    "        val_loss = None\n",
    "        if epoch % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Xv, Yv = get_batch(split=\"val\", batch_size=batch_size)\n",
    "                Xv, Yv = Xv.to(device), Yv.to(device)\n",
    "\n",
    "                logits, _ = model(Xv)\n",
    "                val_loss = loss_fn(\n",
    "                    logits.reshape(-1, logits.size(-1)),\n",
    "                    Yv.reshape(-1)\n",
    "                ).item()\n",
    "\n",
    "        # Lr Scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Epoch and Loss Details\n",
    "        if val_loss is not None:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\n---------------- Training Completed for {model.model_name} Model ----------------\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f7fa40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import os\n",
    "\n",
    "\n",
    "def train_model_with_early_stopping(\n",
    "        model: MyGRUModel,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss_fn,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        device,\n",
    "        clip_value=1.0,\n",
    "        val_interval=1,\n",
    "        steps=200,\n",
    "        patience=5,\n",
    "        checkpoint_path=\"./best_checkpoints/\"\n",
    "    ):\n",
    "    \n",
    "    print(f\"\\n---------------- Training Started for {model.model_name} Model ----------------\\n\")\n",
    "    # steps ---> How many batches will get involve in forwardpass and backward pass\n",
    "    # steps = 200, and batch_size = 64 meaning 200 batches of each size = 64 will get involved in forwardpass and backward pass\n",
    "    # 200 * 64 * seq_length = 200 * 64 * 128 = 1.64M tokens/epoch for forward pass and 1.46M token/epoch for backward pass\n",
    "    # so for larger models, keep larger steps, for Astra-gamma step = 400 \n",
    "\n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve = 0   # counter for early stopping\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # ---------------- TRAINING LOOP ----------------\n",
    "        for _ in range(steps):\n",
    "            X, Y = get_batch(split=\"train\", batch_size=batch_size)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, _ = model(X)\n",
    "            loss = loss_fn(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                Y.reshape(-1)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= steps\n",
    "\n",
    "        # ---------------- VALIDATION ----------------\n",
    "        val_loss = None\n",
    "        if epoch % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Xv, Yv = get_batch(split=\"val\", batch_size=batch_size)\n",
    "                Xv, Yv = Xv.to(device), Yv.to(device)\n",
    "\n",
    "                logits, _ = model(Xv)\n",
    "                val_loss = loss_fn(\n",
    "                    logits.reshape(-1, logits.size(-1)),\n",
    "                    Yv.reshape(-1)\n",
    "                ).item()\n",
    "\n",
    "        # ---------------- SCHEDULER STEP ----------------\n",
    "        scheduler.step()\n",
    "\n",
    "        # ---------------- LOGGING ----------------\n",
    "        if val_loss is not None:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # ---------------- Early Stopping and Best Checkpoint ----------------\n",
    "        if val_loss is not None:\n",
    "            if val_loss < best_val:\n",
    "                best_val = val_loss\n",
    "                best_epoch = epoch\n",
    "                no_improve = 0\n",
    "\n",
    "                # Save checkpoint\n",
    "                save_path = os.path.join(checkpoint_path, f\"{model.model_name}_best.pth\")\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "\n",
    "                print(f\">>> Improved validation! Best checkpoint saved at epoch {epoch}.\")\n",
    "            \n",
    "            else:\n",
    "                no_improve += 1\n",
    "                print(f\">>> No improvement ({no_improve}/{patience}).\")\n",
    "\n",
    "            # Trigger early stopping\n",
    "            if no_improve >= patience:\n",
    "                print(\"\\n================ EARLY STOPPING ACTIVATED ================\")\n",
    "                print(f\"Training stopped at epoch {epoch}. Best epoch: {best_epoch} | Best Val Loss: {best_val:.4f}\")\n",
    "                print(\"==========================================================\\n\")\n",
    "                \n",
    "                # Load best model before returning\n",
    "                best_path = os.path.join(checkpoint_path, f\"{model.model_name}_best.pth\")\n",
    "                model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "                print(f\"Loaded best checkpoint: {best_path}\")\n",
    "                print(f\"\\n---------------- Training Completed for {model.model_name} Model ----------------\\n\")\n",
    "                return model\n",
    "\n",
    "    print(f\"\\n---------------- Training Completed for {model.model_name} Model ----------------\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8391541",
   "metadata": {},
   "source": [
    "## Sampling Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f362e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_greedy(model, sp, start_text=\"A\", max_new_tokens=200):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Encode start text\n",
    "    input_ids = torch.tensor(sp.encode(start_text), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    h_prev = None\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, h_prev = model(input_ids[:, -1:], h_prev)  \n",
    "        # logits: (1, 1, vocab_size)\n",
    "\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1)  # greedy pick\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_id.unsqueeze(1)], dim=1)\n",
    "\n",
    "    return sp.decode(input_ids[0].tolist())\n",
    "\n",
    "\n",
    "def sample_with_temperature(model, sp, start_text=\"A\", max_new_tokens=200, temperature=1.0):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_ids = torch.tensor(sp.encode(start_text), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    h_prev = None\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, h_prev = model(input_ids[:, -1:], h_prev) \n",
    "        logits = logits[:, -1, :] / temperature  \n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_id], dim=1)\n",
    "\n",
    "    return sp.decode(input_ids[0].tolist())\n",
    "\n",
    "\n",
    "def sample_top_k(model, sp, start_text=\"A\", max_new_tokens=200, k=20, temperature=1.0):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_ids = torch.tensor(sp.encode(start_text), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    h_prev = None\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, h_prev = model(input_ids[:, -1:], h_prev)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        # Keep only top-k logits\n",
    "        topk_vals, topk_idx = torch.topk(logits, k)\n",
    "        \n",
    "        probs = F.softmax(topk_vals, dim=-1)\n",
    "\n",
    "        # Sample from top-k\n",
    "        sampled_idx = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        next_id = topk_idx.gather(-1, sampled_idx)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_id], dim=1)\n",
    "\n",
    "    return sp.decode(input_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7351a8e",
   "metadata": {},
   "source": [
    "## Function for Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "739f1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model(model: MyGRUModel, base_name=\"Astra\", path=\"./saved_models/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # versioning\n",
    "    existing = [f for f in os.listdir(path) if f.startswith(base_name) and f.endswith(\".pth\")]\n",
    "    versions = []\n",
    "    for f in existing:\n",
    "        parts = f.replace(\".pth\", \"\").split(\"_v\")\n",
    "        if len(parts) == 2 and parts[1].isdigit():\n",
    "            versions.append(int(parts[1]))\n",
    "    next_version = max(versions, default=0) + 1\n",
    "\n",
    "    filename = f\"{base_name}_v{next_version}.pth\"\n",
    "    save_path = os.path.join(path, filename)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"model_class\": model.__class__.__name__,\n",
    "        \"model_name\": model.model_name,\n",
    "        \"n_classes\": model.n_classes,\n",
    "        \"embd_dim\": model.embd_dim,\n",
    "        \"hidden_dim\": model.hidden_dim,\n",
    "        \"dropout\": model.dropout,\n",
    "        \"vocab_size\": model.vocab_size,\n",
    "        \"num_layers\": model.num_layers,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"version\": next_version,\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, save_path)\n",
    "    torch.save(checkpoint, os.path.join(path, f\"{base_name}_latest.pth\"))\n",
    "\n",
    "    print(f\"\\nModel saved at: {save_path}\")\n",
    "    print(f\"Also updated: {base_name}_latest.pth\\n\")\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c97505",
   "metadata": {},
   "source": [
    "## Function for Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2144e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def load_model(filepath, device):\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "\n",
    "    # extract architecture parameters from checkpoint\n",
    "    model_name  = checkpoint[\"model_name\"]\n",
    "    vocab_size  = checkpoint[\"vocab_size\"]\n",
    "    embd_dim    = checkpoint[\"embd_dim\"]\n",
    "    hidden_dim  = checkpoint[\"hidden_dim\"]\n",
    "    dropout     = checkpoint[\"dropout\"]\n",
    "    num_layers     = checkpoint[\"num_layers\"]\n",
    "\n",
    "    # instantiate model using all saved metadata\n",
    "    model = MyGRUModel(\n",
    "        vocab_size = vocab_size,\n",
    "        embd_dim   = embd_dim,\n",
    "        hidden_dim = hidden_dim,\n",
    "        dropout    = dropout,\n",
    "        model_name = model_name,\n",
    "        num_layers = num_layers\n",
    "    ).to(device)\n",
    "\n",
    "    # load weights\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "    # pretty print metadata\n",
    "    print(\"\\n================ MODEL LOADED ================\")\n",
    "    print(f\"Loaded File      : {filepath}\")\n",
    "    print(f\"Model Name       : {model_name}\")\n",
    "    print(f\"Model Class      : {checkpoint['model_class']}\")\n",
    "    print(f\"Version          : v{checkpoint['version']}\")\n",
    "    print(f\"Timestamp        : {checkpoint['timestamp']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(\"Model Architecture:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if name != \"\":\n",
    "            print(f\"  └── {name}: {module.__class__.__name__}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(f\"Total Parameters : {count_parameters(model):,}\")\n",
    "    print(f\"Loaded on Device : {device}\")\n",
    "    print(\"==============================================\\n\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6023231",
   "metadata": {},
   "source": [
    "## Function for Printing the Summary of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eea244f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_summary(model, model_name, epochs, lr, device):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ASTRA-GRU MODEL SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Model Name       : {model_name}\")\n",
    "    print(f\"Device           : {device}\")\n",
    "    print(f\"Total Epochs     : {epochs}\")\n",
    "    print(f\"Learning Rate    : {lr}\")\n",
    "\n",
    "    print(\"\\nMODEL ARCHITECTURE\")\n",
    "    print(\"-\"*100)\n",
    "    for name, module in model.named_modules():\n",
    "        if name == \"\":\n",
    "            continue\n",
    "        print(f\"  └── {name}: {module.__class__.__name__}()\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model_size_mb = n_params * 4 / (1024**2)\n",
    "\n",
    "    print(f\"\\nTrainable Parameters : {n_params:,}\")\n",
    "    print(f\"Model Size : {model_size_mb:.2f} MB\")\n",
    "\n",
    "    print(\"\\nPARAMETER BREAKDOWN\")\n",
    "    print(\"-\"*100)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name:40s} : {param.numel():,}\")\n",
    "\n",
    "    print(\"=\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828ca21",
   "metadata": {},
   "source": [
    "# **Scribe-GRU Model Family: Architectural Variants and Training Configurations**\n",
    "\n",
    "The **Scribe-GRU** series introduces a new generation of recurrent language models—**Scribe-α**, **Scribe-β**, and **Scribe-γ**—designed for **subword-level language modeling** using a SentencePiece-BPE vocabulary trained on the Tiny Shakespeare corpus.\n",
    "This family serves as a direct evolution of the Astra series, leveraging the richer semantic structure of subword tokens and expanding architectural capacity for more expressive sequence modeling.\n",
    "\n",
    "While the underlying GRUCell and GRULayer implementations remain identical to the handcrafted mechanisms used in the Astra series, the Scribe models are scaled to exploit the advantages of subword tokenization, enabling superior generalization, smoother long-range coherence, and significantly improved generative fluency.\n",
    "\n",
    "The following sections detail the structure and training configuration of all three Scribe variants.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Scribe-α Model (Small Configuration)**\n",
    "\n",
    "### **Architectural Description**\n",
    "\n",
    "Scribe-α is the entry-level configuration in the Scribe-GRU series, designed for fast experimentation on subword-level inputs.\n",
    "Unlike Astra-α—which operated on individual characters—Scribe-α benefits from semantically meaningful BPE tokens, allowing even a compact architecture to produce coherent multi-token sequences.\n",
    "\n",
    "The architecture consists of:\n",
    "\n",
    "* A subword embedding layer\n",
    "* A **2-layer** GRU stack\n",
    "* A linear output projection for next-token prediction\n",
    "\n",
    "This setup strikes a balance between simplicity and sufficient capacity to leverage subword structure.\n",
    "\n",
    "### **Hyperparameter Configuration**\n",
    "\n",
    "```\n",
    "embedding_dim = 128\n",
    "hidden_dim    = 256\n",
    "num_layers    = 2\n",
    "epochs        = 15\n",
    "learning_rate = 2e-3\n",
    "weight_decay  = 0.01\n",
    "optimizer     = AdamW\n",
    "scheduler     = CosineAnnealingLR\n",
    "dropout       = 0.1\n",
    "batch_size    = 64\n",
    "seq_length    = 128\n",
    "```\n",
    "\n",
    "### **Purpose and Expected Behavior**\n",
    "\n",
    "Scribe-α serves as the baseline for subword modeling experiments.\n",
    "It is capable of learning token-level morphology, phrase structure, and simple line-based formatting.\n",
    "Its performance already surpasses Astra-α due to the inherent modeling advantages of BPE segmentation—not architectural complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Scribe-β Model (Medium Configuration)**\n",
    "\n",
    "### **Architectural Description**\n",
    "\n",
    "Scribe-β significantly expands representational capacity through:\n",
    "\n",
    "* **Deeper recurrence (3 GRU layers)**\n",
    "* Wider hidden state (512)\n",
    "* Higher-dimensional embeddings (256)\n",
    "\n",
    "This configuration is designed for intermediate-scale modeling tasks, providing robust learning of mid-range dependencies (e.g., multi-token expressions, sentence-like structures, and repeated theatrical phrasing).\n",
    "\n",
    "The architecture consists of:\n",
    "\n",
    "* Embedding layer\n",
    "* GRU Layer 1 → GRU Layer 2 → GRU Layer 3\n",
    "* Linear projection to vocabulary size\n",
    "\n",
    "### **Hyperparameter Configuration**\n",
    "\n",
    "```\n",
    "embedding_dim = 256\n",
    "hidden_dim    = 512\n",
    "num_layers    = 3\n",
    "epochs        = 20\n",
    "learning_rate = 1.5e-3\n",
    "weight_decay  = 0.01\n",
    "optimizer     = AdamW\n",
    "scheduler     = CosineAnnealingLR\n",
    "dropout       = 0.1\n",
    "batch_size    = 64\n",
    "seq_length    = 128\n",
    "```\n",
    "\n",
    "### **Purpose and Expected Behavior**\n",
    "\n",
    "Scribe-β is designed as the **balanced workhorse** of the series.\n",
    "It is expected to produce coherent multi-line outputs, exhibit stable training curves, and capture much richer structural and stylistic patterns compared to Scribe-α.\n",
    "\n",
    "This model is well-suited for realistic Shakespeare-like generation without excessive computational requirements.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Scribe-γ Model (Large Configuration)**\n",
    "\n",
    "### **Architectural Description**\n",
    "\n",
    "Scribe-γ is the most advanced and expressive model in the Scribe-GRU family.\n",
    "\n",
    "Relative to Astra-γ and even Scribe-β, it introduces:\n",
    "\n",
    "* A **4-layer GRU stack**\n",
    "* High-resolution embeddings (384)\n",
    "* Substantial hidden dimensionality (768)\n",
    "* Increased dropout for stabilization\n",
    "\n",
    "This model maximizes recurrent depth, token-level abstraction, and long-range coherence—essential for producing realistic Shakespearean dialogue that spans multiple sentences or lines.\n",
    "\n",
    "The architecture includes:\n",
    "\n",
    "* Embedding layer\n",
    "* GRU Layer 1 → GRU Layer 2 → GRU Layer 3 → GRU Layer 4\n",
    "* Linear prediction head\n",
    "\n",
    "### **Hyperparameter Configuration**\n",
    "\n",
    "```\n",
    "embedding_dim = 384\n",
    "hidden_dim    = 768\n",
    "num_layers    = 4\n",
    "epochs        = 30\n",
    "learning_rate = 1e-3\n",
    "weight_decay  = 0.01\n",
    "optimizer     = AdamW\n",
    "scheduler     = CosineAnnealingLR\n",
    "dropout       = 0.15\n",
    "batch_size    = 64\n",
    "seq_length    = 128\n",
    "```\n",
    "\n",
    "### **Purpose and Expected Behavior**\n",
    "\n",
    "Scribe-γ is optimized for **high-fidelity subword generation**, offering:\n",
    "\n",
    "* Strong multi-sentence continuity\n",
    "* Accurate dialogue formatting\n",
    "* Rich stylistic imitation of Shakespeare’s syntax and rhythm\n",
    "* Far fewer nonsensical outputs compared to character-level models\n",
    "\n",
    "It stands as the flagship configuration for experiments in recurrent generative modeling.\n",
    "\n",
    "---\n",
    "\n",
    "# **Summary**\n",
    "\n",
    "The Scribe-GRU series—**Scribe-α**, **Scribe-β**, and **Scribe-γ**—reflects a progression in both **architectural depth** and **semantic richness** enabled by subword-level tokenization.\n",
    "\n",
    "Here is a **single unified comparison table** that merges **architecture**, **capacity**, and **parameter statistics** for the entire **Scribe-GRU Series**.\n",
    "\n",
    "Perfect for documentation, reports, or GitHub READMEs.\n",
    "\n",
    "---\n",
    "\n",
    "# **Scribe-GRU Model Comparison Table**\n",
    "\n",
    "| Model        | Layers | Hidden | Embedding | Parameters | Size (MB) |  Params (M) | Expected Behavior                                     |\n",
    "| ------------ | ------ | ------ | --------- | ---------: | --------: | ----------: | ----------------------------------------------------- |\n",
    "| **Scribe-α** | 2      | 256    | 128       |  1,075,688 |   4.10 MB |  **1.08 M** | Baseline subword LM; learns local & midrange patterns |\n",
    "| **Scribe-β** | 3      | 512    | 256       |  5,102,056 |  19.46 MB |  **5.10 M** | Strong coherence and phrase-level structure           |\n",
    "| **Scribe-γ** | 4      | 768    | 384       | 14,439,400 |  55.08 MB | **14.44 M** | High-quality fluent generation; best global structure |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3aef184",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = sp.get_piece_size()   # Subword vocabulary size from SentencePiece\n",
    "\n",
    "\n",
    "# ------------------------------------------------------- Scribe-α -------------------------------------------------------\n",
    "small_model_name = \"Scribe-α\"\n",
    "model_small = MyGRUModel(\n",
    "    vocab_size = vocab_size,\n",
    "    embd_dim = 128,          # Larger than Astra to match subword semantics\n",
    "    hidden_dim = 256,\n",
    "    num_layers = 2,\n",
    "    model_name = small_model_name,\n",
    "    dropout = 0.1\n",
    ").to(device)\n",
    "\n",
    "lr_small_model = 2e-3\n",
    "epochs_small_model = 10\n",
    "weight_decay_small = 0.01\n",
    "\n",
    "optimizer_small = torch.optim.AdamW(\n",
    "    model_small.parameters(),\n",
    "    lr = lr_small_model,\n",
    "    weight_decay = weight_decay_small\n",
    ")\n",
    "\n",
    "scheduler_small = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_small,\n",
    "    T_max = epochs_small_model\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------- Scribe-β -------------------------------------------------------\n",
    "medium_model_name = \"Scribe-β\"\n",
    "model_medium = MyGRUModel(\n",
    "    vocab_size = vocab_size,\n",
    "    embd_dim = 256,\n",
    "    hidden_dim = 512,\n",
    "    num_layers = 3,          # deeper than fevious Astra-β\n",
    "    model_name = medium_model_name,\n",
    "    dropout = 0.1\n",
    ").to(device)\n",
    "\n",
    "lr_medium_model = 1.5e-3\n",
    "epochs_medium_model = 15\n",
    "weight_decay_medium = 0.01\n",
    "\n",
    "optimizer_medium = torch.optim.AdamW(\n",
    "    model_medium.parameters(),\n",
    "    lr = lr_medium_model,\n",
    "    weight_decay = weight_decay_medium\n",
    ")\n",
    "\n",
    "scheduler_medium = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_medium,\n",
    "    T_max = epochs_medium_model\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------- Scribe-γ -------------------------------------------------------\n",
    "large_model_name = \"Scribe-γ\"\n",
    "model_large = MyGRUModel(\n",
    "    vocab_size = vocab_size,\n",
    "    embd_dim = 384,          # very high-quality embeddings\n",
    "    hidden_dim = 768,        # large recurrent representation\n",
    "    num_layers = 4,          # deeper than Astra-γ\n",
    "    model_name = large_model_name,\n",
    "    dropout = 0.15           # slightly more dropout for stability\n",
    ").to(device)\n",
    "\n",
    "lr_large_model = 1e-3\n",
    "epochs_large_model = 20\n",
    "weight_decay_large = 0.01\n",
    "\n",
    "optimizer_large = torch.optim.AdamW(\n",
    "    model_large.parameters(),\n",
    "    lr = lr_large_model,\n",
    "    weight_decay = weight_decay_large\n",
    ")\n",
    "\n",
    "# CosineAnnealingLR scheduler: It will produce cleaner convergence and noticeably better text quality.\n",
    "scheduler_large = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_large,\n",
    "    T_max = epochs_large_model\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924d931",
   "metadata": {},
   "source": [
    "### Scribe-α Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "657b48ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ASTRA-GRU MODEL SUMMARY\n",
      "====================================================================================================\n",
      "Model Name       : Scribe-α\n",
      "Device           : cuda\n",
      "Total Epochs     : 10\n",
      "Learning Rate    : 0.002\n",
      "\n",
      "MODEL ARCHITECTURE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  └── embedding: Embedding()\n",
      "  └── layers: ModuleList()\n",
      "  └── layers.0: GRULayer()\n",
      "  └── layers.0.grucell: GRUCell()\n",
      "  └── layers.0.grucell.Wx: Linear()\n",
      "  └── layers.0.grucell.Wh: Linear()\n",
      "  └── layers.0.grucell.Wzx: Linear()\n",
      "  └── layers.0.grucell.Wzh: Linear()\n",
      "  └── layers.0.grucell.Wrx: Linear()\n",
      "  └── layers.0.grucell.Wrh: Linear()\n",
      "  └── layers.0.dropout: Dropout()\n",
      "  └── layers.1: GRULayer()\n",
      "  └── layers.1.grucell: GRUCell()\n",
      "  └── layers.1.grucell.Wx: Linear()\n",
      "  └── layers.1.grucell.Wh: Linear()\n",
      "  └── layers.1.grucell.Wzx: Linear()\n",
      "  └── layers.1.grucell.Wzh: Linear()\n",
      "  └── layers.1.grucell.Wrx: Linear()\n",
      "  └── layers.1.grucell.Wrh: Linear()\n",
      "  └── layers.1.dropout: Dropout()\n",
      "  └── fc: Linear()\n",
      "  └── fc.linear_projection: Linear()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trainable Parameters : 1,075,688\n",
      "Model Size : 4.10 MB\n",
      "\n",
      "PARAMETER BREAKDOWN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "embedding.weight                         : 128,000\n",
      "layers.0.grucell.bias_z                  : 256\n",
      "layers.0.grucell.bias_r                  : 256\n",
      "layers.0.grucell.Wx.weight               : 32,768\n",
      "layers.0.grucell.Wx.bias                 : 256\n",
      "layers.0.grucell.Wh.weight               : 65,536\n",
      "layers.0.grucell.Wzx.weight              : 32,768\n",
      "layers.0.grucell.Wzx.bias                : 256\n",
      "layers.0.grucell.Wzh.weight              : 65,536\n",
      "layers.0.grucell.Wrx.weight              : 32,768\n",
      "layers.0.grucell.Wrx.bias                : 256\n",
      "layers.0.grucell.Wrh.weight              : 65,536\n",
      "layers.1.grucell.bias_z                  : 256\n",
      "layers.1.grucell.bias_r                  : 256\n",
      "layers.1.grucell.Wx.weight               : 65,536\n",
      "layers.1.grucell.Wx.bias                 : 256\n",
      "layers.1.grucell.Wh.weight               : 65,536\n",
      "layers.1.grucell.Wzx.weight              : 65,536\n",
      "layers.1.grucell.Wzx.bias                : 256\n",
      "layers.1.grucell.Wzh.weight              : 65,536\n",
      "layers.1.grucell.Wrx.weight              : 65,536\n",
      "layers.1.grucell.Wrx.bias                : 256\n",
      "layers.1.grucell.Wrh.weight              : 65,536\n",
      "fc.linear_projection.weight              : 256,000\n",
      "fc.linear_projection.bias                : 1,000\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_model_summary(\n",
    "    model      = model_small,\n",
    "    model_name = small_model_name,\n",
    "    epochs     = epochs_small_model,\n",
    "    lr         = lr_small_model,\n",
    "    device     = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695b2ef",
   "metadata": {},
   "source": [
    "### Scribe-α Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19fc7bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- Training Started for Scribe-α Model ----------------\n",
      "\n",
      "Epoch 01/10 | Train Loss: 4.3808 | Val Loss: 4.0454\n",
      ">>> Improved validation! Best checkpoint saved at epoch 1.\n",
      "Epoch 02/10 | Train Loss: 3.4709 | Val Loss: 3.9232\n",
      ">>> Improved validation! Best checkpoint saved at epoch 2.\n",
      "Epoch 03/10 | Train Loss: 3.2741 | Val Loss: 3.8526\n",
      ">>> Improved validation! Best checkpoint saved at epoch 3.\n",
      "Epoch 04/10 | Train Loss: 3.1582 | Val Loss: 3.9209\n",
      ">>> No improvement (1/5).\n",
      "Epoch 05/10 | Train Loss: 3.0762 | Val Loss: 3.9130\n",
      ">>> No improvement (2/5).\n",
      "Epoch 06/10 | Train Loss: 3.0184 | Val Loss: 4.0360\n",
      ">>> No improvement (3/5).\n",
      "Epoch 07/10 | Train Loss: 2.9788 | Val Loss: 3.9921\n",
      ">>> No improvement (4/5).\n",
      "Epoch 08/10 | Train Loss: 2.9495 | Val Loss: 4.0641\n",
      ">>> No improvement (5/5).\n",
      "\n",
      "================ EARLY STOPPING ACTIVATED ================\n",
      "Training stopped at epoch 8. Best epoch: 3 | Best Val Loss: 3.8526\n",
      "==========================================================\n",
      "\n",
      "Loaded best checkpoint: ./best_checkpoints/Scribe-α_best.pth\n",
      "\n",
      "---------------- Training Completed for Scribe-α Model ----------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_27320\\3328753488.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "model_small = train_model_with_early_stopping(\n",
    "    model       = model_small,\n",
    "    optimizer   = optimizer_small,\n",
    "    scheduler   = scheduler_small,\n",
    "    loss_fn     = loss_fn,\n",
    "    epochs      = epochs_small_model,\n",
    "    batch_size  = 64,\n",
    "    device      = device,\n",
    "    steps       = 300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e2ca7",
   "metadata": {},
   "source": [
    "### Scribe-α Autoregressive Generation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d2fa44",
   "metadata": {},
   "source": [
    "#### Greedy Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12b8bd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have I have\n"
     ]
    }
   ],
   "source": [
    "print(sample_greedy(model_small, sp, \"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603829a9",
   "metadata": {},
   "source": [
    "#### Temperature Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcb5d267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING: Death! BUCKINGS KING HENRY BUCKINGSlike mock You show in bles, what citizens in this armo'tunin. KING EDWARD: Ange: What, and what are king to me not transed in sibew me: married Pet down, and be judged alrewell, not the chargughts to our encap, and envent crand Harrow am home; and dience sake that I have me? KING EDWARD: I have you will I earthly the most, and come That were my leave for the line! Then, and the cloud that att! Ourst thou turn back; chapet we have I make all: Thomest I think if the stirs unto ay by am glfend me: Thou arthips: God, nor\n"
     ]
    }
   ],
   "source": [
    "print(sample_with_temperature(model_small, sp, \"KING: \", temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be352441",
   "metadata": {},
   "source": [
    "#### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2c79cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: I love Somest thou for your close, for him. LEONTES: Moward him? First Shood: 'Tis servant frain. LADY ANTI VI EDWARD: And ste. DUKEATppy Couchbumpherst thou, the vens and my brother? Nursed of all your husband now, To the glicils with the sucking, And cause 'tun and unfectured on the tempt from the house to this is this man washind, and to thee then, a stood my life. LEONTES: 'tis you want furd in the sight: I shall heard. ROMEO: What, for your son! The tribon reason for you wilt not a mets the pass? What'd by my mercy. AURENCE. KING HENRY VI CAMPHOMour\n"
     ]
    }
   ],
   "source": [
    "print(sample_top_k(model_small, sp, \"FIRST CITIZEN: \", k=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb8ecde",
   "metadata": {},
   "source": [
    "### Scribe-α Model Checkpoint Saving and Archival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba962480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at: ./scribe_saved_models\\Scribe_alpha_v1.pth\n",
      "Also updated: Scribe_alpha_latest.pth\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./scribe_saved_models\\\\Scribe_alpha_v1.pth'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(\n",
    "    model      = model_small,\n",
    "    base_name  = \"Scribe_alpha\",\n",
    "    path       = \"./scribe_saved_models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7839cbce",
   "metadata": {},
   "source": [
    "### Scribe-β Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7587d7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ASTRA-GRU MODEL SUMMARY\n",
      "====================================================================================================\n",
      "Model Name       : Scribe-β\n",
      "Device           : cuda\n",
      "Total Epochs     : 15\n",
      "Learning Rate    : 0.0015\n",
      "\n",
      "MODEL ARCHITECTURE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  └── embedding: Embedding()\n",
      "  └── layers: ModuleList()\n",
      "  └── layers.0: GRULayer()\n",
      "  └── layers.0.grucell: GRUCell()\n",
      "  └── layers.0.grucell.Wx: Linear()\n",
      "  └── layers.0.grucell.Wh: Linear()\n",
      "  └── layers.0.grucell.Wzx: Linear()\n",
      "  └── layers.0.grucell.Wzh: Linear()\n",
      "  └── layers.0.grucell.Wrx: Linear()\n",
      "  └── layers.0.grucell.Wrh: Linear()\n",
      "  └── layers.0.dropout: Dropout()\n",
      "  └── layers.1: GRULayer()\n",
      "  └── layers.1.grucell: GRUCell()\n",
      "  └── layers.1.grucell.Wx: Linear()\n",
      "  └── layers.1.grucell.Wh: Linear()\n",
      "  └── layers.1.grucell.Wzx: Linear()\n",
      "  └── layers.1.grucell.Wzh: Linear()\n",
      "  └── layers.1.grucell.Wrx: Linear()\n",
      "  └── layers.1.grucell.Wrh: Linear()\n",
      "  └── layers.1.dropout: Dropout()\n",
      "  └── layers.2: GRULayer()\n",
      "  └── layers.2.grucell: GRUCell()\n",
      "  └── layers.2.grucell.Wx: Linear()\n",
      "  └── layers.2.grucell.Wh: Linear()\n",
      "  └── layers.2.grucell.Wzx: Linear()\n",
      "  └── layers.2.grucell.Wzh: Linear()\n",
      "  └── layers.2.grucell.Wrx: Linear()\n",
      "  └── layers.2.grucell.Wrh: Linear()\n",
      "  └── layers.2.dropout: Dropout()\n",
      "  └── fc: Linear()\n",
      "  └── fc.linear_projection: Linear()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trainable Parameters : 5,102,056\n",
      "Model Size : 19.46 MB\n",
      "\n",
      "PARAMETER BREAKDOWN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "embedding.weight                         : 256,000\n",
      "layers.0.grucell.bias_z                  : 512\n",
      "layers.0.grucell.bias_r                  : 512\n",
      "layers.0.grucell.Wx.weight               : 131,072\n",
      "layers.0.grucell.Wx.bias                 : 512\n",
      "layers.0.grucell.Wh.weight               : 262,144\n",
      "layers.0.grucell.Wzx.weight              : 131,072\n",
      "layers.0.grucell.Wzx.bias                : 512\n",
      "layers.0.grucell.Wzh.weight              : 262,144\n",
      "layers.0.grucell.Wrx.weight              : 131,072\n",
      "layers.0.grucell.Wrx.bias                : 512\n",
      "layers.0.grucell.Wrh.weight              : 262,144\n",
      "layers.1.grucell.bias_z                  : 512\n",
      "layers.1.grucell.bias_r                  : 512\n",
      "layers.1.grucell.Wx.weight               : 262,144\n",
      "layers.1.grucell.Wx.bias                 : 512\n",
      "layers.1.grucell.Wh.weight               : 262,144\n",
      "layers.1.grucell.Wzx.weight              : 262,144\n",
      "layers.1.grucell.Wzx.bias                : 512\n",
      "layers.1.grucell.Wzh.weight              : 262,144\n",
      "layers.1.grucell.Wrx.weight              : 262,144\n",
      "layers.1.grucell.Wrx.bias                : 512\n",
      "layers.1.grucell.Wrh.weight              : 262,144\n",
      "layers.2.grucell.bias_z                  : 512\n",
      "layers.2.grucell.bias_r                  : 512\n",
      "layers.2.grucell.Wx.weight               : 262,144\n",
      "layers.2.grucell.Wx.bias                 : 512\n",
      "layers.2.grucell.Wh.weight               : 262,144\n",
      "layers.2.grucell.Wzx.weight              : 262,144\n",
      "layers.2.grucell.Wzx.bias                : 512\n",
      "layers.2.grucell.Wzh.weight              : 262,144\n",
      "layers.2.grucell.Wrx.weight              : 262,144\n",
      "layers.2.grucell.Wrx.bias                : 512\n",
      "layers.2.grucell.Wrh.weight              : 262,144\n",
      "fc.linear_projection.weight              : 512,000\n",
      "fc.linear_projection.bias                : 1,000\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_model_summary(\n",
    "    model      = model_medium,\n",
    "    model_name = medium_model_name,\n",
    "    epochs     = epochs_medium_model,\n",
    "    lr         = lr_medium_model,\n",
    "    device     = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869bc0e0",
   "metadata": {},
   "source": [
    "### Scribe-β Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df3c393d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- Training Started for Scribe-β Model ----------------\n",
      "\n",
      "Epoch 01/15 | Train Loss: 3.9959 | Val Loss: 3.9552\n",
      ">>> Improved validation! Best checkpoint saved at epoch 1.\n",
      "Epoch 02/15 | Train Loss: 2.9168 | Val Loss: 4.1868\n",
      ">>> No improvement (1/5).\n",
      "Epoch 03/15 | Train Loss: 2.5432 | Val Loss: 4.3154\n",
      ">>> No improvement (2/5).\n",
      "Epoch 04/15 | Train Loss: 2.3196 | Val Loss: 4.3671\n",
      ">>> No improvement (3/5).\n",
      "Epoch 05/15 | Train Loss: 2.1622 | Val Loss: 4.5978\n",
      ">>> No improvement (4/5).\n",
      "Epoch 06/15 | Train Loss: 2.0360 | Val Loss: 4.7648\n",
      ">>> No improvement (5/5).\n",
      "\n",
      "================ EARLY STOPPING ACTIVATED ================\n",
      "Training stopped at epoch 6. Best epoch: 1 | Best Val Loss: 3.9552\n",
      "==========================================================\n",
      "\n",
      "Loaded best checkpoint: ./best_checkpoints/Scribe-β_best.pth\n",
      "\n",
      "---------------- Training Completed for Scribe-β Model ----------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_27320\\3328753488.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "model_medium = train_model_with_early_stopping(\n",
    "    model       = model_medium,\n",
    "    optimizer   = optimizer_medium,\n",
    "    scheduler   = scheduler_medium,\n",
    "    loss_fn     = loss_fn,\n",
    "    epochs      = epochs_medium_model,\n",
    "    batch_size  = 64,\n",
    "    device      = device,\n",
    "    steps       = 350\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea64c8",
   "metadata": {},
   "source": [
    "### Scribe-β Autoregressive Generation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09453bc",
   "metadata": {},
   "source": [
    "#### Greedy Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ea429be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: I have done, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free, and free,\n"
     ]
    }
   ],
   "source": [
    "print(sample_greedy(model_medium, sp, \"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce29fae",
   "metadata": {},
   "source": [
    "#### Temperature Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c74042e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING: Cans of my poor eyes Wheretinge amen's that-axford 'Wisten of a grabout wilt were full made Montague ange. AN Grewife be a pay be fools to the earth, with me! Fareless like numbemate the and shiles, More, lastever the provipherish, and give; wholy in the wast? She to be deadly new, I now, To suffice and like with her, That I, and dwell, Ox are the court it may faith home? CORIOLANUS: They thus ga! Abever, That times, Avant overs never he was, that hath soleLAND: Let him; Only I amongs have done: My lies in our day of yourselves to one love\n"
     ]
    }
   ],
   "source": [
    "print(sample_with_temperature(model_medium, sp, \"KING: \", temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028ddd2",
   "metadata": {},
   "source": [
    "#### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "963f0c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: For my law, Against, To the ready, and force, my cuteel'shiccent, that I do more. Fromise, I speak. Second Servant in her, to dost thou art once with thee? I say, More-d for a falted at Look: Let him? Most as my tities. Make my father that I was done. QUEEN ELIZABETH: Why, and his brothers from this cast onceard, and to behos. Hads me To hear, and a tarnes to death, my sons, in him: The queen, See: Oxtain, and reverence that will, I have I would they are here with him and phy, Liffector to a curst to the cause I would have you have frown the duke! Watch, fort\n"
     ]
    }
   ],
   "source": [
    "print(sample_top_k(model_medium, sp, \"FIRST CITIZEN: \", k=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54da2aa",
   "metadata": {},
   "source": [
    "### Scribe-β Model Checkpoint Saving and Archival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a7bbe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at: ./scribe_saved_models\\Scribe_beta_v1.pth\n",
      "Also updated: Scribe_beta_latest.pth\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./scribe_saved_models\\\\Scribe_beta_v1.pth'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(\n",
    "    model      = model_medium,\n",
    "    base_name  = \"Scribe_beta\",\n",
    "    path       = \"./scribe_saved_models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6522c5",
   "metadata": {},
   "source": [
    "### Scribe-γ Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c1b2ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ASTRA-GRU MODEL SUMMARY\n",
      "====================================================================================================\n",
      "Model Name       : Scribe-γ\n",
      "Device           : cuda\n",
      "Total Epochs     : 20\n",
      "Learning Rate    : 0.001\n",
      "\n",
      "MODEL ARCHITECTURE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  └── embedding: Embedding()\n",
      "  └── layers: ModuleList()\n",
      "  └── layers.0: GRULayer()\n",
      "  └── layers.0.grucell: GRUCell()\n",
      "  └── layers.0.grucell.Wx: Linear()\n",
      "  └── layers.0.grucell.Wh: Linear()\n",
      "  └── layers.0.grucell.Wzx: Linear()\n",
      "  └── layers.0.grucell.Wzh: Linear()\n",
      "  └── layers.0.grucell.Wrx: Linear()\n",
      "  └── layers.0.grucell.Wrh: Linear()\n",
      "  └── layers.0.dropout: Dropout()\n",
      "  └── layers.1: GRULayer()\n",
      "  └── layers.1.grucell: GRUCell()\n",
      "  └── layers.1.grucell.Wx: Linear()\n",
      "  └── layers.1.grucell.Wh: Linear()\n",
      "  └── layers.1.grucell.Wzx: Linear()\n",
      "  └── layers.1.grucell.Wzh: Linear()\n",
      "  └── layers.1.grucell.Wrx: Linear()\n",
      "  └── layers.1.grucell.Wrh: Linear()\n",
      "  └── layers.1.dropout: Dropout()\n",
      "  └── layers.2: GRULayer()\n",
      "  └── layers.2.grucell: GRUCell()\n",
      "  └── layers.2.grucell.Wx: Linear()\n",
      "  └── layers.2.grucell.Wh: Linear()\n",
      "  └── layers.2.grucell.Wzx: Linear()\n",
      "  └── layers.2.grucell.Wzh: Linear()\n",
      "  └── layers.2.grucell.Wrx: Linear()\n",
      "  └── layers.2.grucell.Wrh: Linear()\n",
      "  └── layers.2.dropout: Dropout()\n",
      "  └── layers.3: GRULayer()\n",
      "  └── layers.3.grucell: GRUCell()\n",
      "  └── layers.3.grucell.Wx: Linear()\n",
      "  └── layers.3.grucell.Wh: Linear()\n",
      "  └── layers.3.grucell.Wzx: Linear()\n",
      "  └── layers.3.grucell.Wzh: Linear()\n",
      "  └── layers.3.grucell.Wrx: Linear()\n",
      "  └── layers.3.grucell.Wrh: Linear()\n",
      "  └── layers.3.dropout: Dropout()\n",
      "  └── fc: Linear()\n",
      "  └── fc.linear_projection: Linear()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trainable Parameters : 14,439,400\n",
      "Model Size : 55.08 MB\n",
      "\n",
      "PARAMETER BREAKDOWN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "embedding.weight                         : 384,000\n",
      "layers.0.grucell.bias_z                  : 768\n",
      "layers.0.grucell.bias_r                  : 768\n",
      "layers.0.grucell.Wx.weight               : 294,912\n",
      "layers.0.grucell.Wx.bias                 : 768\n",
      "layers.0.grucell.Wh.weight               : 589,824\n",
      "layers.0.grucell.Wzx.weight              : 294,912\n",
      "layers.0.grucell.Wzx.bias                : 768\n",
      "layers.0.grucell.Wzh.weight              : 589,824\n",
      "layers.0.grucell.Wrx.weight              : 294,912\n",
      "layers.0.grucell.Wrx.bias                : 768\n",
      "layers.0.grucell.Wrh.weight              : 589,824\n",
      "layers.1.grucell.bias_z                  : 768\n",
      "layers.1.grucell.bias_r                  : 768\n",
      "layers.1.grucell.Wx.weight               : 589,824\n",
      "layers.1.grucell.Wx.bias                 : 768\n",
      "layers.1.grucell.Wh.weight               : 589,824\n",
      "layers.1.grucell.Wzx.weight              : 589,824\n",
      "layers.1.grucell.Wzx.bias                : 768\n",
      "layers.1.grucell.Wzh.weight              : 589,824\n",
      "layers.1.grucell.Wrx.weight              : 589,824\n",
      "layers.1.grucell.Wrx.bias                : 768\n",
      "layers.1.grucell.Wrh.weight              : 589,824\n",
      "layers.2.grucell.bias_z                  : 768\n",
      "layers.2.grucell.bias_r                  : 768\n",
      "layers.2.grucell.Wx.weight               : 589,824\n",
      "layers.2.grucell.Wx.bias                 : 768\n",
      "layers.2.grucell.Wh.weight               : 589,824\n",
      "layers.2.grucell.Wzx.weight              : 589,824\n",
      "layers.2.grucell.Wzx.bias                : 768\n",
      "layers.2.grucell.Wzh.weight              : 589,824\n",
      "layers.2.grucell.Wrx.weight              : 589,824\n",
      "layers.2.grucell.Wrx.bias                : 768\n",
      "layers.2.grucell.Wrh.weight              : 589,824\n",
      "layers.3.grucell.bias_z                  : 768\n",
      "layers.3.grucell.bias_r                  : 768\n",
      "layers.3.grucell.Wx.weight               : 589,824\n",
      "layers.3.grucell.Wx.bias                 : 768\n",
      "layers.3.grucell.Wh.weight               : 589,824\n",
      "layers.3.grucell.Wzx.weight              : 589,824\n",
      "layers.3.grucell.Wzx.bias                : 768\n",
      "layers.3.grucell.Wzh.weight              : 589,824\n",
      "layers.3.grucell.Wrx.weight              : 589,824\n",
      "layers.3.grucell.Wrx.bias                : 768\n",
      "layers.3.grucell.Wrh.weight              : 589,824\n",
      "fc.linear_projection.weight              : 768,000\n",
      "fc.linear_projection.bias                : 1,000\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_model_summary(\n",
    "    model      = model_large,\n",
    "    model_name = large_model_name,\n",
    "    epochs     = epochs_large_model,\n",
    "    lr         = lr_large_model,\n",
    "    device     = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f161ca",
   "metadata": {},
   "source": [
    "### Scribe-γ Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b181f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- Training Started for Scribe-γ Model ----------------\n",
      "\n",
      "Epoch 01/20 | Train Loss: 5.8324 | Val Loss: 5.0179\n",
      ">>> Improved validation! Best checkpoint saved at epoch 1.\n",
      "Epoch 02/20 | Train Loss: 4.4485 | Val Loss: 4.5103\n",
      ">>> Improved validation! Best checkpoint saved at epoch 2.\n",
      "Epoch 03/20 | Train Loss: 3.6452 | Val Loss: 4.1581\n",
      ">>> Improved validation! Best checkpoint saved at epoch 3.\n",
      "Epoch 04/20 | Train Loss: 3.2797 | Val Loss: 4.2000\n",
      ">>> No improvement (1/5).\n",
      "Epoch 05/20 | Train Loss: 3.0617 | Val Loss: 4.2525\n",
      ">>> No improvement (2/5).\n",
      "Epoch 06/20 | Train Loss: 2.9104 | Val Loss: 4.3303\n",
      ">>> No improvement (3/5).\n",
      "Epoch 07/20 | Train Loss: 2.7828 | Val Loss: 4.3232\n",
      ">>> No improvement (4/5).\n",
      "Epoch 08/20 | Train Loss: 2.6766 | Val Loss: 4.4206\n",
      ">>> No improvement (5/5).\n",
      "\n",
      "================ EARLY STOPPING ACTIVATED ================\n",
      "Training stopped at epoch 8. Best epoch: 3 | Best Val Loss: 4.1581\n",
      "==========================================================\n",
      "\n",
      "Loaded best checkpoint: ./best_checkpoints/Scribe-γ_best.pth\n",
      "\n",
      "---------------- Training Completed for Scribe-γ Model ----------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_27320\\3328753488.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "model_large = train_model_with_early_stopping(\n",
    "    model       = model_large,\n",
    "    optimizer   = optimizer_large,\n",
    "    scheduler   = scheduler_large,\n",
    "    loss_fn     = loss_fn,\n",
    "    epochs      = epochs_large_model,\n",
    "    batch_size  = 64,\n",
    "    device      = device,\n",
    "    steps       = 450     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b055c5",
   "metadata": {},
   "source": [
    "### Scribe-γ Autoregressive Generation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e521587",
   "metadata": {},
   "source": [
    "#### Greedy Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "039100c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: I am the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and the charger, and\n"
     ]
    }
   ],
   "source": [
    "print(sample_greedy(model_large, sp, \"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f894b6",
   "metadata": {},
   "source": [
    "#### Temperature Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc16c9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING: He had a cast; I sehile I sievid: This and to act to shameled, Tillabley mean not, Forting a diing blood will behoppifts, But bes, By my sons tovick me, I have become, sir, And bear to do a holaat and then, joft, nex our bace and then, sir That shall kiss what, nor needful tracet on thee: The country. ROMEO: His savenion, my miscond myself, which a lost me to have die, When I am o't, my news, What to to Edward is enter! Hint To live, for cure Hath therefore in thy brother hadely, some presence a comfiver, that she'sting my jon a supy. Would not a\n"
     ]
    }
   ],
   "source": [
    "print(sample_with_temperature(model_large, sp, \"KING: \", temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17eb033",
   "metadata": {},
   "source": [
    "#### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "848a2f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: For the strull; and reumeverless in me now as he had the people, if the suvid of wared, in mines, with some falblasaals, which to her. KING RICHARDoth, which and breat to me that I dress'fush, then so. KING RICHARD III: Aprembal, and his brother that of ser that he was, when to reason of my valless touch a faining by her lights, as I could have the daal, And give my son; if the suppiigh, a deeper, And indees, he shall be while. FRAD: So. YORK: Why to-bemes; And I have so cast we may be a wish-s; And that he would say. What is strief, it; the dies\n"
     ]
    }
   ],
   "source": [
    "print(sample_top_k(model_large, sp, \"FIRST CITIZEN: \", k=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df492c6",
   "metadata": {},
   "source": [
    "### Scribe-γ Model Checkpoint Saving and Archival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "300fcca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at: ./scribe_saved_models\\Scribe_gamma_v1.pth\n",
      "Also updated: Scribe_gamma_latest.pth\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./scribe_saved_models\\\\Scribe_gamma_v1.pth'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(\n",
    "    model      = model_large,\n",
    "    base_name  = \"Scribe_gamma\",\n",
    "    path       = \"./scribe_saved_models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206273e0",
   "metadata": {},
   "source": [
    "## Loading the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "914ddab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ MODEL LOADED ================\n",
      "Loaded File      : ./scribe_saved_models/Scribe_alpha_latest.pth\n",
      "Model Name       : Scribe-α\n",
      "Model Class      : MyGRUModel\n",
      "Version          : v1\n",
      "Timestamp        : 2025-12-14 23:18:14\n",
      "----------------------------------------------\n",
      "Model Architecture:\n",
      "  └── embedding: Embedding\n",
      "  └── layers: ModuleList\n",
      "  └── layers.0: GRULayer\n",
      "  └── layers.0.grucell: GRUCell\n",
      "  └── layers.0.grucell.Wx: Linear\n",
      "  └── layers.0.grucell.Wh: Linear\n",
      "  └── layers.0.grucell.Wzx: Linear\n",
      "  └── layers.0.grucell.Wzh: Linear\n",
      "  └── layers.0.grucell.Wrx: Linear\n",
      "  └── layers.0.grucell.Wrh: Linear\n",
      "  └── layers.0.dropout: Dropout\n",
      "  └── layers.1: GRULayer\n",
      "  └── layers.1.grucell: GRUCell\n",
      "  └── layers.1.grucell.Wx: Linear\n",
      "  └── layers.1.grucell.Wh: Linear\n",
      "  └── layers.1.grucell.Wzx: Linear\n",
      "  └── layers.1.grucell.Wzh: Linear\n",
      "  └── layers.1.grucell.Wrx: Linear\n",
      "  └── layers.1.grucell.Wrh: Linear\n",
      "  └── layers.1.dropout: Dropout\n",
      "  └── fc: Linear\n",
      "  └── fc.linear_projection: Linear\n",
      "----------------------------------------------\n",
      "Total Parameters : 1,075,688\n",
      "Loaded on Device : cuda\n",
      "==============================================\n",
      "\n",
      "\n",
      "================ MODEL LOADED ================\n",
      "Loaded File      : ./scribe_saved_models/Scribe_beta_latest.pth\n",
      "Model Name       : Scribe-β\n",
      "Model Class      : MyGRUModel\n",
      "Version          : v1\n",
      "Timestamp        : 2025-12-14 23:44:53\n",
      "----------------------------------------------\n",
      "Model Architecture:\n",
      "  └── embedding: Embedding\n",
      "  └── layers: ModuleList\n",
      "  └── layers.0: GRULayer\n",
      "  └── layers.0.grucell: GRUCell\n",
      "  └── layers.0.grucell.Wx: Linear\n",
      "  └── layers.0.grucell.Wh: Linear\n",
      "  └── layers.0.grucell.Wzx: Linear\n",
      "  └── layers.0.grucell.Wzh: Linear\n",
      "  └── layers.0.grucell.Wrx: Linear\n",
      "  └── layers.0.grucell.Wrh: Linear\n",
      "  └── layers.0.dropout: Dropout\n",
      "  └── layers.1: GRULayer\n",
      "  └── layers.1.grucell: GRUCell\n",
      "  └── layers.1.grucell.Wx: Linear\n",
      "  └── layers.1.grucell.Wh: Linear\n",
      "  └── layers.1.grucell.Wzx: Linear\n",
      "  └── layers.1.grucell.Wzh: Linear\n",
      "  └── layers.1.grucell.Wrx: Linear\n",
      "  └── layers.1.grucell.Wrh: Linear\n",
      "  └── layers.1.dropout: Dropout\n",
      "  └── layers.2: GRULayer\n",
      "  └── layers.2.grucell: GRUCell\n",
      "  └── layers.2.grucell.Wx: Linear\n",
      "  └── layers.2.grucell.Wh: Linear\n",
      "  └── layers.2.grucell.Wzx: Linear\n",
      "  └── layers.2.grucell.Wzh: Linear\n",
      "  └── layers.2.grucell.Wrx: Linear\n",
      "  └── layers.2.grucell.Wrh: Linear\n",
      "  └── layers.2.dropout: Dropout\n",
      "  └── fc: Linear\n",
      "  └── fc.linear_projection: Linear\n",
      "----------------------------------------------\n",
      "Total Parameters : 5,102,056\n",
      "Loaded on Device : cuda\n",
      "==============================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_27320\\794920423.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ MODEL LOADED ================\n",
      "Loaded File      : ./scribe_saved_models/Scribe_gamma_latest.pth\n",
      "Model Name       : Scribe-γ\n",
      "Model Class      : MyGRUModel\n",
      "Version          : v1\n",
      "Timestamp        : 2025-12-15 01:12:33\n",
      "----------------------------------------------\n",
      "Model Architecture:\n",
      "  └── embedding: Embedding\n",
      "  └── layers: ModuleList\n",
      "  └── layers.0: GRULayer\n",
      "  └── layers.0.grucell: GRUCell\n",
      "  └── layers.0.grucell.Wx: Linear\n",
      "  └── layers.0.grucell.Wh: Linear\n",
      "  └── layers.0.grucell.Wzx: Linear\n",
      "  └── layers.0.grucell.Wzh: Linear\n",
      "  └── layers.0.grucell.Wrx: Linear\n",
      "  └── layers.0.grucell.Wrh: Linear\n",
      "  └── layers.0.dropout: Dropout\n",
      "  └── layers.1: GRULayer\n",
      "  └── layers.1.grucell: GRUCell\n",
      "  └── layers.1.grucell.Wx: Linear\n",
      "  └── layers.1.grucell.Wh: Linear\n",
      "  └── layers.1.grucell.Wzx: Linear\n",
      "  └── layers.1.grucell.Wzh: Linear\n",
      "  └── layers.1.grucell.Wrx: Linear\n",
      "  └── layers.1.grucell.Wrh: Linear\n",
      "  └── layers.1.dropout: Dropout\n",
      "  └── layers.2: GRULayer\n",
      "  └── layers.2.grucell: GRUCell\n",
      "  └── layers.2.grucell.Wx: Linear\n",
      "  └── layers.2.grucell.Wh: Linear\n",
      "  └── layers.2.grucell.Wzx: Linear\n",
      "  └── layers.2.grucell.Wzh: Linear\n",
      "  └── layers.2.grucell.Wrx: Linear\n",
      "  └── layers.2.grucell.Wrh: Linear\n",
      "  └── layers.2.dropout: Dropout\n",
      "  └── layers.3: GRULayer\n",
      "  └── layers.3.grucell: GRUCell\n",
      "  └── layers.3.grucell.Wx: Linear\n",
      "  └── layers.3.grucell.Wh: Linear\n",
      "  └── layers.3.grucell.Wzx: Linear\n",
      "  └── layers.3.grucell.Wzh: Linear\n",
      "  └── layers.3.grucell.Wrx: Linear\n",
      "  └── layers.3.grucell.Wrh: Linear\n",
      "  └── layers.3.dropout: Dropout\n",
      "  └── fc: Linear\n",
      "  └── fc.linear_projection: Linear\n",
      "----------------------------------------------\n",
      "Total Parameters : 14,439,400\n",
      "Loaded on Device : cuda\n",
      "==============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "scribe_alpha = load_model(\n",
    "    filepath = \"./scribe_saved_models/Scribe_alpha_latest.pth\",\n",
    "    device   = device\n",
    ")\n",
    "\n",
    "scribe_beta = load_model(\n",
    "    filepath = \"./scribe_saved_models/Scribe_beta_latest.pth\",\n",
    "    device   = device\n",
    ")\n",
    "\n",
    "scribe_gamma = load_model(\n",
    "    filepath = \"./scribe_saved_models/Scribe_gamma_latest.pth\",\n",
    "    device   = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8a275f",
   "metadata": {},
   "source": [
    "## Sampling from the Loaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b64ec3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: Master me, for the king! Mort me, Com of loveday: Montent To the proud, my heart of heaven: The died. But his power: If thou shake Mercy's at her, I have you doges 'tis a bodies I cause your pale lips: Mow'd to sitive, forteen fortake, To-jector, I do me, youthough forceven. KING EDWARD IV: he be done; for a tendsalks, welack' the depon; whole is a little. I say to hear her? CORIOLANUS: Liewitors, sir: he doth burdraveltheades 'tis with it was ABY Aughting: Ty: Occess. WARWICK, letters; and helding to do it be\n"
     ]
    }
   ],
   "source": [
    "print(sample_top_k(scribe_alpha, sp, \"FIRST CITIZEN: \", k=30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75efba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: What chargage, Saint, That hell'ppy danch! Once that wearnees of the queen. DUKE VINCENTIO: Nay, the sorrow I save so great din, That misest that hellows I want, Shaping, sir, and I have been: There'll, Monton, Meethurst thou hastealten: What stain of your grace? First: Let us I, Abs. Cory-dge with you well-hes on the duke, behad at it, be ready Must; And damshipurlough, to my souls of my meth I should befallhipulous len the gave a man: Thou can chell in my bothes to his chollenerible, and fore, as helding\n"
     ]
    }
   ],
   "source": [
    "print(sample_top_k(scribe_beta, sp, \"FIRST CITIZEN: \", k=30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68789cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: I do so and he while? DUKEALGLLLLO: 's; 's. Plam's and a bad, thou beling to me. Ataves him, At. But he hath pramed the pale? O'd to me at the cursed. What? Hapts, which it. DUKE VINCENTIO: I am a courtler a doupard. VIure, to him as thou, thou wilt have strong in my sort and his sats. KING RICHARDoth on his deemerly is, as your father that I do a gavers, and gl: I have sect the people, I have to the courtle the sor. DUKE OF GLOUCESTER: 'bits. CORIOLANUS: But he would be dass: I prayed my mercile to the stolily, my f\n"
     ]
    }
   ],
   "source": [
    "print(sample_top_k(scribe_gamma, sp, \"FIRST CITIZEN: \", k=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ccf93e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
