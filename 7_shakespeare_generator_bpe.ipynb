{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f102aa5d",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook presents an extended investigation into recurrent neural network architectures by advancing the original character-level Astra-GRU experiments into a **subword-level language modeling framework**.\n",
    "In this new study, we introduce the **Scribe-GRU Series**—a family of three recurrent models (Scribe-α, Scribe-β, and Scribe-γ) trained on the *Tiny Shakespeare* corpus using **SentencePiece-based subword tokenization** instead of raw characters.\n",
    "\n",
    "Where the Astra-GRU series examined the expressive limits of GRUs at the character level, the Scribe-GRU series explores how transitioning to **morpheme-like subword units** affects modeling capacity, compositional structure, and generative quality. Subword-level modeling provides a powerful intermediate granularity: tokens are more meaningful than characters but significantly more flexible than full words, allowing the model to learn syntax, phonetic structure, and stylistic patterns with far greater efficiency.\n",
    "\n",
    "Despite this shift in linguistic representation, **all core computational components remain unchanged**.\n",
    "Each model continues to use the same manually implemented GRUCell and GRULayer classes defined previously, preserving:\n",
    "\n",
    "* the **single-bias gate formulation**,\n",
    "* the **canonical reset-gate application to the raw hidden state**, and\n",
    "* the fully hand-written recurrent logic that diverges intentionally from PyTorch’s optimized GRU internals.\n",
    "\n",
    "This ensures that any observed improvements in depth, coherence, or stylistic fidelity arise solely from the **change in tokenization strategy**, not from modifications to the recurrent architecture itself.\n",
    "\n",
    "The Tiny Shakespeare dataset remains the underlying training corpus, but the text is now processed using a **SentencePiece BPE subword tokenizer**, enabling the model to operate over a compact vocabulary of semantically meaningful units. This allows GRUs to capture longer-range dependencies, more stable word-like structures, and richer stylistic patterns that are difficult to model at the character level.\n",
    "\n",
    "To systematically study scaling behavior under this new tokenization regime, three GRU architectures of increasing complexity are trained:\n",
    "\n",
    "* **Scribe-α** — a lightweight single-layer subword GRU\n",
    "* **Scribe-β** — a medium-capacity two-layer configuration\n",
    "* **Scribe-γ** — a high-capacity three-layer recurrent model with expanded hidden representation\n",
    "\n",
    "Each model is evaluated on its ability to perform autoregressive subword generation, reconstruct Shakespearean style, and maintain coherent linguistic structure across extended sequences. Comparative analyses are provided in terms of architecture, parameter count, training dynamics, and qualitative generation quality.\n",
    "\n",
    "This notebook documents the complete workflow—from subword tokenizer construction and dataset encoding to model training, evaluation, and sampling—thereby building upon and extending the original Astra-GRU experimental framework into a richer and more linguistically grounded modeling paradigm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b3c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_file = \"tiny_shakespeare.txt\"\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=input_file,\n",
    "    model_prefix=\"shakespeare_bpe\",\n",
    "    vocab_size=1000,\n",
    "    character_coverage=1.0,     \n",
    "    model_type=\"bpe\",           \n",
    "    bos_id=1,                   \n",
    "    eos_id=2,\n",
    "    unk_id=0                    \n",
    ")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"shakespeare_bpe.model\")\n",
    "\n",
    "data = sp.encode(open(\"tiny_shakespeare.txt\", \"r\", encoding=\"utf-8\").read(), out_type=int)\n",
    "n = int(0.9 * len(data))\n",
    "data = torch.tensor(data, dtype=torch.long)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "block_size = 128\n",
    "def get_batch(split=\"train\", batch_size=64):\n",
    "    source = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(source) - block_size - 1, (batch_size,))\n",
    "    X = torch.stack([source[i:i+block_size] for i in ix])\n",
    "    Y = torch.stack([source[i+1:i+block_size+1] for i in ix])\n",
    "    return X.to(device), Y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d05337",
   "metadata": {},
   "source": [
    "## GRUCell's Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cae64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    \"\"\" \n",
    "    This Implementation is Differ from the PyTorch's Official Implementation in 2 Different Ways\n",
    "    NOTE-1:\n",
    "        This is not the official Implementation of PyTorch's GRUcell Because they use 2 biases per Gate\n",
    "        and i'm only using 1 bias per Gate\n",
    "\n",
    "        PyTorch's Official Implementation: \n",
    "        r = σ(W_ir x + b_ir + W_hr h + b_hr)\n",
    "        z = σ(W_iz x + b_iz + W_hz h + b_hz)\n",
    "        n = tanh(W_in x + b_in + r ⊙ (W_hn h + b_hn))\n",
    "\n",
    "        They Use 2 Bias per Gate\n",
    "    \n",
    "    NOTE-2: \n",
    "        They apply the reset gate (r) after the Multiplication of W_hn and addition of b_hn on the h_prev\n",
    "        2. Original implementation: Apply the Hadamard product (⊙) between r_t and h_prev and then apply the\n",
    "            Matrix Transformation and bias addition\n",
    "        3. What PyTorch does is, they apply the Matrix Transformation (Matrix Multiplication and bias addition) 1st and then\n",
    "            they apply the Hadamard product (⊙) between (W_hn h + b_hn)\n",
    "    \"\"\"\n",
    "    def __init__(self, embd_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        -> Bias only on x: input\n",
    "        -> No Bias on Hidden States\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Candidate transformation\n",
    "        self.Wx = nn.Linear(embd_dim, hidden_dim, bias = True)\n",
    "        self.Wh = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "\n",
    "        # Update Gate Specific Parameters\n",
    "        self.Wzx = nn.Linear(embd_dim, hidden_dim, bias = True)\n",
    "        self.Wzh = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.bias_z = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        # Reset Gate Specific Parameters\n",
    "        self.Wrx = nn.Linear(embd_dim, hidden_dim, bias = True)\n",
    "        self.Wrh = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.bias_r = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        --> A proposed update → candidate (h̃_t)\n",
    "        --> A decision gate → update gate (z_t)\n",
    "        --> A final controlled update → h_t\n",
    "\n",
    "        NOTE:   1. Reset Gate Filters h_prev\n",
    "                    - r_t = sigmoid( ( (x_t @ W_rx) + (h_prev @ W_rh) + b_r) )\n",
    "                2. Apply filter to h_prev\n",
    "                    - filtered_h_prev = r_t * h_prev [NOTE: (where * is element-wise multiplication)]\n",
    "                        - Meaning:\n",
    "                            - If r_t ≈ 0 → ignore old memory when forming candidate\n",
    "                            - If r_t ≈ 1 → use old memory fully\n",
    "                3. Compute candidate\n",
    "                    - h̃_t = tanh( ( (x_t @ W_hx) + (filtered_h_prev @ W_hh) + b_h) )\n",
    "                        - Meaning: \n",
    "                            - This produces a new memory proposal: A proposed update\n",
    "                4. Final hidden state\n",
    "                    - h_t = (1 - z_t) * h_prev + z_t * h̃_t [NOTE: (where * is element-wise multiplication)]\n",
    "        \"\"\"\n",
    "        r_t = torch.sigmoid((self.Wrx(x)) + (self.Wrh(h_prev)) + self.bias_r)\n",
    "        z_t = torch.sigmoid((self.Wzx(x)) + (self.Wzh(h_prev)) + self.bias_z)\n",
    "        h_tilde = torch.tanh(self.Wx(x) + self.Wh(r_t * h_prev))\n",
    "\n",
    "        h = (1 - z_t) * h_prev + z_t * h_tilde\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00153af7",
   "metadata": {},
   "source": [
    "## GRULayer's Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7404df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULayer(nn.Module):\n",
    "    def __init__(self, embd_dim, hidden_dim, dropout = 0.0):\n",
    "        super().__init__()\n",
    "        self.grucell = GRUCell(embd_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, h_prev = None):\n",
    "        batch, seq_length, _ = x.shape # x.shape --> batch, seq_length, embd_dim\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(batch, self.grucell.hidden_dim, device = x.device)\n",
    "\n",
    "        hidden_states = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h_prev = self.grucell(x_t, h_prev)\n",
    "            h_prev = self.dropout(h_prev)\n",
    "            hidden_states.append(h_prev)\n",
    "        \n",
    "        # Stack list into tensor\n",
    "        hidden_states = torch.stack(hidden_states, dim=1)\n",
    "        return hidden_states\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db3a14",
   "metadata": {},
   "source": [
    "## Linear Layer Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a23fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_classes):\n",
    "        \"\"\"\n",
    "        This layer performs a linear projection on the GRU hidden states.\n",
    "        It maps the hidden vector (of size hidden_dim) into the vocabulary space (n_classes)\n",
    "        by applying a learnable affine transformation:\n",
    "\n",
    "            logits = W h + b\n",
    "\n",
    "        This is used to convert each GRU hidden state into class probabilities\n",
    "        (e.g., next-character prediction in a name generation model).\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.linear_projection = nn.Linear(in_features = hidden_dim, out_features = n_classes, bias = True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_projection(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05612cbf",
   "metadata": {},
   "source": [
    "## Custom GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "986c395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_dim, hidden_dim, num_layers, model_name, dropout=0.0, ):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_classes = vocab_size\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embd_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(GRULayer(embd_dim, hidden_dim, dropout))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(GRULayer(hidden_dim, hidden_dim, dropout))\n",
    "\n",
    "        self.fc = Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_length)\n",
    "        x = self.embedding(x)   # (batch, seq, embd_dim)\n",
    "\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)  # (batch, seq, hidden_dim)\n",
    "\n",
    "        logits = self.fc(h)     # (batch, seq, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b5abb",
   "metadata": {},
   "source": [
    "## Function for Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc4cf01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def train_model(\n",
    "        model: MyGRUModel,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss_fn,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        device,\n",
    "        clip_value=1.0,\n",
    "        val_interval=1,\n",
    "        steps = 200\n",
    "    ):\n",
    "    \n",
    "    print(f\"\\n---------------- Training Started for {model.model_name} Model ----------------\\n\")\n",
    "    \n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for _ in range(steps):\n",
    "            X, Y = get_batch(split=\"train\", batch_size=batch_size)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                Y.reshape(-1)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= steps\n",
    "\n",
    "        # Validation\n",
    "        val_loss = None\n",
    "        if epoch % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Xv, Yv = get_batch(split=\"val\", batch_size=batch_size)\n",
    "                Xv, Yv = Xv.to(device), Yv.to(device)\n",
    "\n",
    "                logits = model(Xv)\n",
    "                val_loss = loss_fn(\n",
    "                    logits.reshape(-1, logits.size(-1)),\n",
    "                    Yv.reshape(-1)\n",
    "                ).item()\n",
    "\n",
    "        # Lr Scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Epoch and Loss Details\n",
    "        if val_loss is not None:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\n---------------- Training Completed for {model.model_name} Model ----------------\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f7fa40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import os\n",
    "\n",
    "\n",
    "def train_model_with_early_stopping(\n",
    "        model: MyGRUModel,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss_fn,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        device,\n",
    "        clip_value=1.0,\n",
    "        val_interval=1,\n",
    "        steps=200,\n",
    "        patience=5,\n",
    "        checkpoint_path=\"./best_checkpoints/\"\n",
    "    ):\n",
    "    \n",
    "    print(f\"\\n---------------- Training Started for {model.model_name} Model ----------------\\n\")\n",
    "    # steps ---> How many batches will get involve in forwardpass and backward pass\n",
    "    # steps = 200, and batch_size = 64 meaning 200 batches of each size = 64 will get involved in forwardpass and backward pass\n",
    "    # 200 * 64 * seq_length = 200 * 64 * 128 = 1.64M tokens/epoch for forward pass and 1.46M token/epoch for backward pass\n",
    "    # so for larger models, keep larger steps, for Astra-gamma step = 400 \n",
    "\n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve = 0   # counter for early stopping\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # ---------------- TRAINING LOOP ----------------\n",
    "        for _ in range(steps):\n",
    "            X, Y = get_batch(split=\"train\", batch_size=batch_size)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                Y.reshape(-1)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= steps\n",
    "\n",
    "        # ---------------- VALIDATION ----------------\n",
    "        val_loss = None\n",
    "        if epoch % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Xv, Yv = get_batch(split=\"val\", batch_size=batch_size)\n",
    "                Xv, Yv = Xv.to(device), Yv.to(device)\n",
    "\n",
    "                logits = model(Xv)\n",
    "                val_loss = loss_fn(\n",
    "                    logits.reshape(-1, logits.size(-1)),\n",
    "                    Yv.reshape(-1)\n",
    "                ).item()\n",
    "\n",
    "        # ---------------- SCHEDULER STEP ----------------\n",
    "        scheduler.step()\n",
    "\n",
    "        # ---------------- LOGGING ----------------\n",
    "        if val_loss is not None:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # ---------------- Early Stopping and Best Checkpoint ----------------\n",
    "        if val_loss is not None:\n",
    "            if val_loss < best_val:\n",
    "                best_val = val_loss\n",
    "                best_epoch = epoch\n",
    "                no_improve = 0\n",
    "\n",
    "                # Save checkpoint\n",
    "                save_path = os.path.join(checkpoint_path, f\"{model.model_name}_best.pth\")\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "\n",
    "                print(f\">>> Improved validation! Best checkpoint saved at epoch {epoch}.\")\n",
    "            \n",
    "            else:\n",
    "                no_improve += 1\n",
    "                print(f\">>> No improvement ({no_improve}/{patience}).\")\n",
    "\n",
    "            # Trigger early stopping\n",
    "            if no_improve >= patience:\n",
    "                print(\"\\n================ EARLY STOPPING ACTIVATED ================\")\n",
    "                print(f\"Training stopped at epoch {epoch}. Best epoch: {best_epoch} | Best Val Loss: {best_val:.4f}\")\n",
    "                print(\"==========================================================\\n\")\n",
    "                \n",
    "                # Load best model before returning\n",
    "                best_path = os.path.join(checkpoint_path, f\"{model.model_name}_best.pth\")\n",
    "                model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "                print(f\"Loaded best checkpoint: {best_path}\")\n",
    "                print(f\"\\n---------------- Training Completed for {model.model_name} Model ----------------\\n\")\n",
    "                return model\n",
    "\n",
    "    print(f\"\\n---------------- Training Completed for {model.model_name} Model ----------------\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8391541",
   "metadata": {},
   "source": [
    "## Sampling Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f362e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_greedy(model, sp, start_text=\"A\", max_new_tokens=200):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Encode start text\n",
    "    input_ids = torch.tensor(sp.encode(start_text), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids[:, -1:])  \n",
    "        # logits: (1, 1, vocab_size)\n",
    "\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1)  # greedy pick\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_id.unsqueeze(0)], dim=1)\n",
    "\n",
    "    return sp.decode(input_ids[0].tolist())\n",
    "\n",
    "\n",
    "def sample_with_temperature(model, sp, start_text=\"A\", max_new_tokens=200, temperature=1.0):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_ids = torch.tensor(sp.encode(start_text), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids[:, -1:])\n",
    "        logits = logits[:, -1, :] / temperature  \n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_id], dim=1)\n",
    "\n",
    "    return sp.decode(input_ids[0].tolist())\n",
    "\n",
    "\n",
    "def sample_top_k(model, sp, start_text=\"A\", max_new_tokens=200, k=20, temperature=1.0):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_ids = torch.tensor(sp.encode(start_text), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids[:, -1:])\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        # Keep only top-k logits\n",
    "        topk_vals, topk_idx = torch.topk(logits, k)\n",
    "        \n",
    "        probs = F.softmax(topk_vals, dim=-1)\n",
    "\n",
    "        # Sample from top-k\n",
    "        sampled_idx = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        next_id = topk_idx.gather(-1, sampled_idx)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_id], dim=1)\n",
    "\n",
    "    return sp.decode(input_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7351a8e",
   "metadata": {},
   "source": [
    "## Function for Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "739f1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model(model: MyGRUModel, base_name=\"Astra\", path=\"./saved_models/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # versioning\n",
    "    existing = [f for f in os.listdir(path) if f.startswith(base_name) and f.endswith(\".pth\")]\n",
    "    versions = []\n",
    "    for f in existing:\n",
    "        parts = f.replace(\".pth\", \"\").split(\"_v\")\n",
    "        if len(parts) == 2 and parts[1].isdigit():\n",
    "            versions.append(int(parts[1]))\n",
    "    next_version = max(versions, default=0) + 1\n",
    "\n",
    "    filename = f\"{base_name}_v{next_version}.pth\"\n",
    "    save_path = os.path.join(path, filename)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"model_class\": model.__class__.__name__,\n",
    "        \"model_name\": model.model_name,\n",
    "        \"n_classes\": model.n_classes,\n",
    "        \"embd_dim\": model.embd_dim,\n",
    "        \"hidden_dim\": model.hidden_dim,\n",
    "        \"dropout\": model.dropout,\n",
    "        \"vocab_size\": model.vocab_size,\n",
    "        \"num_layers\": model.num_layers,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"version\": next_version,\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, save_path)\n",
    "    torch.save(checkpoint, os.path.join(path, f\"{base_name}_latest.pth\"))\n",
    "\n",
    "    print(f\"\\nModel saved at: {save_path}\")\n",
    "    print(f\"Also updated: {base_name}_latest.pth\\n\")\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c97505",
   "metadata": {},
   "source": [
    "## Function for Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2144e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def load_model(filepath, device):\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "\n",
    "    # extract architecture parameters from checkpoint\n",
    "    model_name  = checkpoint[\"model_name\"]\n",
    "    vocab_size  = checkpoint[\"vocab_size\"]\n",
    "    embd_dim    = checkpoint[\"embd_dim\"]\n",
    "    hidden_dim  = checkpoint[\"hidden_dim\"]\n",
    "    dropout     = checkpoint[\"dropout\"]\n",
    "    num_layers     = checkpoint[\"num_layers\"]\n",
    "\n",
    "    # instantiate model using all saved metadata\n",
    "    model = MyGRUModel(\n",
    "        vocab_size = vocab_size,\n",
    "        embd_dim   = embd_dim,\n",
    "        hidden_dim = hidden_dim,\n",
    "        dropout    = dropout,\n",
    "        model_name = model_name,\n",
    "        num_layers = num_layers\n",
    "    ).to(device)\n",
    "\n",
    "    # load weights\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "    # pretty print metadata\n",
    "    print(\"\\n================ MODEL LOADED ================\")\n",
    "    print(f\"Loaded File      : {filepath}\")\n",
    "    print(f\"Model Name       : {model_name}\")\n",
    "    print(f\"Model Class      : {checkpoint['model_class']}\")\n",
    "    print(f\"Version          : v{checkpoint['version']}\")\n",
    "    print(f\"Timestamp        : {checkpoint['timestamp']}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(\"Model Architecture:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if name != \"\":\n",
    "            print(f\"  └── {name}: {module.__class__.__name__}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(f\"Total Parameters : {count_parameters(model):,}\")\n",
    "    print(f\"Loaded on Device : {device}\")\n",
    "    print(\"==============================================\\n\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6023231",
   "metadata": {},
   "source": [
    "## Function for Printing the Summary of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eea244f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_summary(model, model_name, epochs, lr, device):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ASTRA-GRU MODEL SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Model Name       : {model_name}\")\n",
    "    print(f\"Device           : {device}\")\n",
    "    print(f\"Total Epochs     : {epochs}\")\n",
    "    print(f\"Learning Rate    : {lr}\")\n",
    "\n",
    "    print(\"\\nMODEL ARCHITECTURE\")\n",
    "    print(\"-\"*100)\n",
    "    for name, module in model.named_modules():\n",
    "        if name == \"\":\n",
    "            continue\n",
    "        print(f\"  └── {name}: {module.__class__.__name__}()\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model_size_mb = n_params * 4 / (1024**2)\n",
    "\n",
    "    print(f\"\\nTrainable Parameters : {n_params:,}\")\n",
    "    print(f\"Model Size : {model_size_mb:.2f} MB\")\n",
    "\n",
    "    print(\"\\nPARAMETER BREAKDOWN\")\n",
    "    print(\"-\"*100)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name:40s} : {param.numel():,}\")\n",
    "\n",
    "    print(\"=\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828ca21",
   "metadata": {},
   "source": [
    "# **Scribe-GRU Model Family: Architectural Variants and Training Configurations**\n",
    "\n",
    "The **Scribe-GRU** series introduces a new generation of recurrent language models—**Scribe-α**, **Scribe-β**, and **Scribe-γ**—designed for **subword-level language modeling** using a SentencePiece-BPE vocabulary trained on the Tiny Shakespeare corpus.\n",
    "This family serves as a direct evolution of the Astra series, leveraging the richer semantic structure of subword tokens and expanding architectural capacity for more expressive sequence modeling.\n",
    "\n",
    "While the underlying GRUCell and GRULayer implementations remain identical to the handcrafted mechanisms used in the Astra series, the Scribe models are scaled to exploit the advantages of subword tokenization, enabling superior generalization, smoother long-range coherence, and significantly improved generative fluency.\n",
    "\n",
    "The following sections detail the structure and training configuration of all three Scribe variants.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Scribe-α Model (Small Configuration)**\n",
    "\n",
    "### **Architectural Description**\n",
    "\n",
    "Scribe-α is the entry-level configuration in the Scribe-GRU series, designed for fast experimentation on subword-level inputs.\n",
    "Unlike Astra-α—which operated on individual characters—Scribe-α benefits from semantically meaningful BPE tokens, allowing even a compact architecture to produce coherent multi-token sequences.\n",
    "\n",
    "The architecture consists of:\n",
    "\n",
    "* A subword embedding layer\n",
    "* A **2-layer** GRU stack\n",
    "* A linear output projection for next-token prediction\n",
    "\n",
    "This setup strikes a balance between simplicity and sufficient capacity to leverage subword structure.\n",
    "\n",
    "### **Hyperparameter Configuration**\n",
    "\n",
    "```\n",
    "embedding_dim = 128\n",
    "hidden_dim    = 256\n",
    "num_layers    = 2\n",
    "epochs        = 15\n",
    "learning_rate = 2e-3\n",
    "weight_decay  = 0.01\n",
    "optimizer     = AdamW\n",
    "scheduler     = CosineAnnealingLR\n",
    "dropout       = 0.1\n",
    "batch_size    = 64\n",
    "seq_length    = 128\n",
    "```\n",
    "\n",
    "### **Purpose and Expected Behavior**\n",
    "\n",
    "Scribe-α serves as the baseline for subword modeling experiments.\n",
    "It is capable of learning token-level morphology, phrase structure, and simple line-based formatting.\n",
    "Its performance already surpasses Astra-α due to the inherent modeling advantages of BPE segmentation—not architectural complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Scribe-β Model (Medium Configuration)**\n",
    "\n",
    "### **Architectural Description**\n",
    "\n",
    "Scribe-β significantly expands representational capacity through:\n",
    "\n",
    "* **Deeper recurrence (3 GRU layers)**\n",
    "* Wider hidden state (512)\n",
    "* Higher-dimensional embeddings (256)\n",
    "\n",
    "This configuration is designed for intermediate-scale modeling tasks, providing robust learning of mid-range dependencies (e.g., multi-token expressions, sentence-like structures, and repeated theatrical phrasing).\n",
    "\n",
    "The architecture consists of:\n",
    "\n",
    "* Embedding layer\n",
    "* GRU Layer 1 → GRU Layer 2 → GRU Layer 3\n",
    "* Linear projection to vocabulary size\n",
    "\n",
    "### **Hyperparameter Configuration**\n",
    "\n",
    "```\n",
    "embedding_dim = 256\n",
    "hidden_dim    = 512\n",
    "num_layers    = 3\n",
    "epochs        = 20\n",
    "learning_rate = 1.5e-3\n",
    "weight_decay  = 0.01\n",
    "optimizer     = AdamW\n",
    "scheduler     = CosineAnnealingLR\n",
    "dropout       = 0.1\n",
    "batch_size    = 64\n",
    "seq_length    = 128\n",
    "```\n",
    "\n",
    "### **Purpose and Expected Behavior**\n",
    "\n",
    "Scribe-β is designed as the **balanced workhorse** of the series.\n",
    "It is expected to produce coherent multi-line outputs, exhibit stable training curves, and capture much richer structural and stylistic patterns compared to Scribe-α.\n",
    "\n",
    "This model is well-suited for realistic Shakespeare-like generation without excessive computational requirements.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Scribe-γ Model (Large Configuration)**\n",
    "\n",
    "### **Architectural Description**\n",
    "\n",
    "Scribe-γ is the most advanced and expressive model in the Scribe-GRU family.\n",
    "\n",
    "Relative to Astra-γ and even Scribe-β, it introduces:\n",
    "\n",
    "* A **4-layer GRU stack**\n",
    "* High-resolution embeddings (384)\n",
    "* Substantial hidden dimensionality (768)\n",
    "* Increased dropout for stabilization\n",
    "\n",
    "This model maximizes recurrent depth, token-level abstraction, and long-range coherence—essential for producing realistic Shakespearean dialogue that spans multiple sentences or lines.\n",
    "\n",
    "The architecture includes:\n",
    "\n",
    "* Embedding layer\n",
    "* GRU Layer 1 → GRU Layer 2 → GRU Layer 3 → GRU Layer 4\n",
    "* Linear prediction head\n",
    "\n",
    "### **Hyperparameter Configuration**\n",
    "\n",
    "```\n",
    "embedding_dim = 384\n",
    "hidden_dim    = 768\n",
    "num_layers    = 4\n",
    "epochs        = 30\n",
    "learning_rate = 1e-3\n",
    "weight_decay  = 0.01\n",
    "optimizer     = AdamW\n",
    "scheduler     = CosineAnnealingLR\n",
    "dropout       = 0.15\n",
    "batch_size    = 64\n",
    "seq_length    = 128\n",
    "```\n",
    "\n",
    "### **Purpose and Expected Behavior**\n",
    "\n",
    "Scribe-γ is optimized for **high-fidelity subword generation**, offering:\n",
    "\n",
    "* Strong multi-sentence continuity\n",
    "* Accurate dialogue formatting\n",
    "* Rich stylistic imitation of Shakespeare’s syntax and rhythm\n",
    "* Far fewer nonsensical outputs compared to character-level models\n",
    "\n",
    "It stands as the flagship configuration for experiments in recurrent generative modeling.\n",
    "\n",
    "---\n",
    "\n",
    "# **Summary**\n",
    "\n",
    "The Scribe-GRU series—**Scribe-α**, **Scribe-β**, and **Scribe-γ**—reflects a progression in both **architectural depth** and **semantic richness** enabled by subword-level tokenization.\n",
    "\n",
    "Here is a **single unified comparison table** that merges **architecture**, **capacity**, and **parameter statistics** for the entire **Scribe-GRU Series**.\n",
    "\n",
    "Perfect for documentation, reports, or GitHub READMEs.\n",
    "\n",
    "---\n",
    "\n",
    "# **Scribe-GRU Model Comparison Table**\n",
    "\n",
    "| Model        | Layers | Hidden | Embedding | Parameters | Size (MB) |  Params (M) | Expected Behavior                                     |\n",
    "| ------------ | ------ | ------ | --------- | ---------: | --------: | ----------: | ----------------------------------------------------- |\n",
    "| **Scribe-α** | 2      | 256    | 128       |  1,075,688 |   4.10 MB |  **1.08 M** | Baseline subword LM; learns local & midrange patterns |\n",
    "| **Scribe-β** | 3      | 512    | 256       |  5,102,056 |  19.46 MB |  **5.10 M** | Strong coherence and phrase-level structure           |\n",
    "| **Scribe-γ** | 4      | 768    | 384       | 14,439,400 |  55.08 MB | **14.44 M** | High-quality fluent generation; best global structure |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3aef184",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = sp.get_piece_size()   # Subword vocabulary size from SentencePiece\n",
    "\n",
    "\n",
    "# ------------------------------------------------------- Scribe-α -------------------------------------------------------\n",
    "small_model_name = \"Scribe-α\"\n",
    "model_small = MyGRUModel(\n",
    "    vocab_size = vocab_size,\n",
    "    embd_dim = 128,          # Larger than Astra to match subword semantics\n",
    "    hidden_dim = 256,\n",
    "    num_layers = 2,\n",
    "    model_name = small_model_name,\n",
    "    dropout = 0.1\n",
    ").to(device)\n",
    "\n",
    "lr_small_model = 2e-3\n",
    "epochs_small_model = 30\n",
    "weight_decay_small = 0.01\n",
    "\n",
    "optimizer_small = torch.optim.AdamW(\n",
    "    model_small.parameters(),\n",
    "    lr = lr_small_model,\n",
    "    weight_decay = weight_decay_small\n",
    ")\n",
    "\n",
    "scheduler_small = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_small,\n",
    "    T_max = epochs_small_model\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------- Scribe-β -------------------------------------------------------\n",
    "medium_model_name = \"Scribe-β\"\n",
    "model_medium = MyGRUModel(\n",
    "    vocab_size = vocab_size,\n",
    "    embd_dim = 256,\n",
    "    hidden_dim = 512,\n",
    "    num_layers = 3,          # deeper than fevious Astra-β\n",
    "    model_name = medium_model_name,\n",
    "    dropout = 0.1\n",
    ").to(device)\n",
    "\n",
    "lr_medium_model = 1.5e-3\n",
    "epochs_medium_model = 35\n",
    "weight_decay_medium = 0.01\n",
    "\n",
    "optimizer_medium = torch.optim.AdamW(\n",
    "    model_medium.parameters(),\n",
    "    lr = lr_medium_model,\n",
    "    weight_decay = weight_decay_medium\n",
    ")\n",
    "\n",
    "scheduler_medium = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_medium,\n",
    "    T_max = epochs_medium_model\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------- Scribe-γ -------------------------------------------------------\n",
    "large_model_name = \"Scribe-γ\"\n",
    "model_large = MyGRUModel(\n",
    "    vocab_size = vocab_size,\n",
    "    embd_dim = 384,          # very high-quality embeddings\n",
    "    hidden_dim = 768,        # large recurrent representation\n",
    "    num_layers = 4,          # deeper than Astra-γ\n",
    "    model_name = large_model_name,\n",
    "    dropout = 0.15           # slightly more dropout for stability\n",
    ").to(device)\n",
    "\n",
    "lr_large_model = 1e-3\n",
    "epochs_large_model = 50\n",
    "weight_decay_large = 0.01\n",
    "\n",
    "optimizer_large = torch.optim.AdamW(\n",
    "    model_large.parameters(),\n",
    "    lr = lr_large_model,\n",
    "    weight_decay = weight_decay_large\n",
    ")\n",
    "\n",
    "# CosineAnnealingLR scheduler: It will produce cleaner convergence and noticeably better text quality.\n",
    "scheduler_large = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_large,\n",
    "    T_max = epochs_large_model\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924d931",
   "metadata": {},
   "source": [
    "### Scribe-α Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "657b48ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ASTRA-GRU MODEL SUMMARY\n",
      "====================================================================================================\n",
      "Model Name       : Scribe-α\n",
      "Device           : cuda\n",
      "Total Epochs     : 30\n",
      "Learning Rate    : 0.002\n",
      "\n",
      "MODEL ARCHITECTURE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  └── embedding: Embedding()\n",
      "  └── layers: ModuleList()\n",
      "  └── layers.0: GRULayer()\n",
      "  └── layers.0.grucell: GRUCell()\n",
      "  └── layers.0.grucell.Wx: Linear()\n",
      "  └── layers.0.grucell.Wh: Linear()\n",
      "  └── layers.0.grucell.Wzx: Linear()\n",
      "  └── layers.0.grucell.Wzh: Linear()\n",
      "  └── layers.0.grucell.Wrx: Linear()\n",
      "  └── layers.0.grucell.Wrh: Linear()\n",
      "  └── layers.0.dropout: Dropout()\n",
      "  └── layers.1: GRULayer()\n",
      "  └── layers.1.grucell: GRUCell()\n",
      "  └── layers.1.grucell.Wx: Linear()\n",
      "  └── layers.1.grucell.Wh: Linear()\n",
      "  └── layers.1.grucell.Wzx: Linear()\n",
      "  └── layers.1.grucell.Wzh: Linear()\n",
      "  └── layers.1.grucell.Wrx: Linear()\n",
      "  └── layers.1.grucell.Wrh: Linear()\n",
      "  └── layers.1.dropout: Dropout()\n",
      "  └── fc: Linear()\n",
      "  └── fc.linear_projection: Linear()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trainable Parameters : 1,075,688\n",
      "Model Size : 4.10 MB\n",
      "\n",
      "PARAMETER BREAKDOWN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "embedding.weight                         : 128,000\n",
      "layers.0.grucell.bias_z                  : 256\n",
      "layers.0.grucell.bias_r                  : 256\n",
      "layers.0.grucell.Wx.weight               : 32,768\n",
      "layers.0.grucell.Wx.bias                 : 256\n",
      "layers.0.grucell.Wh.weight               : 65,536\n",
      "layers.0.grucell.Wzx.weight              : 32,768\n",
      "layers.0.grucell.Wzx.bias                : 256\n",
      "layers.0.grucell.Wzh.weight              : 65,536\n",
      "layers.0.grucell.Wrx.weight              : 32,768\n",
      "layers.0.grucell.Wrx.bias                : 256\n",
      "layers.0.grucell.Wrh.weight              : 65,536\n",
      "layers.1.grucell.bias_z                  : 256\n",
      "layers.1.grucell.bias_r                  : 256\n",
      "layers.1.grucell.Wx.weight               : 65,536\n",
      "layers.1.grucell.Wx.bias                 : 256\n",
      "layers.1.grucell.Wh.weight               : 65,536\n",
      "layers.1.grucell.Wzx.weight              : 65,536\n",
      "layers.1.grucell.Wzx.bias                : 256\n",
      "layers.1.grucell.Wzh.weight              : 65,536\n",
      "layers.1.grucell.Wrx.weight              : 65,536\n",
      "layers.1.grucell.Wrx.bias                : 256\n",
      "layers.1.grucell.Wrh.weight              : 65,536\n",
      "fc.linear_projection.weight              : 256,000\n",
      "fc.linear_projection.bias                : 1,000\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_model_summary(\n",
    "    model      = model_small,\n",
    "    model_name = small_model_name,\n",
    "    epochs     = epochs_small_model,\n",
    "    lr         = lr_small_model,\n",
    "    device     = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695b2ef",
   "metadata": {},
   "source": [
    "### Scribe-α Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19fc7bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- Training Started for Scribe-α Model ----------------\n",
      "\n",
      "Epoch 01/30 | Train Loss: 4.4040 | Val Loss: 3.8884\n",
      ">>> Improved validation! Best checkpoint saved at epoch 1.\n",
      "Epoch 02/30 | Train Loss: 3.4838 | Val Loss: 3.9794\n",
      ">>> No improvement (1/5).\n",
      "Epoch 03/30 | Train Loss: 3.2784 | Val Loss: 4.0091\n",
      ">>> No improvement (2/5).\n",
      "Epoch 04/30 | Train Loss: 3.1669 | Val Loss: 3.8738\n",
      ">>> Improved validation! Best checkpoint saved at epoch 4.\n",
      "Epoch 05/30 | Train Loss: 3.0916 | Val Loss: 3.9977\n",
      ">>> No improvement (1/5).\n",
      "Epoch 06/30 | Train Loss: 3.0313 | Val Loss: 4.1396\n",
      ">>> No improvement (2/5).\n",
      "Epoch 07/30 | Train Loss: 2.9861 | Val Loss: 3.9533\n",
      ">>> No improvement (3/5).\n",
      "Epoch 08/30 | Train Loss: 2.9525 | Val Loss: 3.9693\n",
      ">>> No improvement (4/5).\n",
      "Epoch 09/30 | Train Loss: 2.9200 | Val Loss: 4.1530\n",
      ">>> No improvement (5/5).\n",
      "\n",
      "================ EARLY STOPPING ACTIVATED ================\n",
      "Training stopped at epoch 9. Best epoch: 4 | Best Val Loss: 3.8738\n",
      "==========================================================\n",
      "\n",
      "Loaded best checkpoint: ./best_checkpoints/Scribe-α_best.pth\n",
      "\n",
      "---------------- Training Completed for Scribe-α Model ----------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_9888\\840331499.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "model_small = train_model_with_early_stopping(\n",
    "    model       = model_small,\n",
    "    optimizer   = optimizer_small,\n",
    "    scheduler   = scheduler_small,\n",
    "    loss_fn     = loss_fn,\n",
    "    epochs      = epochs_small_model,\n",
    "    batch_size  = 64,\n",
    "    device      = device,\n",
    "    steps       = 300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e2ca7",
   "metadata": {},
   "source": [
    "### Scribe-α Autoregressive Generation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d2fa44",
   "metadata": {},
   "source": [
    "#### Greedy Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12b8bd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: I amends, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the sounds, and the s\n"
     ]
    }
   ],
   "source": [
    "print(sample_greedy(model_small, sp, \"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603829a9",
   "metadata": {},
   "source": [
    "#### Temperature Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcb5d267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING: I am all o' to my good nates: Why, Thou fetervelave by some score as this cuten by the proud to make me of his law? Againmity grant! First Lord Lady, And perfull husband The cute'day, Fratertain, letter's Mine in your honour and by my father, King killed with carest thou dire the truthor on my design, go To her! He was pardon, Were to keep with thee here Wereason'dracurs, 'Twake yet body'sclet'ships, if thou hastevellow! I age That deser, and young Prong grong. Bafere proud witness to the dour oft's, I'dd by my mother\n"
     ]
    }
   ],
   "source": [
    "print(sample_with_temperature(model_small, sp, \"KING: \", temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be352441",
   "metadata": {},
   "source": [
    "#### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2c79cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: For the noble sorrow to dogive requides! I ambert, And thou cook to have I was the tounce, I to my father; But to the other velace it is's: From either: All shrustenessenncusiness, 'tis that belay the stropass of my pity: Humplace and a sound in this present. Thinkering of that I amongs; that mague to become gets, if his hand, but a drungthight: Hapect's; and all my mises, Signors have it cut. Owatory as if this wickly dark, in all your heart. KING EDWARD: For the sorrow to this puss hell, and in the restrain, if that will be not be\n"
     ]
    }
   ],
   "source": [
    "print(sample_top_k(model_small, sp, \"FIRST CITIZEN: \", k=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb8ecde",
   "metadata": {},
   "source": [
    "### Scribe-α Model Checkpoint Saving and Archival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba962480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at: ./scribe_saved_models\\Scribe_alpha_v1.pth\n",
      "Also updated: Scribe_alpha_latest.pth\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./scribe_saved_models\\\\Scribe_alpha_v1.pth'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(\n",
    "    model      = model_small,\n",
    "    base_name  = \"Scribe_alpha\",\n",
    "    path       = \"./scribe_saved_models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7839cbce",
   "metadata": {},
   "source": [
    "### Scribe-β Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7587d7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ASTRA-GRU MODEL SUMMARY\n",
      "====================================================================================================\n",
      "Model Name       : Scribe-β\n",
      "Device           : cuda\n",
      "Total Epochs     : 35\n",
      "Learning Rate    : 0.0015\n",
      "\n",
      "MODEL ARCHITECTURE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  └── embedding: Embedding()\n",
      "  └── layers: ModuleList()\n",
      "  └── layers.0: GRULayer()\n",
      "  └── layers.0.grucell: GRUCell()\n",
      "  └── layers.0.grucell.Wx: Linear()\n",
      "  └── layers.0.grucell.Wh: Linear()\n",
      "  └── layers.0.grucell.Wzx: Linear()\n",
      "  └── layers.0.grucell.Wzh: Linear()\n",
      "  └── layers.0.grucell.Wrx: Linear()\n",
      "  └── layers.0.grucell.Wrh: Linear()\n",
      "  └── layers.0.dropout: Dropout()\n",
      "  └── layers.1: GRULayer()\n",
      "  └── layers.1.grucell: GRUCell()\n",
      "  └── layers.1.grucell.Wx: Linear()\n",
      "  └── layers.1.grucell.Wh: Linear()\n",
      "  └── layers.1.grucell.Wzx: Linear()\n",
      "  └── layers.1.grucell.Wzh: Linear()\n",
      "  └── layers.1.grucell.Wrx: Linear()\n",
      "  └── layers.1.grucell.Wrh: Linear()\n",
      "  └── layers.1.dropout: Dropout()\n",
      "  └── layers.2: GRULayer()\n",
      "  └── layers.2.grucell: GRUCell()\n",
      "  └── layers.2.grucell.Wx: Linear()\n",
      "  └── layers.2.grucell.Wh: Linear()\n",
      "  └── layers.2.grucell.Wzx: Linear()\n",
      "  └── layers.2.grucell.Wzh: Linear()\n",
      "  └── layers.2.grucell.Wrx: Linear()\n",
      "  └── layers.2.grucell.Wrh: Linear()\n",
      "  └── layers.2.dropout: Dropout()\n",
      "  └── fc: Linear()\n",
      "  └── fc.linear_projection: Linear()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trainable Parameters : 5,102,056\n",
      "Model Size : 19.46 MB\n",
      "\n",
      "PARAMETER BREAKDOWN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "embedding.weight                         : 256,000\n",
      "layers.0.grucell.bias_z                  : 512\n",
      "layers.0.grucell.bias_r                  : 512\n",
      "layers.0.grucell.Wx.weight               : 131,072\n",
      "layers.0.grucell.Wx.bias                 : 512\n",
      "layers.0.grucell.Wh.weight               : 262,144\n",
      "layers.0.grucell.Wzx.weight              : 131,072\n",
      "layers.0.grucell.Wzx.bias                : 512\n",
      "layers.0.grucell.Wzh.weight              : 262,144\n",
      "layers.0.grucell.Wrx.weight              : 131,072\n",
      "layers.0.grucell.Wrx.bias                : 512\n",
      "layers.0.grucell.Wrh.weight              : 262,144\n",
      "layers.1.grucell.bias_z                  : 512\n",
      "layers.1.grucell.bias_r                  : 512\n",
      "layers.1.grucell.Wx.weight               : 262,144\n",
      "layers.1.grucell.Wx.bias                 : 512\n",
      "layers.1.grucell.Wh.weight               : 262,144\n",
      "layers.1.grucell.Wzx.weight              : 262,144\n",
      "layers.1.grucell.Wzx.bias                : 512\n",
      "layers.1.grucell.Wzh.weight              : 262,144\n",
      "layers.1.grucell.Wrx.weight              : 262,144\n",
      "layers.1.grucell.Wrx.bias                : 512\n",
      "layers.1.grucell.Wrh.weight              : 262,144\n",
      "layers.2.grucell.bias_z                  : 512\n",
      "layers.2.grucell.bias_r                  : 512\n",
      "layers.2.grucell.Wx.weight               : 262,144\n",
      "layers.2.grucell.Wx.bias                 : 512\n",
      "layers.2.grucell.Wh.weight               : 262,144\n",
      "layers.2.grucell.Wzx.weight              : 262,144\n",
      "layers.2.grucell.Wzx.bias                : 512\n",
      "layers.2.grucell.Wzh.weight              : 262,144\n",
      "layers.2.grucell.Wrx.weight              : 262,144\n",
      "layers.2.grucell.Wrx.bias                : 512\n",
      "layers.2.grucell.Wrh.weight              : 262,144\n",
      "fc.linear_projection.weight              : 512,000\n",
      "fc.linear_projection.bias                : 1,000\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_model_summary(\n",
    "    model      = model_medium,\n",
    "    model_name = medium_model_name,\n",
    "    epochs     = epochs_medium_model,\n",
    "    lr         = lr_medium_model,\n",
    "    device     = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869bc0e0",
   "metadata": {},
   "source": [
    "### Scribe-β Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df3c393d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- Training Started for Scribe-β Model ----------------\n",
      "\n",
      "Epoch 01/35 | Train Loss: 4.1447 | Val Loss: 3.8422\n",
      ">>> Improved validation! Best checkpoint saved at epoch 1.\n",
      "Epoch 02/35 | Train Loss: 2.9913 | Val Loss: 4.0418\n",
      ">>> No improvement (1/5).\n",
      "Epoch 03/35 | Train Loss: 2.6152 | Val Loss: 4.1717\n",
      ">>> No improvement (2/5).\n",
      "Epoch 04/35 | Train Loss: 2.3918 | Val Loss: 4.4219\n",
      ">>> No improvement (3/5).\n",
      "Epoch 05/35 | Train Loss: 2.2441 | Val Loss: 4.6182\n",
      ">>> No improvement (4/5).\n",
      "Epoch 06/35 | Train Loss: 2.1431 | Val Loss: 4.7304\n",
      ">>> No improvement (5/5).\n",
      "\n",
      "================ EARLY STOPPING ACTIVATED ================\n",
      "Training stopped at epoch 6. Best epoch: 1 | Best Val Loss: 3.8422\n",
      "==========================================================\n",
      "\n",
      "Loaded best checkpoint: ./best_checkpoints/Scribe-β_best.pth\n",
      "\n",
      "---------------- Training Completed for Scribe-β Model ----------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_9888\\840331499.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "model_medium = train_model_with_early_stopping(\n",
    "    model       = model_medium,\n",
    "    optimizer   = optimizer_medium,\n",
    "    scheduler   = scheduler_medium,\n",
    "    loss_fn     = loss_fn,\n",
    "    epochs      = epochs_medium_model,\n",
    "    batch_size  = 64,\n",
    "    device      = device,\n",
    "    steps       = 350\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea64c8",
   "metadata": {},
   "source": [
    "### Scribe-β Autoregressive Generation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09453bc",
   "metadata": {},
   "source": [
    "#### Greedy Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ea429be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: I amongs, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse, and the curse,\n"
     ]
    }
   ],
   "source": [
    "print(sample_greedy(model_medium, sp, \"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce29fae",
   "metadata": {},
   "source": [
    "#### Temperature Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c74042e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING: I'ded call me, itself, But mished, this, he is my words. From words to the goes. CL VI Alooks, for The wisall good Pety readeed me, Ours those that lies, and at all the time, nor a keeple look the arm on the courtild lies. LORDIUS oppy to entire that bos. Thou living such authure so. Owise, that bid thee more chold farry, and haw foul yever-tent Ebriefth and I prayer to make some chretchom, I say doth that bushipest best, said The charge hath he that thou a vie sleeper Gruck'st most lost thou talk's. QUEENARD Of no more to be your ey\n"
     ]
    }
   ],
   "source": [
    "print(sample_with_temperature(model_medium, sp, \"KING: \", temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028ddd2",
   "metadata": {},
   "source": [
    "#### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "963f0c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: And fit upon mineself! Pars: Sh, Accord? Bolent a tastly, by your love a little mouden, I'd: No server, and, And am in pass, I think; Bairt of the gen, and the acus in him with thee; and good mercy, Scon in the way: Afort and my lady'd-held: 'twed, which I did fully, and toget. Ad! What say, that, and post, which they are towards A carch in her servford, that tale are the world, Would with him. Londily sen; But in a man, by mine: For such grant that you's Suitation, and goods: He that heard. Cancons that will it\n"
     ]
    }
   ],
   "source": [
    "print(sample_top_k(model_medium, sp, \"FIRST CITIZEN: \", k=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54da2aa",
   "metadata": {},
   "source": [
    "### Scribe-β Model Checkpoint Saving and Archival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a7bbe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at: ./scribe_saved_models\\Scribe_beta_v1.pth\n",
      "Also updated: Scribe_beta_latest.pth\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./scribe_saved_models\\\\Scribe_beta_v1.pth'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(\n",
    "    model      = model_medium,\n",
    "    base_name  = \"Scribe_beta\",\n",
    "    path       = \"./scribe_saved_models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6522c5",
   "metadata": {},
   "source": [
    "### Scribe-γ Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c1b2ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ASTRA-GRU MODEL SUMMARY\n",
      "====================================================================================================\n",
      "Model Name       : Scribe-γ\n",
      "Device           : cuda\n",
      "Total Epochs     : 50\n",
      "Learning Rate    : 0.001\n",
      "\n",
      "MODEL ARCHITECTURE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  └── embedding: Embedding()\n",
      "  └── layers: ModuleList()\n",
      "  └── layers.0: GRULayer()\n",
      "  └── layers.0.grucell: GRUCell()\n",
      "  └── layers.0.grucell.Wx: Linear()\n",
      "  └── layers.0.grucell.Wh: Linear()\n",
      "  └── layers.0.grucell.Wzx: Linear()\n",
      "  └── layers.0.grucell.Wzh: Linear()\n",
      "  └── layers.0.grucell.Wrx: Linear()\n",
      "  └── layers.0.grucell.Wrh: Linear()\n",
      "  └── layers.0.dropout: Dropout()\n",
      "  └── layers.1: GRULayer()\n",
      "  └── layers.1.grucell: GRUCell()\n",
      "  └── layers.1.grucell.Wx: Linear()\n",
      "  └── layers.1.grucell.Wh: Linear()\n",
      "  └── layers.1.grucell.Wzx: Linear()\n",
      "  └── layers.1.grucell.Wzh: Linear()\n",
      "  └── layers.1.grucell.Wrx: Linear()\n",
      "  └── layers.1.grucell.Wrh: Linear()\n",
      "  └── layers.1.dropout: Dropout()\n",
      "  └── layers.2: GRULayer()\n",
      "  └── layers.2.grucell: GRUCell()\n",
      "  └── layers.2.grucell.Wx: Linear()\n",
      "  └── layers.2.grucell.Wh: Linear()\n",
      "  └── layers.2.grucell.Wzx: Linear()\n",
      "  └── layers.2.grucell.Wzh: Linear()\n",
      "  └── layers.2.grucell.Wrx: Linear()\n",
      "  └── layers.2.grucell.Wrh: Linear()\n",
      "  └── layers.2.dropout: Dropout()\n",
      "  └── layers.3: GRULayer()\n",
      "  └── layers.3.grucell: GRUCell()\n",
      "  └── layers.3.grucell.Wx: Linear()\n",
      "  └── layers.3.grucell.Wh: Linear()\n",
      "  └── layers.3.grucell.Wzx: Linear()\n",
      "  └── layers.3.grucell.Wzh: Linear()\n",
      "  └── layers.3.grucell.Wrx: Linear()\n",
      "  └── layers.3.grucell.Wrh: Linear()\n",
      "  └── layers.3.dropout: Dropout()\n",
      "  └── fc: Linear()\n",
      "  └── fc.linear_projection: Linear()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trainable Parameters : 14,439,400\n",
      "Model Size : 55.08 MB\n",
      "\n",
      "PARAMETER BREAKDOWN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "embedding.weight                         : 384,000\n",
      "layers.0.grucell.bias_z                  : 768\n",
      "layers.0.grucell.bias_r                  : 768\n",
      "layers.0.grucell.Wx.weight               : 294,912\n",
      "layers.0.grucell.Wx.bias                 : 768\n",
      "layers.0.grucell.Wh.weight               : 589,824\n",
      "layers.0.grucell.Wzx.weight              : 294,912\n",
      "layers.0.grucell.Wzx.bias                : 768\n",
      "layers.0.grucell.Wzh.weight              : 589,824\n",
      "layers.0.grucell.Wrx.weight              : 294,912\n",
      "layers.0.grucell.Wrx.bias                : 768\n",
      "layers.0.grucell.Wrh.weight              : 589,824\n",
      "layers.1.grucell.bias_z                  : 768\n",
      "layers.1.grucell.bias_r                  : 768\n",
      "layers.1.grucell.Wx.weight               : 589,824\n",
      "layers.1.grucell.Wx.bias                 : 768\n",
      "layers.1.grucell.Wh.weight               : 589,824\n",
      "layers.1.grucell.Wzx.weight              : 589,824\n",
      "layers.1.grucell.Wzx.bias                : 768\n",
      "layers.1.grucell.Wzh.weight              : 589,824\n",
      "layers.1.grucell.Wrx.weight              : 589,824\n",
      "layers.1.grucell.Wrx.bias                : 768\n",
      "layers.1.grucell.Wrh.weight              : 589,824\n",
      "layers.2.grucell.bias_z                  : 768\n",
      "layers.2.grucell.bias_r                  : 768\n",
      "layers.2.grucell.Wx.weight               : 589,824\n",
      "layers.2.grucell.Wx.bias                 : 768\n",
      "layers.2.grucell.Wh.weight               : 589,824\n",
      "layers.2.grucell.Wzx.weight              : 589,824\n",
      "layers.2.grucell.Wzx.bias                : 768\n",
      "layers.2.grucell.Wzh.weight              : 589,824\n",
      "layers.2.grucell.Wrx.weight              : 589,824\n",
      "layers.2.grucell.Wrx.bias                : 768\n",
      "layers.2.grucell.Wrh.weight              : 589,824\n",
      "layers.3.grucell.bias_z                  : 768\n",
      "layers.3.grucell.bias_r                  : 768\n",
      "layers.3.grucell.Wx.weight               : 589,824\n",
      "layers.3.grucell.Wx.bias                 : 768\n",
      "layers.3.grucell.Wh.weight               : 589,824\n",
      "layers.3.grucell.Wzx.weight              : 589,824\n",
      "layers.3.grucell.Wzx.bias                : 768\n",
      "layers.3.grucell.Wzh.weight              : 589,824\n",
      "layers.3.grucell.Wrx.weight              : 589,824\n",
      "layers.3.grucell.Wrx.bias                : 768\n",
      "layers.3.grucell.Wrh.weight              : 589,824\n",
      "fc.linear_projection.weight              : 768,000\n",
      "fc.linear_projection.bias                : 1,000\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_model_summary(\n",
    "    model      = model_large,\n",
    "    model_name = large_model_name,\n",
    "    epochs     = epochs_large_model,\n",
    "    lr         = lr_large_model,\n",
    "    device     = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f161ca",
   "metadata": {},
   "source": [
    "### Scribe-γ Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b181f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- Training Started for Scribe-γ Model ----------------\n",
      "\n",
      "Epoch 01/50 | Train Loss: 5.1040 | Val Loss: 4.2732\n",
      ">>> Improved validation! Best checkpoint saved at epoch 1.\n",
      "Epoch 02/50 | Train Loss: 3.5860 | Val Loss: 4.0858\n",
      ">>> Improved validation! Best checkpoint saved at epoch 2.\n",
      "Epoch 03/50 | Train Loss: 3.1148 | Val Loss: 4.0519\n",
      ">>> Improved validation! Best checkpoint saved at epoch 3.\n",
      "Epoch 04/50 | Train Loss: 2.8456 | Val Loss: 4.1766\n",
      ">>> No improvement (1/5).\n",
      "Epoch 05/50 | Train Loss: 2.6498 | Val Loss: 4.3601\n",
      ">>> No improvement (2/5).\n",
      "Epoch 06/50 | Train Loss: 2.5036 | Val Loss: 4.3753\n",
      ">>> No improvement (3/5).\n",
      "Epoch 07/50 | Train Loss: 2.3918 | Val Loss: 4.5689\n",
      ">>> No improvement (4/5).\n",
      "Epoch 08/50 | Train Loss: 2.3005 | Val Loss: 4.4060\n",
      ">>> No improvement (5/5).\n",
      "\n",
      "================ EARLY STOPPING ACTIVATED ================\n",
      "Training stopped at epoch 8. Best epoch: 3 | Best Val Loss: 4.0519\n",
      "==========================================================\n",
      "\n",
      "Loaded best checkpoint: ./best_checkpoints/Scribe-γ_best.pth\n",
      "\n",
      "---------------- Training Completed for Scribe-γ Model ----------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_9888\\840331499.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "model_large = train_model_with_early_stopping(\n",
    "    model       = model_large,\n",
    "    optimizer   = optimizer_large,\n",
    "    scheduler   = scheduler_large,\n",
    "    loss_fn     = loss_fn,\n",
    "    epochs      = epochs_large_model,\n",
    "    batch_size  = 64,\n",
    "    device      = device,\n",
    "    steps       = 450     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b055c5",
   "metadata": {},
   "source": [
    "### Scribe-γ Autoregressive Generation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e521587",
   "metadata": {},
   "source": [
    "#### Greedy Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "039100c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: I have done, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king, and the king,\n"
     ]
    }
   ],
   "source": [
    "print(sample_greedy(model_large, sp, \"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f894b6",
   "metadata": {},
   "source": [
    "#### Temperature Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc16c9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING: Heranty, put on him to speak with change-s, I ve I shall live of this worse king, I, and that are alimes, I should be begin your lords, your stand in love, Glted for those that for a word. Ty offallcused be! Ty, much like. Procell proceed worse he knife? ISABELLA: the Take the wivoletrunant with all the duke? Second Is not know meeter with herself; a charge, bravelack, Uning he lod, Where, sir, are like lible ind; trust the kingding sent. LEONTES: I had not mets, and be as I have crawife, as the guest, yeased? Say sight, and saw he was enter as the deedy\n"
     ]
    }
   ],
   "source": [
    "print(sample_with_temperature(model_large, sp, \"KING: \", temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17eb033",
   "metadata": {},
   "source": [
    "#### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "848a2f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: I am, sir, I did all of my heads are all no drawns become, in his death and by my soul with awed, nor, sir, sir. KING EDWARD IV: PARENCE: What shall beaving w: 'tis you shall meeter and your pers: May. ISABELLA: No draw I did not. First More the wo'd, ins, to the cause, as a tn you do you thanks. MARCIA: Most in the goes, and, forward; then me to since thou, for this ords od, not: And then but one as the duke; But to me to dish; and wret, with the moud and a good and solding: Let me, and a draworch all the house, if he hath done, as I come. GLOUCESTER: '\n"
     ]
    }
   ],
   "source": [
    "print(sample_top_k(model_large, sp, \"FIRST CITIZEN: \", k=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df492c6",
   "metadata": {},
   "source": [
    "### Scribe-γ Model Checkpoint Saving and Archival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "300fcca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at: ./scribe_saved_models\\Scribe_gamma_v1.pth\n",
      "Also updated: Scribe_gamma_latest.pth\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./scribe_saved_models\\\\Scribe_gamma_v1.pth'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(\n",
    "    model      = model_large,\n",
    "    base_name  = \"Scribe_gamma\",\n",
    "    path       = \"./scribe_saved_models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206273e0",
   "metadata": {},
   "source": [
    "## Loading the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "914ddab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ MODEL LOADED ================\n",
      "Loaded File      : ./scribe_saved_models/Scribe_alpha_latest.pth\n",
      "Model Name       : Scribe-α\n",
      "Model Class      : MyGRUModel\n",
      "Version          : v1\n",
      "Timestamp        : 2025-12-12 19:59:22\n",
      "----------------------------------------------\n",
      "Model Architecture:\n",
      "  └── embedding: Embedding\n",
      "  └── layers: ModuleList\n",
      "  └── layers.0: GRULayer\n",
      "  └── layers.0.grucell: GRUCell\n",
      "  └── layers.0.grucell.Wx: Linear\n",
      "  └── layers.0.grucell.Wh: Linear\n",
      "  └── layers.0.grucell.Wzx: Linear\n",
      "  └── layers.0.grucell.Wzh: Linear\n",
      "  └── layers.0.grucell.Wrx: Linear\n",
      "  └── layers.0.grucell.Wrh: Linear\n",
      "  └── layers.0.dropout: Dropout\n",
      "  └── layers.1: GRULayer\n",
      "  └── layers.1.grucell: GRUCell\n",
      "  └── layers.1.grucell.Wx: Linear\n",
      "  └── layers.1.grucell.Wh: Linear\n",
      "  └── layers.1.grucell.Wzx: Linear\n",
      "  └── layers.1.grucell.Wzh: Linear\n",
      "  └── layers.1.grucell.Wrx: Linear\n",
      "  └── layers.1.grucell.Wrh: Linear\n",
      "  └── layers.1.dropout: Dropout\n",
      "  └── fc: Linear\n",
      "  └── fc.linear_projection: Linear\n",
      "----------------------------------------------\n",
      "Total Parameters : 1,075,688\n",
      "Loaded on Device : cuda\n",
      "==============================================\n",
      "\n",
      "\n",
      "================ MODEL LOADED ================\n",
      "Loaded File      : ./scribe_saved_models/Scribe_beta_latest.pth\n",
      "Model Name       : Scribe-β\n",
      "Model Class      : MyGRUModel\n",
      "Version          : v1\n",
      "Timestamp        : 2025-12-12 20:25:12\n",
      "----------------------------------------------\n",
      "Model Architecture:\n",
      "  └── embedding: Embedding\n",
      "  └── layers: ModuleList\n",
      "  └── layers.0: GRULayer\n",
      "  └── layers.0.grucell: GRUCell\n",
      "  └── layers.0.grucell.Wx: Linear\n",
      "  └── layers.0.grucell.Wh: Linear\n",
      "  └── layers.0.grucell.Wzx: Linear\n",
      "  └── layers.0.grucell.Wzh: Linear\n",
      "  └── layers.0.grucell.Wrx: Linear\n",
      "  └── layers.0.grucell.Wrh: Linear\n",
      "  └── layers.0.dropout: Dropout\n",
      "  └── layers.1: GRULayer\n",
      "  └── layers.1.grucell: GRUCell\n",
      "  └── layers.1.grucell.Wx: Linear\n",
      "  └── layers.1.grucell.Wh: Linear\n",
      "  └── layers.1.grucell.Wzx: Linear\n",
      "  └── layers.1.grucell.Wzh: Linear\n",
      "  └── layers.1.grucell.Wrx: Linear\n",
      "  └── layers.1.grucell.Wrh: Linear\n",
      "  └── layers.1.dropout: Dropout\n",
      "  └── layers.2: GRULayer\n",
      "  └── layers.2.grucell: GRUCell\n",
      "  └── layers.2.grucell.Wx: Linear\n",
      "  └── layers.2.grucell.Wh: Linear\n",
      "  └── layers.2.grucell.Wzx: Linear\n",
      "  └── layers.2.grucell.Wzh: Linear\n",
      "  └── layers.2.grucell.Wrx: Linear\n",
      "  └── layers.2.grucell.Wrh: Linear\n",
      "  └── layers.2.dropout: Dropout\n",
      "  └── fc: Linear\n",
      "  └── fc.linear_projection: Linear\n",
      "----------------------------------------------\n",
      "Total Parameters : 5,102,056\n",
      "Loaded on Device : cuda\n",
      "==============================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_9888\\794920423.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ MODEL LOADED ================\n",
      "Loaded File      : ./scribe_saved_models/Scribe_gamma_latest.pth\n",
      "Model Name       : Scribe-γ\n",
      "Model Class      : MyGRUModel\n",
      "Version          : v1\n",
      "Timestamp        : 2025-12-12 21:40:24\n",
      "----------------------------------------------\n",
      "Model Architecture:\n",
      "  └── embedding: Embedding\n",
      "  └── layers: ModuleList\n",
      "  └── layers.0: GRULayer\n",
      "  └── layers.0.grucell: GRUCell\n",
      "  └── layers.0.grucell.Wx: Linear\n",
      "  └── layers.0.grucell.Wh: Linear\n",
      "  └── layers.0.grucell.Wzx: Linear\n",
      "  └── layers.0.grucell.Wzh: Linear\n",
      "  └── layers.0.grucell.Wrx: Linear\n",
      "  └── layers.0.grucell.Wrh: Linear\n",
      "  └── layers.0.dropout: Dropout\n",
      "  └── layers.1: GRULayer\n",
      "  └── layers.1.grucell: GRUCell\n",
      "  └── layers.1.grucell.Wx: Linear\n",
      "  └── layers.1.grucell.Wh: Linear\n",
      "  └── layers.1.grucell.Wzx: Linear\n",
      "  └── layers.1.grucell.Wzh: Linear\n",
      "  └── layers.1.grucell.Wrx: Linear\n",
      "  └── layers.1.grucell.Wrh: Linear\n",
      "  └── layers.1.dropout: Dropout\n",
      "  └── layers.2: GRULayer\n",
      "  └── layers.2.grucell: GRUCell\n",
      "  └── layers.2.grucell.Wx: Linear\n",
      "  └── layers.2.grucell.Wh: Linear\n",
      "  └── layers.2.grucell.Wzx: Linear\n",
      "  └── layers.2.grucell.Wzh: Linear\n",
      "  └── layers.2.grucell.Wrx: Linear\n",
      "  └── layers.2.grucell.Wrh: Linear\n",
      "  └── layers.2.dropout: Dropout\n",
      "  └── layers.3: GRULayer\n",
      "  └── layers.3.grucell: GRUCell\n",
      "  └── layers.3.grucell.Wx: Linear\n",
      "  └── layers.3.grucell.Wh: Linear\n",
      "  └── layers.3.grucell.Wzx: Linear\n",
      "  └── layers.3.grucell.Wzh: Linear\n",
      "  └── layers.3.grucell.Wrx: Linear\n",
      "  └── layers.3.grucell.Wrh: Linear\n",
      "  └── layers.3.dropout: Dropout\n",
      "  └── fc: Linear\n",
      "  └── fc.linear_projection: Linear\n",
      "----------------------------------------------\n",
      "Total Parameters : 14,439,400\n",
      "Loaded on Device : cuda\n",
      "==============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "scribe_alpha = load_model(\n",
    "    filepath = \"./scribe_saved_models/Scribe_alpha_latest.pth\",\n",
    "    device   = device\n",
    ")\n",
    "\n",
    "scribe_beta = load_model(\n",
    "    filepath = \"./scribe_saved_models/Scribe_beta_latest.pth\",\n",
    "    device   = device\n",
    ")\n",
    "\n",
    "scribe_gamma = load_model(\n",
    "    filepath = \"./scribe_saved_models/Scribe_gamma_latest.pth\",\n",
    "    device   = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8a275f",
   "metadata": {},
   "source": [
    "## Sampling from the Loaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b64ec3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: This day! KING RICHARD: But me nobling to this servically in a since the other and his sounds, That ever I amprought, the fits a came as infility, and fours I among me as these warrant, and the gain a wish and my talk: he doges all to dracious childre yally? Shallench! See-welievoke yours he's, if The faulted with answer? I amend! I am nothing; and ages, And in Volkener, oratharcherthight with our streneking. But this inde; and deeperved by the doung. Second Muspoint, my loath of love as well, forges, or your grace apl\n"
     ]
    }
   ],
   "source": [
    "print(sample_top_k(scribe_alpha, sp, \"FIRST CITIZEN: \", k=30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75efba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: Ay, and clectireses your broke more than any gain of me soon: Ourne's. Camio. KING KING HENRY Bause I speaks of us with the cause match! ABRFIS MARols of a prom starnates, I have not me for you not. BINGBR QUEEN ELIZABETH sadeepest thyself that tites a misters; But to me; I will take, or recept, which I'der of your groadd; For a gible with my lord, to hear our a pack on my good a lander's on herself on thee: If we's! Ohood shall become, ass yours. What's! Back's, or else are yourselves not yourself to watch from me. CLAUY\n"
     ]
    }
   ],
   "source": [
    "print(sample_top_k(scribe_beta, sp, \"FIRST CITIZEN: \", k=30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68789cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CITIZEN: 'KELO: CLIENGARIVOLIONRIVOLINGR: Come, that hath reason, but your voties he' quits! JULIET: I do, As thou hastapect me of his maid it soft, I must be thy hand, and in him in my bloody. DUKE VINCENTIO: The sch'd, he is a manneal on a wised from her and I have weather'll, and law, you for the queen, not. LUCIO: Nay. Sor: Hell'ssiling mood: O: I shall you fired to beler as he is dam? YORK: If ever fier; I doing you know, who hath less. DUKELO; And barch, That hell, in my love-guced, and with the mount to prate.\n"
     ]
    }
   ],
   "source": [
    "print(sample_top_k(scribe_gamma, sp, \"FIRST CITIZEN: \", k=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf7e195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f8ce3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
