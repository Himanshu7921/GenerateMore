{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1018dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab5ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I'll implement the self-attention block then i'll implement the Multi-head-attention as a wrapper on top of a self-attention\n",
    "# I'll use B = batch_dim, d_model = number of embedding dimensions, T = input sequence length, d_k, d_v as dimensions of key and values matrix\n",
    "\n",
    "# Symbols\n",
    "# X --> Input embeddings = embeddings + positional embeddings --> (B, T, d_model)\n",
    "# W_Q --> Query projection --> (d_model, h * d_k) [assuming h number of heads]\n",
    "# W_K --> Key projection --> (d_model, h * d_k) [assuming h number of heads]\n",
    "# W_V --> Value projection --> (d_model, h * d_v) [assuming h number of heads]\n",
    "# Q, K, V --> (B, h, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b877d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummySelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, debugger: bool = False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.debugger = debugger\n",
    "\n",
    "        self.W_Q = torch.nn.Linear(d_model, d_k, bias = False)\n",
    "        self.W_K = torch.nn.Linear(d_model, d_k, bias = False)\n",
    "        self.W_V = torch.nn.Linear(d_model, d_v, bias = False)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor):\n",
    "        B, T, d_model = X.shape\n",
    "        assert self.d_model == d_model, \"self.d_model must be equal to d_model from x.shape\"\n",
    "        # X.shape = (B, T, d_model)\n",
    "        Q = self.W_Q(X) # Q.shape = (B, T, d_k)\n",
    "        K = self.W_K(X) # K.shape = (B, T, d_k)\n",
    "        V = self.W_V(X) # V.shape = (B, T, d_v)\n",
    "        S = (Q @ K.transpose(-2, -1)) / (self.d_k ** 0.5) # S.shape = (B, T, T)\n",
    "        A = torch.softmax(S, dim = -1)\n",
    "        Z = A @ V # Z.shape = (B, T, d_v)\n",
    "        if self.debugger:\n",
    "            print(f\"X.shape  = (B, T, d_model) = {X.shape}\")\n",
    "            print(f\"B = {B}\")\n",
    "            print(f\"T = {T}\")\n",
    "            print(f\"d_model = {d_model}\")\n",
    "            print(\"-\" * 100)\n",
    "            print(f\"Q.shape = {Q.shape}\")\n",
    "            print(f\"K.shape = {K.shape}\")\n",
    "            print(f\"V.shape = {V.shape}\")\n",
    "            print(f\"S = Q @ K.T\")\n",
    "            print(f\"A = softmax(S) | Attention must normalize over keys.\")\n",
    "            print(f\"Z = A @ V\")\n",
    "            print(f\"so, Z = (softmax(Q @ K.T)/d_k ** 0.5) @ V\")\n",
    "            print(A.sum(dim = -1))\n",
    "            print(f\"S.shape = {S.shape}\")\n",
    "            print(f\"A.shape = {A.shape}\")\n",
    "            print(f\"Z.shape = {Z.shape}\")\n",
    "        \n",
    "        return Z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3741bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape  = (B, T, d_model) = torch.Size([1, 20, 4])\n",
      "B = 1\n",
      "T = 20\n",
      "d_model = 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Q.shape = torch.Size([1, 20, 3])\n",
      "K.shape = torch.Size([1, 20, 3])\n",
      "V.shape = torch.Size([1, 20, 5])\n",
      "S = Q @ K.T\n",
      "A = softmax(S) | Attention must normalize over keys.\n",
      "Z = A @ V\n",
      "so, Z = (softmax(Q @ K.T)/d_k ** 0.5) @ V\n",
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "S.shape = torch.Size([1, 20, 20])\n",
      "A.shape = torch.Size([1, 20, 20])\n",
      "Z.shape = torch.Size([1, 20, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3819,  0.0240,  0.1881,  0.4650, -0.6376],\n",
       "         [ 0.3804,  0.0230,  0.1929,  0.4665, -0.6356],\n",
       "         [ 0.3816,  0.0251,  0.1833,  0.4621, -0.6370],\n",
       "         [ 0.3817,  0.0222,  0.1955,  0.4691, -0.6373],\n",
       "         [ 0.3826,  0.0232,  0.1898,  0.4669, -0.6381],\n",
       "         [ 0.3840,  0.0244,  0.1858,  0.4654, -0.6403],\n",
       "         [ 0.3793,  0.0232,  0.1918,  0.4651, -0.6340],\n",
       "         [ 0.3780,  0.0238,  0.1878,  0.4621, -0.6316],\n",
       "         [ 0.3833,  0.0229,  0.1913,  0.4682, -0.6392],\n",
       "         [ 0.3800,  0.0234,  0.1880,  0.4640, -0.6342],\n",
       "         [ 0.3828,  0.0239,  0.1888,  0.4661, -0.6389],\n",
       "         [ 0.3800,  0.0236,  0.1910,  0.4650, -0.6352],\n",
       "         [ 0.3789,  0.0247,  0.1841,  0.4608, -0.6330],\n",
       "         [ 0.3799,  0.0238,  0.1863,  0.4630, -0.6340],\n",
       "         [ 0.3788,  0.0231,  0.1901,  0.4642, -0.6327],\n",
       "         [ 0.3815,  0.0242,  0.1867,  0.4640, -0.6369],\n",
       "         [ 0.3822,  0.0233,  0.1894,  0.4663, -0.6376],\n",
       "         [ 0.3823,  0.0229,  0.1912,  0.4674, -0.6378],\n",
       "         [ 0.3797,  0.0234,  0.1903,  0.4646, -0.6343],\n",
       "         [ 0.3812,  0.0252,  0.1835,  0.4619, -0.6366]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_d_model = 4\n",
    "dummy_d_k = 3\n",
    "dummy_d_v = 5\n",
    "dummy_seq_length = 20\n",
    "dummy_batch_size = 1\n",
    "X = torch.rand(dummy_batch_size, dummy_seq_length, dummy_d_model)\n",
    "dummy_sf = DummySelfAttention(d_model = dummy_d_model, d_k = dummy_d_k, d_v = dummy_d_v, debugger = True)\n",
    "dummy_sf(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "735b1f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model:int , d_k: int, d_v: int):\n",
    "        \"\"\"\n",
    "        > Implements single-head scaled dot-product self-attention.\n",
    "\n",
    "        > This class is only responsible for Self-Attention Mechanism and not for transforming the Attention's Output's Value space to d_model's space\n",
    "        > Explanation: Self-Attention's output lives in d_v vector space,\n",
    "                       this class is not responsible for projecting output back to d_model's vector space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        # Initializing Learned Linear Projection Matrix\n",
    "        # The original Transformer paper uses no bias for Q/K/V, so I'm also not using them by setting bias = False\n",
    "        self.W_Q = nn.Linear(d_model, d_k, bias = False)\n",
    "        self.W_K = nn.Linear(d_model, d_k, bias = False)\n",
    "        self.W_V = nn.Linear(d_model, d_v, bias = False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, _, d_model = x.shape # x.shape = (B, T, d_model)\n",
    "        assert self.d_model == d_model, \"self.d_model must be equal to d_model from x.shape\"\n",
    "        # Calculate Query, Key and Value Matrix\n",
    "        Q = self.W_Q(x)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "\n",
    "        # Get the Updated Information and update each token's representation\n",
    "        S = (Q @ K.transpose(-2, -1) / (self.d_k ** 0.5)) # to ensure that standard deviation is consistent after transformation \n",
    "        A = torch.softmax(S, dim = -1)\n",
    "        Z = A @ V # Right now, Z.shape != x.shape because attention outputs are in value space we project Z back into,\n",
    "                  # d_model so residual connections and multiple Transformer layers can be stacked\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91ee403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float):\n",
    "        \"\"\"\n",
    "        This class is responsible for Performing the Multi-head-Attention and,\n",
    "        this will also project back into d_model with correct basis vector system\n",
    "        the Architecture's Output is also in d_model's vector space but in wrong basis vector because of concatenation\n",
    "        so we need to Project the output back to d_model to fix the vecto's basis system\n",
    "\n",
    "        Also this supports the cross attention, so q, k, v are forwarded explicitly\n",
    "        becaue in deoder q comes from hidden states, k and v comes from encoders output\n",
    "        so: forward(self, q, k, v) --> supports the encoder-decoder attention\n",
    "        so: forward(self, x) --> supports the self-MHA\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0, \"self.d_model must be a multiple of h\"\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.d_k = self.d_model // self.h\n",
    "        self.d_v = self.d_model // self.h\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.W_Q = nn.Linear(self.d_model, self.h * self.d_k, bias = False)\n",
    "        self.W_K = nn.Linear(self.d_model, self.h * self.d_k, bias = False)\n",
    "        self.W_V = nn.Linear(self.d_model, self.h * self.d_v, bias = False)\n",
    "        self.W_O = nn.Linear(self.d_model, self.d_model, bias = False) # This is the output Projection \n",
    "        # The output of This Multi-head Attention is in d_model space we need an output projection layer so that it must learn,\n",
    "        # the importance of Each head and how each head are relevent to each other\n",
    "        # It is required to enable Stacking, Residual Conncetions\n",
    "        # mathematically,\n",
    "                # The Dimensions are numerically equal but the basis systems of vectors are different\n",
    "                # changes the basis,  not the dimensionality.\n",
    "        # W_O does three things simultaneously:\n",
    "            # Mixes information across heads\n",
    "            # Learns head importance\n",
    "            # Re-embeds into a shared semantic space\n",
    "    \n",
    "    def forward_selfMHA(self, x):\n",
    "        \"\"\"\n",
    "        This forward method is not acceptable for cross-attention (encoder-decoder transformers)\n",
    "        because cross-attention expects,\n",
    "        # q from decoders hidden's state\n",
    "        # k, v from encoders output\n",
    "        \"\"\"\n",
    "        B, T, _ = x.shape\n",
    "        Q = self.W_Q(x).view(B, T, self.h, self.d_k).transpose(1, 2) # We split this and transpose this --> shape: [B, h, T, d_k]\n",
    "        K = self.W_K(x).view(B, T, self.h, self.d_k).transpose(1, 2)\n",
    "        V = self.W_V(x).view(B, T, self.h, self.d_k).transpose(1, 2)\n",
    "        # We split this and transpose this\n",
    "        # After Splitting --> shape: [B, T, h, d_k]\n",
    "        # Now Transpose: --> shape: [B, h, T, d_k] --> each head now have its' completely own sequence\n",
    "        print(\"Q.shape\", Q.shape)\n",
    "        print(\"K.shape\", K.shape)\n",
    "        print(\"V.shape\", V.shape)\n",
    "\n",
    "        S = (Q @ K.transpose(-1, -2))/(self.d_k ** 0.5) # or we can do, K.transpose(-2, -1)\n",
    "        S = torch.softmax(S, dim = -1)\n",
    "        \n",
    "        # Update the information of each of the tokens for each of the heads\n",
    "        # Z.shape = (B, h, T, d_v)\n",
    "        Z = (S @ V) # This will update the information and this will project into the d_v = d_k = d_model/h dimentional space\n",
    "\n",
    "        # Now we concatenate this to get the Representation into our embedding space i.e. d_model's Vector Space\n",
    "        print(\"Before concatenation: output.Shape = \", Z.shape)\n",
    "        Z = Z.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "        Z = self.W_O(Z) # This is the output projection we need this, compulsorily\n",
    "        print(\"After concatenation: output.Shape = \", Z.shape)\n",
    "        # we want to view this as, (B, T, d_model)\n",
    "        return Z\n",
    "    \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        # Shape of q: (B, T, d_model) --> (B, T, h, d_model/h) --> --> (B, h, T, d_model/h)\n",
    "        B, T, _ = q.shape\n",
    "        query = self.W_Q(q).view(B, T, self.h, self.d_k).transpose(1, 2)\n",
    "        key = self.W_K(k).view(B, T, self.h, self.d_k).transpose(1, 2)\n",
    "        value = self.W_V(v).view(B, T, self.h, self.d_v).transpose(1, 2) # (B, h, T, d_model/h)\n",
    "\n",
    "        S = ((query @ key.transpose(-2, -1)) / (self.d_k ** 0.5)) # S.Shape: (B, h, T, T)\n",
    "        if mask is not None:\n",
    "            S = S.masked_fill(mask == 0, -1e9)\n",
    "        S = torch.softmax(S, dim = -1)\n",
    "        S = self.dropout(S)\n",
    "        Z = S @ value\n",
    "        # (B, h, T, T) @ (B, h, T, d_model/h) = (B, h, T, d_model/h) --> (B, T, h, d_model/h)\n",
    "        return self.dropout(self.W_O(Z.transpose(1, 2).contiguous().view(B, T, -1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee121cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0567, -0.0278,  0.0725,  ...,  0.0000, -0.0933,  0.0224],\n",
       "         [-0.0315, -0.0000,  0.0219,  ...,  0.1418, -0.1089,  0.0003],\n",
       "         [ 0.0059, -0.0657,  0.0499,  ...,  0.0000, -0.1144,  0.0086],\n",
       "         ...,\n",
       "         [-0.0141, -0.0465, -0.0024,  ...,  0.0966, -0.1304, -0.0165],\n",
       "         [-0.0496, -0.0172,  0.0437,  ...,  0.1461, -0.1158,  0.0000],\n",
       "         [ 0.0021, -0.0493,  0.0219,  ...,  0.1515, -0.0959,  0.0083]],\n",
       "\n",
       "        [[-0.1196, -0.0000,  0.0462,  ...,  0.1545, -0.1757,  0.0294],\n",
       "         [-0.1239, -0.0412,  0.0602,  ...,  0.1473, -0.1611,  0.0597],\n",
       "         [-0.0890, -0.0603,  0.0585,  ...,  0.0865, -0.1498,  0.0127],\n",
       "         ...,\n",
       "         [-0.0826, -0.0387,  0.0564,  ...,  0.1146, -0.1289,  0.0003],\n",
       "         [-0.1009, -0.0204,  0.0352,  ...,  0.1145, -0.1133,  0.0081],\n",
       "         [-0.1303, -0.0231,  0.0386,  ...,  0.1276, -0.1231, -0.0049]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 10, 64)\n",
    "MultiHeadAttention(d_model = 64, h = 8, dropout = 0.1)(x, x, x, mask = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a10dd86",
   "metadata": {},
   "source": [
    "torch.Size([2, 10, 64]) --> we have 2 examples of each 10, 64 --> meaning 2 examples having seq_length = 10 and each char is represented as 64-Dimensional vector\n",
    "\n",
    "torch.Size([2, 10, 8, 8]) --> we have 2 examples of each containing seq_length = 10 having 8 different heads and each heads are 8-Dimensional vectors\n",
    "\n",
    "torch.Size([2, 8, 10, 8]) ---> we have 2 examples of each containing 8 heads and each head will be seq_length = 10 long and each will opearate on 8-D vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8109f77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2105, -0.0680, -0.0706,  0.0245, -0.5060],\n",
       "         [ 0.2104, -0.0675, -0.0711,  0.0248, -0.5056],\n",
       "         [ 0.2104, -0.0677, -0.0705,  0.0246, -0.5059],\n",
       "         [ 0.2108, -0.0679, -0.0717,  0.0250, -0.5054],\n",
       "         [ 0.2107, -0.0683, -0.0705,  0.0243, -0.5062],\n",
       "         [ 0.2111, -0.0681, -0.0725,  0.0255, -0.5050],\n",
       "         [ 0.2103, -0.0664, -0.0732,  0.0263, -0.5043],\n",
       "         [ 0.2094, -0.0671, -0.0685,  0.0236, -0.5070],\n",
       "         [ 0.2111, -0.0681, -0.0725,  0.0254, -0.5050],\n",
       "         [ 0.2104, -0.0664, -0.0734,  0.0264, -0.5042],\n",
       "         [ 0.2110, -0.0676, -0.0732,  0.0260, -0.5045],\n",
       "         [ 0.2103, -0.0670, -0.0719,  0.0254, -0.5051],\n",
       "         [ 0.2097, -0.0669, -0.0697,  0.0244, -0.5062],\n",
       "         [ 0.2102, -0.0665, -0.0725,  0.0259, -0.5046],\n",
       "         [ 0.2099, -0.0668, -0.0708,  0.0248, -0.5057],\n",
       "         [ 0.2104, -0.0679, -0.0701,  0.0243, -0.5062],\n",
       "         [ 0.2109, -0.0674, -0.0732,  0.0260, -0.5045],\n",
       "         [ 0.2108, -0.0680, -0.0716,  0.0250, -0.5055],\n",
       "         [ 0.2102, -0.0670, -0.0715,  0.0252, -0.5053],\n",
       "         [ 0.2103, -0.0676, -0.0705,  0.0246, -0.5059]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SelfAttention(d_model = 4, d_k = 3, d_v = 5)(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a25c65",
   "metadata": {},
   "source": [
    "### Now that i've implemented the Transformer Architecture in src/transformer\n",
    "\n",
    "- Let's see an Inference time optimization technique called as KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c0505bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"tiny_shakespeare.txt\", 'r', encoding='utf-8').read()\n",
    "input_text = \"Hi, I'm Himanshu Sing\"\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "vocab_size = len(chars)\n",
    "d_model = 2\n",
    "\n",
    "# Let's Create an Emebedding\n",
    "embeddings = torch.rand(vocab_size, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cca0ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_idx = torch.tensor([stoi[x] for x in input_text]).unsqueeze(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7ff4077",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_enc = embeddings[x_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b16c7713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8190, 0.0844],\n",
       "         [0.6219, 0.3023],\n",
       "         [0.5767, 0.7185],\n",
       "         [0.0599, 0.1221],\n",
       "         [0.3400, 0.0207],\n",
       "         [0.4904, 0.4405],\n",
       "         [0.6588, 0.4593],\n",
       "         [0.0599, 0.1221],\n",
       "         [0.8190, 0.0844],\n",
       "         [0.6219, 0.3023],\n",
       "         [0.6588, 0.4593],\n",
       "         [0.0848, 0.1006],\n",
       "         [0.6150, 0.9959],\n",
       "         [0.3872, 0.2297],\n",
       "         [0.2240, 0.5658],\n",
       "         [0.2021, 0.2924],\n",
       "         [0.0599, 0.1221],\n",
       "         [0.2628, 0.3887],\n",
       "         [0.6219, 0.3023],\n",
       "         [0.6150, 0.9959],\n",
       "         [0.7015, 0.8609]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "362e4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head = MultiHeadAttention(d_model = d_model, h = 2, dropout = 0.1)\n",
    "output = multi_head(x_enc, x_enc, x_enc, mask = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b845e95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1882, -0.0909],\n",
       "         [ 0.2497, -0.1380],\n",
       "         [ 0.2690, -0.1536],\n",
       "         [ 0.2712, -0.1551],\n",
       "         [ 0.2561, -0.1431],\n",
       "         [ 0.2238, -0.1202],\n",
       "         [ 0.2396, -0.1343],\n",
       "         [ 0.2431, -0.1425],\n",
       "         [ 0.2376, -0.1347],\n",
       "         [ 0.0000, -0.1095],\n",
       "         [ 0.0000, -0.1369],\n",
       "         [ 0.0000, -0.1517],\n",
       "         [ 0.2649, -0.1569],\n",
       "         [ 0.0000, -0.1494],\n",
       "         [ 0.0000, -0.1077],\n",
       "         [ 0.2175, -0.1115],\n",
       "         [ 0.2347, -0.1369],\n",
       "         [ 0.2684, -0.1530],\n",
       "         [ 0.2368, -0.1449],\n",
       "         [ 0.2339, -0.1362],\n",
       "         [ 0.2294, -0.1240]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ffcc3c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h_enc': {'query_h': tensor([-0.1649, -0.3664], grad_fn=<SqueezeBackward4>),\n",
       "  'key_h': tensor([-0.4249,  0.3146], grad_fn=<SqueezeBackward4>),\n",
       "  'value_h': tensor([-0.0890,  0.1684], grad_fn=<SqueezeBackward4>)},\n",
       " 'i_enc': {'query_i': tensor([-0.0372, -0.3314], grad_fn=<SqueezeBackward4>),\n",
       "  'key_i': tensor([-0.3966,  0.0626], grad_fn=<SqueezeBackward4>),\n",
       "  'value_i': tensor([0.2937, 0.3658], grad_fn=<SqueezeBackward4>)},\n",
       " 'm_enc': {'query_m': tensor([-0.0855, -0.4262], grad_fn=<SqueezeBackward4>),\n",
       "  'key_m': tensor([-0.5059,  0.1552], grad_fn=<SqueezeBackward4>),\n",
       "  'value_m': tensor([0.2518, 0.3986], grad_fn=<SqueezeBackward4>)}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_1 = \"h\"\n",
    "char_2 = \"hi\"\n",
    "char_3 = \"him\"\n",
    "char_4 = \"hima\"\n",
    "char_5 = \"himan\"\n",
    "char_6 = \"himans\"\n",
    "char_7 = \"himansh\"\n",
    "char_8 = \"himanshu\"\n",
    "scores = {}\n",
    "\n",
    "# For char = h\n",
    "char_enc = embeddings[[stoi[x] for x in char_1]]\n",
    "query_h = multi_head.W_Q(char_enc[-1])\n",
    "key_h = multi_head.W_K(char_enc[-1])\n",
    "value_h = multi_head.W_V(char_enc[-1])\n",
    "scores['for_h'] = {}\n",
    "scores['for_h']['query_h'] = query_h\n",
    "scores['for_h']['key_h'] = key_h\n",
    "scores['for_h']['value_h'] = value_h\n",
    "\n",
    "# For char = i\n",
    "char_enc = embeddings[[stoi[x] for x in char_2]]\n",
    "query_i = multi_head.W_Q(char_enc[-1])\n",
    "key_i = multi_head.W_K(char_enc[-1])\n",
    "value_i = multi_head.W_V(char_enc[-1])\n",
    "scores['for_i'] = {}\n",
    "scores['for_i']['query_i'] = query_i\n",
    "scores['for_i']['key_i'] = key_i\n",
    "scores['for_i']['value_i'] = value_i\n",
    "\n",
    "# For char = m\n",
    "char_enc = embeddings[[stoi[x] for x in char_3]]\n",
    "query_m = multi_head.W_Q(char_enc[-1])\n",
    "key_m = multi_head.W_K(char_enc[-1])\n",
    "value_m = multi_head.W_V(char_enc[-1])\n",
    "scores['for_m'] = {}\n",
    "scores['for_m']['query_m'] = query_m\n",
    "scores['for_m']['key_m'] = key_m\n",
    "scores['for_m']['value_m'] = value_m\n",
    "\n",
    "scores\n",
    "\n",
    "\n",
    "# -------------- For char_3 | char_3 = \"him\" -----------------------\n",
    "x_enc_3 = embeddings[[stoi[x] for x in char_3]]\n",
    "x_enc_3\n",
    "h_enc = x_enc_3[0]\n",
    "i_enc = x_enc_3[1]\n",
    "m_enc = x_enc_3[2]\n",
    "dict_scores_char_3 = {}\n",
    "dict_scores_char_3['h_enc'] = {}\n",
    "dict_scores_char_3['h_enc']['query_h'] = {}\n",
    "dict_scores_char_3['h_enc']['key_h'] = {}\n",
    "dict_scores_char_3['h_enc']['value_h'] = {}\n",
    "query_h = multi_head.W_Q(h_enc)\n",
    "key_h = multi_head.W_K(h_enc)\n",
    "value_h = multi_head.W_V(h_enc)\n",
    "dict_scores_char_3['h_enc']['query_h'] = query_h\n",
    "dict_scores_char_3['h_enc']['key_h'] = key_h\n",
    "dict_scores_char_3['h_enc']['value_h'] = value_h\n",
    "\n",
    "dict_scores_char_3['i_enc'] = {}\n",
    "dict_scores_char_3['i_enc']['query_i'] = {}\n",
    "dict_scores_char_3['i_enc']['key_i'] = {}\n",
    "dict_scores_char_3['i_enc']['value_i'] = {}\n",
    "query_i = multi_head.W_Q(i_enc)\n",
    "key_i = multi_head.W_K(i_enc)\n",
    "value_i = multi_head.W_V(i_enc)\n",
    "dict_scores_char_3['i_enc']['query_i'] = query_i\n",
    "dict_scores_char_3['i_enc']['key_i'] = key_i\n",
    "dict_scores_char_3['i_enc']['value_i'] = value_i\n",
    "\n",
    "dict_scores_char_3['m_enc'] = {}\n",
    "dict_scores_char_3['m_enc']['query_m'] = {}\n",
    "dict_scores_char_3['m_enc']['key_m'] = {}\n",
    "dict_scores_char_3['m_enc']['value_m'] = {}\n",
    "query_m = multi_head.W_Q(m_enc)\n",
    "key_m = multi_head.W_K(m_enc)\n",
    "value_m = multi_head.W_V(m_enc)\n",
    "dict_scores_char_3['m_enc']['query_m'] = query_m\n",
    "dict_scores_char_3['m_enc']['key_m'] = key_m\n",
    "dict_scores_char_3['m_enc']['value_m'] = value_m\n",
    "\n",
    "dict_scores_char_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47fc2f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'for_h': {'query_h': tensor([-0.1649, -0.3664], grad_fn=<SqueezeBackward4>),\n",
       "  'key_h': tensor([-0.4249,  0.3146], grad_fn=<SqueezeBackward4>),\n",
       "  'value_h': tensor([-0.0890,  0.1684], grad_fn=<SqueezeBackward4>)},\n",
       " 'for_i': {'query_i': tensor([-0.0372, -0.3314], grad_fn=<SqueezeBackward4>),\n",
       "  'key_i': tensor([-0.3966,  0.0626], grad_fn=<SqueezeBackward4>),\n",
       "  'value_i': tensor([0.2937, 0.3658], grad_fn=<SqueezeBackward4>)},\n",
       " 'for_m': {'query_m': tensor([-0.0855, -0.4262], grad_fn=<SqueezeBackward4>),\n",
       "  'key_m': tensor([-0.5059,  0.1552], grad_fn=<SqueezeBackward4>),\n",
       "  'value_m': tensor([0.2518, 0.3986], grad_fn=<SqueezeBackward4>)}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ae8d4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0551, -0.2668], grad_fn=<SqueezeBackward4>),\n",
       " tensor([-0.3285, -0.1181], grad_fn=<SqueezeBackward4>),\n",
       " tensor([0.5205, 0.4565], grad_fn=<SqueezeBackward4>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_h, key_h, value_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2bd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
